{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "899a8a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9be7ac0",
   "metadata": {},
   "source": [
    "# Populate unique tokens of PPs to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "73b9e819",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pp_lexicon/unique_tokens_copy.json', 'r') as f:\n",
    "    unique_tokens_copy = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2d7fc5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_lexicon = {}\n",
    "\n",
    "# Define a template for each lexical entry\n",
    "template = {\n",
    "    \"isAtomicMorph\": \"\",\n",
    "    \"class\": \"\",\n",
    "    \"spellOutHEAD\": [\"\"],\n",
    "    \"path_p_morphology\": \"\",\n",
    "    \"measure_allowed\": \"\"\n",
    "}\n",
    "\n",
    "for token in unique_tokens_copy:\n",
    "    p_lexicon[token] = {\n",
    "        \"isAtomicMorph\": template[\"isAtomicMorph\"],\n",
    "        \"class\": template[\"class\"],\n",
    "        \"spellOutHEAD\": list(template[\"spellOutHEAD\"]),\n",
    "        \"path_p_morphology\": template[\"path_p_morphology\"],\n",
    "        \"measure_allowed\": template[\"measure_allowed\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8acce877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write the p_lexicon to a JSON file\n",
    "# with open('pp_lexicon/p_lexicon.json', 'w') as f:\n",
    "#     json.dump(p_lexicon, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c9e6acec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pp_lexicon/p_lexicon.json', 'r') as f:\n",
    "    annotated_p_lex = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a9fd592f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty tokens: ['without', 'towards', 'except', 'heart', 'afore', 'tween', 'aboard', 'throughout', 'foot', 'nearer']\n",
      "Number of entries with empty values: 31\n"
     ]
    }
   ],
   "source": [
    "# check if annotated_p_lex value is empty string and calculate how many entries have empty values\n",
    "# also print the empty token\n",
    "empty_count = 0\n",
    "empty_tokens = []\n",
    "for token, entry in annotated_p_lex.items():\n",
    "    if entry[\"isAtomicMorph\"] == \"\" and entry[\"class\"] == \"\" and entry[\"path_p_morphology\"] == \"\" and entry[\"measure_allowed\"] == \"\" and entry[\"spellOutHEAD\"] == [\"\"]:\n",
    "        empty_count += 1\n",
    "        empty_tokens.append(token)\n",
    "print(f\"Empty tokens: {empty_tokens[8:18]}\")\n",
    "print(f\"Number of entries with empty values: {empty_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "81c67b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pps = list(annotated_p_lex.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c2a00db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pp_lexicon/unique_tokens_to_decompose.json', 'r') as f:\n",
    "    ut_decompose = json.load(f)\n",
    "\n",
    "to_decompose = ut_decompose[0]['to_decompose'] \n",
    "atomic = ut_decompose[1]['atomic'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "30c08429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'top',\n",
       " 'by',\n",
       " 'to',\n",
       " 'but',\n",
       " 'end',\n",
       " 'for',\n",
       " 'high',\n",
       " 'via',\n",
       " 'side',\n",
       " 'back',\n",
       " 'rear',\n",
       " 'part',\n",
       " 'back',\n",
       " 'center',\n",
       " 'with',\n",
       " 'prior',\n",
       " 'front',\n",
       " 'opposite',\n",
       " 'from',\n",
       " 'beneath',\n",
       " 'rear',\n",
       " 'of',\n",
       " 'far',\n",
       " 'amid',\n",
       " 'board',\n",
       " 'next',\n",
       " 'ward',\n",
       " 'wards',\n",
       " 'under',\n",
       " 'neath',\n",
       " 'skin',\n",
       " 'flank',\n",
       " 'plus',\n",
       " 'middle',\n",
       " 'rim',\n",
       " 'face',\n",
       " 'close',\n",
       " 'amidst',\n",
       " 'virtue']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atomic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c9f2144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposer_pps = list_pps.copy()\n",
    "# drop values in decomposer_pps = ['astride', 'edge', 'on', 'to']\n",
    "decomposer_pps = [pp for pp in decomposer_pps if pp not in ['astride', 'edge', 'on', 'to']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0b3f8576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english_word(w):\n",
    "    w = w.lower()\n",
    "    return bool(wn.synsets(w))\n",
    "\n",
    "def decompose_preposition(preposition, atomic_tokens, require_atomic_remainder=True):\n",
    "   \n",
    "    result = {}\n",
    "    p = preposition.lower()\n",
    "    atomic_lower = {tok.lower() for tok in atomic_tokens}\n",
    "\n",
    "    for token in atomic_tokens:\n",
    "        t = token.lower()\n",
    "        # Skip trivial cases and tokens that don't even appear inside p\n",
    "        if t == p:\n",
    "            continue\n",
    "        if t not in p:\n",
    "            continue\n",
    "\n",
    "        # Count how many times ‘t’ shows up in ‘p’\n",
    "        count = p.count(t)  \n",
    "        # Remove exactly one occurrence of t from p\n",
    "        remainder = p.replace(t, \"\", 1)\n",
    "\n",
    "        # Check remainder:\n",
    "        if require_atomic_remainder:\n",
    "            # We only accept remainders that are themselves in atomic_tokens:\n",
    "            if remainder in atomic_lower:\n",
    "                result[token] = {\n",
    "                    'decomposition': [token, remainder],\n",
    "                    'occurrence': count\n",
    "                }\n",
    "        else:\n",
    "            # We accept any English‐word remainder\n",
    "            if is_english_word(remainder):\n",
    "                result[token] = {\n",
    "                    'decomposition': [token, remainder],\n",
    "                    'occurrence': count\n",
    "                }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225f4271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8728f4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'top',\n",
       " 'by',\n",
       " 'to',\n",
       " 'but',\n",
       " 'end',\n",
       " 'for',\n",
       " 'high',\n",
       " 'via',\n",
       " 'side',\n",
       " 'back',\n",
       " 'rear',\n",
       " 'part',\n",
       " 'back',\n",
       " 'center',\n",
       " 'with',\n",
       " 'prior',\n",
       " 'front',\n",
       " 'opposite',\n",
       " 'from',\n",
       " 'beneath',\n",
       " 'rear',\n",
       " 'of',\n",
       " 'far',\n",
       " 'amid',\n",
       " 'board',\n",
       " 'next',\n",
       " 'ward',\n",
       " 'wards',\n",
       " 'under',\n",
       " 'neath',\n",
       " 'skin',\n",
       " 'flank',\n",
       " 'plus',\n",
       " 'middle',\n",
       " 'rim',\n",
       " 'face',\n",
       " 'close',\n",
       " 'amidst',\n",
       " 'virtue']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atomic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a3371752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['astride',\n",
       " 'atop',\n",
       " 'corner',\n",
       " 'by',\n",
       " 'edge',\n",
       " 'onto',\n",
       " 'after',\n",
       " 'but',\n",
       " 'end',\n",
       " 'into',\n",
       " 'upside',\n",
       " 'for',\n",
       " 'base',\n",
       " 'higher',\n",
       " 'bottom',\n",
       " 'as',\n",
       " 'via',\n",
       " 'aside',\n",
       " 'back',\n",
       " 'nearest',\n",
       " 'betwixt',\n",
       " 'at',\n",
       " 'ahead',\n",
       " 'astern',\n",
       " 'following',\n",
       " 'center',\n",
       " 'within',\n",
       " 'prior',\n",
       " 'without',\n",
       " 'front',\n",
       " 'before',\n",
       " 'opposite',\n",
       " 'from',\n",
       " 'beneath',\n",
       " 'means',\n",
       " 'apart',\n",
       " 'towards',\n",
       " 'except',\n",
       " 'heart',\n",
       " 'afore',\n",
       " 'tween',\n",
       " 'aboard',\n",
       " 'throughout',\n",
       " 'foot',\n",
       " 'nearer',\n",
       " 'nigh',\n",
       " 'alongside',\n",
       " 'toward',\n",
       " 'amongst',\n",
       " 'adjacent',\n",
       " 'underneath',\n",
       " 'subsequent',\n",
       " 'addition',\n",
       " 'cross',\n",
       " 'surface',\n",
       " 'amidst',\n",
       " 'underside']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b70c0e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ac7d9859",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposer_pps = list_pps.copy()\n",
    "\n",
    "\n",
    "result_decompose = {}\n",
    "for pp in to_decompose:\n",
    "    comps = decompose_preposition(pp, atomic, require_atomic_remainder=False)\n",
    "    if comps:\n",
    "        result_decompose[pp] = comps\n",
    "\n",
    "# turn it into a flat table\n",
    "rows = []\n",
    "for preposition, comps in result_decompose.items():\n",
    "    for token, details in comps.items():\n",
    "        rows.append({\n",
    "            'preposition': preposition,\n",
    "            'token': token,\n",
    "            'decomposition': details['decomposition'],\n",
    "            'occurrence': details['occurrence']\n",
    "        })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "de014758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print result_decompose dict as pandas\n",
    "df_decompose = pd.DataFrame(rows)\n",
    "df_decompose = df_decompose.sort_values(by=['preposition', 'occurrence'], ascending=[True, False])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ae4465d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decompose\n",
    "# drop df_decompose row by index\n",
    "df_decompose = df_decompose.drop(index=[8,12, 6, 17, 19, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c5fd4f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preposition</th>\n",
       "      <th>token</th>\n",
       "      <th>decomposition</th>\n",
       "      <th>occurrence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>aboard</td>\n",
       "      <td>a</td>\n",
       "      <td>[a, board]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>aboard</td>\n",
       "      <td>board</td>\n",
       "      <td>[board, a]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>afore</td>\n",
       "      <td>a</td>\n",
       "      <td>[a, fore]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ahead</td>\n",
       "      <td>a</td>\n",
       "      <td>[a, head]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>alongside</td>\n",
       "      <td>side</td>\n",
       "      <td>[side, along]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>amidst</td>\n",
       "      <td>a</td>\n",
       "      <td>[a, midst]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>apart</td>\n",
       "      <td>a</td>\n",
       "      <td>[a, part]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>apart</td>\n",
       "      <td>part</td>\n",
       "      <td>[part, a]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>aside</td>\n",
       "      <td>a</td>\n",
       "      <td>[a, side]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>aside</td>\n",
       "      <td>side</td>\n",
       "      <td>[side, a]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>astern</td>\n",
       "      <td>a</td>\n",
       "      <td>[a, stern]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>astride</td>\n",
       "      <td>a</td>\n",
       "      <td>[a, stride]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atop</td>\n",
       "      <td>a</td>\n",
       "      <td>[a, top]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>atop</td>\n",
       "      <td>top</td>\n",
       "      <td>[top, a]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>beneath</td>\n",
       "      <td>neath</td>\n",
       "      <td>[neath, be]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>higher</td>\n",
       "      <td>high</td>\n",
       "      <td>[high, er]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>into</td>\n",
       "      <td>to</td>\n",
       "      <td>[to, in]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>onto</td>\n",
       "      <td>to</td>\n",
       "      <td>[to, on]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>surface</td>\n",
       "      <td>face</td>\n",
       "      <td>[face, sur]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>toward</td>\n",
       "      <td>to</td>\n",
       "      <td>[to, ward]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>towards</td>\n",
       "      <td>to</td>\n",
       "      <td>[to, wards]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>underneath</td>\n",
       "      <td>neath</td>\n",
       "      <td>[neath, under]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>underside</td>\n",
       "      <td>side</td>\n",
       "      <td>[side, under]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>underside</td>\n",
       "      <td>under</td>\n",
       "      <td>[under, side]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>upside</td>\n",
       "      <td>side</td>\n",
       "      <td>[side, up]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>within</td>\n",
       "      <td>with</td>\n",
       "      <td>[with, in]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>without</td>\n",
       "      <td>with</td>\n",
       "      <td>[with, out]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   preposition  token   decomposition  occurrence\n",
       "24      aboard      a      [a, board]           2\n",
       "25      aboard  board      [board, a]           1\n",
       "23       afore      a       [a, fore]           1\n",
       "13       ahead      a       [a, head]           2\n",
       "26   alongside   side   [side, along]           1\n",
       "30      amidst      a      [a, midst]           1\n",
       "20       apart      a       [a, part]           2\n",
       "21       apart   part       [part, a]           1\n",
       "10       aside      a       [a, side]           1\n",
       "11       aside   side       [side, a]           1\n",
       "14      astern      a      [a, stern]           1\n",
       "0      astride      a     [a, stride]           1\n",
       "1         atop      a        [a, top]           1\n",
       "2         atop    top        [top, a]           1\n",
       "18     beneath  neath     [neath, be]           1\n",
       "7       higher   high      [high, er]           1\n",
       "4         into     to        [to, in]           1\n",
       "3         onto     to        [to, on]           1\n",
       "29     surface   face     [face, sur]           1\n",
       "27      toward     to      [to, ward]           1\n",
       "22     towards     to     [to, wards]           1\n",
       "28  underneath  neath  [neath, under]           1\n",
       "31   underside   side   [side, under]           1\n",
       "32   underside  under   [under, side]           1\n",
       "5       upside   side      [side, up]           1\n",
       "15      within   with      [with, in]           1\n",
       "16     without   with     [with, out]           1"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8af761ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aboard',\n",
       " 'afore',\n",
       " 'ahead',\n",
       " 'alongside',\n",
       " 'amidst',\n",
       " 'apart',\n",
       " 'aside',\n",
       " 'astern',\n",
       " 'astride',\n",
       " 'atop',\n",
       " 'beneath',\n",
       " 'higher',\n",
       " 'into',\n",
       " 'onto',\n",
       " 'surface',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'underneath',\n",
       " 'underside',\n",
       " 'upside',\n",
       " 'within',\n",
       " 'without'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decomposed = set(df_decompose['preposition'])\n",
    "decomposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "89c2abca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['behind', 'above', 'below', 'beyond', 'in front of', 'inside', 'outside', 'left', 'right', 'against', 'among', 'beside', 'between', 'near', 'next to', 'upon', 'across', 'along', 'around', 'over', 'past', 'through', 'under', 'up', 'down', 'on', 'off', 'in', 'out', 'away'])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('pp_lexicon/atomic_p.json', 'r') as f:\n",
    "    atomic_p = json.load(f)\n",
    "atomic_p['atomic_p'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ea313ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'above',\n",
       " 'across',\n",
       " 'against',\n",
       " 'along',\n",
       " 'among',\n",
       " 'around',\n",
       " 'away',\n",
       " 'behind',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'down',\n",
       " 'in',\n",
       " 'in front of',\n",
       " 'inside',\n",
       " 'left',\n",
       " 'near',\n",
       " 'next to',\n",
       " 'off',\n",
       " 'on',\n",
       " 'out',\n",
       " 'outside',\n",
       " 'over',\n",
       " 'past',\n",
       " 'right',\n",
       " 'through',\n",
       " 'under',\n",
       " 'up',\n",
       " 'upon'}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract keys into a separate set so atomic_p (the dict) isn’t overwritten\n",
    "atomic_p_keys = set(atomic_p['atomic_p'].keys())\n",
    "atomic_p_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "936143c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neath', 'plus', 'under', 'virtue', 'ward', 'wards'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# match ut_decompose[1]['atomic'] with annotated_p_lex that is not empty entry\n",
    "atomic_set = set(atomic)\n",
    "annotated_atomic = set()\n",
    "for token, entry in annotated_p_lex.items():\n",
    "    if entry[\"isAtomicMorph\"] != \"\" or entry[\"class\"] != \"\" or entry[\"path_p_morphology\"] != \"\" or entry[\"measure_allowed\"] != \"\" or entry[\"spellOutHEAD\"] != [\"\"]:\n",
    "        annotated_atomic.add(token)\n",
    "annotated_atomic = annotated_atomic.intersection(atomic_set)\n",
    "# annotated_atomic\n",
    "\n",
    "# not annotated atomic tokens\n",
    "not_annotated_atomic = atomic_set - annotated_atomic\n",
    "not_annotated_atomic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f9ff2c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'top',\n",
       " 'by',\n",
       " 'to',\n",
       " 'but',\n",
       " 'end',\n",
       " 'for',\n",
       " 'high',\n",
       " 'via',\n",
       " 'side',\n",
       " 'back',\n",
       " 'rear',\n",
       " 'part',\n",
       " 'back',\n",
       " 'center',\n",
       " 'with',\n",
       " 'prior',\n",
       " 'front',\n",
       " 'opposite',\n",
       " 'from',\n",
       " 'beneath',\n",
       " 'rear',\n",
       " 'of',\n",
       " 'far',\n",
       " 'amid',\n",
       " 'board',\n",
       " 'next',\n",
       " 'ward',\n",
       " 'wards',\n",
       " 'under',\n",
       " 'neath',\n",
       " 'skin',\n",
       " 'flank',\n",
       " 'plus',\n",
       " 'middle',\n",
       " 'rim',\n",
       " 'face',\n",
       " 'close',\n",
       " 'amidst',\n",
       " 'virtue']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ut_decompose[1]['atomic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "efa61cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neath', 'plus', 'virtue', 'ward', 'wards'}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_annotated_atomic = not_annotated_atomic - atomic_p_keys\n",
    "not_annotated_atomic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37ac407",
   "metadata": {},
   "source": [
    "list of morphemes that are not annotated: {'neath', 'plus', 'virtue', 'ward', 'wards'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ec1b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21c333b8",
   "metadata": {},
   "source": [
    "# Decompose spatial prepositional phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9640e02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'preposition', 'is_atomic', 'is_spatial', 'class',\n",
       "       'transitivity', 'synonyms', 'antonyms', 'hypernym', 'hyponym',\n",
       "       'meronym', 'holonym', 'supersense'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open /dictionaries/pp_wordnet_wiki_pop.csv\n",
    "df_pp_wordnet = pd.read_csv('dictionaries/pp_wordnet_wiki_pop.csv', sep=',')\n",
    "spatial_df_pp_wordnet = df_pp_wordnet[df_pp_wordnet['is_spatial'] == True]\n",
    "spatial_df_pp_wordnet.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f5a66565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/2jj_gcx548x_37sfbd_5ny_h0000gn/T/ipykernel_27183/2395340022.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spatial_df_pp_wordnet.drop(columns=['Unnamed: 0'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "spatial_df_pp_wordnet.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ae1d91b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_full_pp = list(spatial_df_pp_wordnet['preposition'])\n",
    "len(list_full_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bbc89b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "intransitive_p = [\n",
    "    \"abroad\",\n",
    "    \"adrift\",\n",
    "    \"aft\",\n",
    "    \"afterward(s)\",\n",
    "    \"ahead\",\n",
    "    \"apart\",\n",
    "    \"ashore\",\n",
    "    \"aside\",\n",
    "    \"away\",\n",
    "    \"back\",\n",
    "    \"backward(s)\",\n",
    "    \"beforehand\",\n",
    "    \"downhill\",\n",
    "    \"downstage\",\n",
    "    \"downstairs\",\n",
    "    \"downstream\",\n",
    "    \"downward(s)\",\n",
    "    \"downwind\",\n",
    "    \"east\",\n",
    "    \"eastward(s)\",\n",
    "    \"forth\",\n",
    "    \"forward(s)\",\n",
    "    \"heavenward(s)\",\n",
    "    \"hence\",\n",
    "    \"henceforth\",\n",
    "    \"here\",\n",
    "    \"hereby\",\n",
    "    \"herein\",\n",
    "    \"hereof\",\n",
    "    \"hereto\",\n",
    "    \"herewith\",\n",
    "    \"home\",\n",
    "    \"homeward(s)\",\n",
    "    \"indoors\",\n",
    "    \"inward(s)\",\n",
    "    \"leftward(s)\",\n",
    "    \"north\",\n",
    "    \"northeast\",\n",
    "    \"northward(s)\",\n",
    "    \"northwest\",\n",
    "    \"now\",\n",
    "    \"onward(s)\",\n",
    "    \"outdoors\",\n",
    "    \"outward(s)\",\n",
    "    \"overboard\",\n",
    "    \"overhead\",\n",
    "    \"overland\",\n",
    "    \"overseas\",\n",
    "    \"rightward(s)\",\n",
    "    \"seaward(s)\",\n",
    "    \"skyward(s)\",\n",
    "    \"south\",\n",
    "    \"southeast\",\n",
    "    \"southward(s)\",\n",
    "    \"southwest\",\n",
    "    \"then\",\n",
    "    \"thence\",\n",
    "    \"thenceforth\",\n",
    "    \"there\",\n",
    "    \"thereby\",\n",
    "    \"therein\",\n",
    "    \"thereof\",\n",
    "    \"thereto\",\n",
    "    \"therewith\",\n",
    "    \"together\",\n",
    "    \"underfoot\",\n",
    "    \"underground\",\n",
    "    \"uphill\",\n",
    "    \"upstage\",\n",
    "    \"upstairs\",\n",
    "    \"upstream\",\n",
    "    \"upward(s)\",\n",
    "    \"upwind\",\n",
    "    \"west\",\n",
    "    \"westward(s)\",\n",
    "    \"when\",\n",
    "    \"whence\",\n",
    "    \"where\",\n",
    "    \"whereby\",\n",
    "    \"wherein\",\n",
    "    \"whereto\",\n",
    "    \"wherewith\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0fce24a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/2jj_gcx548x_37sfbd_5ny_h0000gn/T/ipykernel_27183/1036242634.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spatial_df_pp_wordnet['transitivity'] = spatial_df_pp_wordnet.apply(lambda row: set_transitivity(row, intransitive_p), axis=1)\n"
     ]
    }
   ],
   "source": [
    "def set_transitivity(row, intransitive_list):\n",
    "    if row['preposition'] in intransitive_list:\n",
    "        return 'intransitive'\n",
    "    else:\n",
    "        return row['transitivity'] \n",
    "\n",
    "spatial_df_pp_wordnet['transitivity'] = spatial_df_pp_wordnet.apply(lambda row: set_transitivity(row, intransitive_p), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6f658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pp_wordnet['transitivity'] == 'intransitive'\n",
    "spatial_df_pp_wordnet['transitivity'].value_counts()\n",
    "\n",
    "# save the spatial_df_pp_wordnet to a csv file\n",
    "# spatial_df_pp_wordnet.to_csv('dictionaries/pp_wordnet_wiki_pop_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2718a8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conjunctive_p = [\n",
    "    \"after\",\n",
    "    \"although\",\n",
    "    \"as\",\n",
    "    \"at\",\n",
    "    \"because\",\n",
    "    \"before\",\n",
    "    \"beside\",\n",
    "    \"besides\",\n",
    "    \"between\",\n",
    "    \"by\",\n",
    "    \"considering\",\n",
    "    \"despite\",\n",
    "    \"except\",\n",
    "    \"for\",\n",
    "    \"from\",\n",
    "    \"given\",\n",
    "    \"granted\",\n",
    "    \"if\",\n",
    "    \"into\",\n",
    "    \"lest\",\n",
    "    \"like\",\n",
    "    \"notwithstanding\",\n",
    "    \"now\",\n",
    "    \"of\",\n",
    "    \"on\",\n",
    "    \"once\",\n",
    "    \"provided\",\n",
    "    \"providing\",\n",
    "    \"save\",\n",
    "    \"seeing\",\n",
    "    \"since\",\n",
    "    \"so\",\n",
    "    \"supposing\",\n",
    "    \"than\",\n",
    "    \"though\",\n",
    "    \"till\",\n",
    "    \"to\",\n",
    "    \"unless\",\n",
    "    \"until\",\n",
    "    \"upon\",\n",
    "    \"when\",\n",
    "    \"whenever\",\n",
    "    \"where\",\n",
    "    \"whereas\",\n",
    "    \"wherever\",\n",
    "    \"while\",\n",
    "    \"whilst\",\n",
    "    \"with\",\n",
    "    \"without\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0fa276bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/2jj_gcx548x_37sfbd_5ny_h0000gn/T/ipykernel_27183/2190568508.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spatial_df_pp_wordnet['is_conjunctive'] = spatial_df_pp_wordnet['preposition'].isin(conjunctive_p)\n"
     ]
    }
   ],
   "source": [
    "# add spatial_df_pp_wordnet['is_conjunctive'] = spatial_df_pp_wordnet['preposition'].apply(lambda x: x in conjunctive_p)\n",
    "# and remov e\n",
    "spatial_df_pp_wordnet['is_conjunctive'] = spatial_df_pp_wordnet['preposition'].isin(conjunctive_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5b868a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preposition</th>\n",
       "      <th>is_atomic</th>\n",
       "      <th>is_spatial</th>\n",
       "      <th>class</th>\n",
       "      <th>transitivity</th>\n",
       "      <th>synonyms</th>\n",
       "      <th>antonyms</th>\n",
       "      <th>hypernym</th>\n",
       "      <th>hyponym</th>\n",
       "      <th>meronym</th>\n",
       "      <th>holonym</th>\n",
       "      <th>supersense</th>\n",
       "      <th>is_conjunctive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aboard</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "      <td>alongside, on base, on board</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>above</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>True</td>\n",
       "      <td>PROJECTIVE</td>\n",
       "      <td>both</td>\n",
       "      <td>higher up, in a higher place, supra, to a high...</td>\n",
       "      <td>below</td>\n",
       "      <td>section, subdivision</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>across</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>True</td>\n",
       "      <td>EXTENDED</td>\n",
       "      <td>both</td>\n",
       "      <td>crossways, crosswise</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>adjacent</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>with</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>within</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "      <td>inside</td>\n",
       "      <td>outside</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>without</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>next</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>transitive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>away</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>True</td>\n",
       "      <td>PARTICLE</td>\n",
       "      <td>intransitive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    preposition is_atomic is_spatial       class  transitivity  \\\n",
       "0             a      TRUE       True         NaN           NaN   \n",
       "1        aboard      TRUE       True         NaN          both   \n",
       "3         above      TRUE       True  PROJECTIVE          both   \n",
       "6        across      TRUE       True    EXTENDED          both   \n",
       "7      adjacent      TRUE       True         NaN           NaN   \n",
       "..          ...       ...        ...         ...           ...   \n",
       "177        with      TRUE       True         NaN           NaN   \n",
       "181      within      TRUE       True         NaN          both   \n",
       "182     without      TRUE       True         NaN          both   \n",
       "184        next      TRUE       True         NaN    transitive   \n",
       "185        away      TRUE       True    PARTICLE  intransitive   \n",
       "\n",
       "                                              synonyms antonyms  \\\n",
       "0                                                  NaN      NaN   \n",
       "1                         alongside, on base, on board      NaN   \n",
       "3    higher up, in a higher place, supra, to a high...    below   \n",
       "6                                 crossways, crosswise      NaN   \n",
       "7                                                  NaN      NaN   \n",
       "..                                                 ...      ...   \n",
       "177                                                NaN      NaN   \n",
       "181                                             inside  outside   \n",
       "182                                                NaN      NaN   \n",
       "184                                                NaN      NaN   \n",
       "185                                                NaN      NaN   \n",
       "\n",
       "                 hypernym hyponym meronym holonym supersense  is_conjunctive  \n",
       "0                     NaN     NaN     NaN     NaN        NaN           False  \n",
       "1                     NaN     NaN     NaN     NaN        NaN           False  \n",
       "3    section, subdivision     NaN     NaN     NaN        NaN           False  \n",
       "6                     NaN     NaN     NaN     NaN        NaN           False  \n",
       "7                     NaN     NaN     NaN     NaN        NaN           False  \n",
       "..                    ...     ...     ...     ...        ...             ...  \n",
       "177                   NaN     NaN     NaN     NaN        NaN            True  \n",
       "181                   NaN     NaN     NaN     NaN        NaN           False  \n",
       "182                   NaN     NaN     NaN     NaN        NaN            True  \n",
       "184                   NaN     NaN     NaN     NaN        NaN           False  \n",
       "185                   NaN     NaN     NaN     NaN        NaN           False  \n",
       "\n",
       "[135 rows x 13 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spatial_df_pp_wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113aea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_plus_p = [\n",
    "    \"across from\",\n",
    "    \"ahead of\",\n",
    "    \"along with\",\n",
    "    \"apart from\",\n",
    "    \"as for\",\n",
    "    \"as from\",\n",
    "    \"as of\",\n",
    "    \"as per\",\n",
    "    \"as regards\",\n",
    "    \"as to\",\n",
    "    \"aside from\",\n",
    "    \"away from\",\n",
    "    \"back to\",\n",
    "    \"counter to\",\n",
    "    \"in between\",\n",
    "    \"near to\",\n",
    "    \"next to\",\n",
    "    \"opposite of\",\n",
    "    \"out from\",\n",
    "    \"out of\",\n",
    "    \"outside of\",\n",
    "    \"round about\",\n",
    "    \"up against\",\n",
    "    \"up to\",\n",
    "    \"close to\",\n",
    "    \"due to\",\n",
    "    \"far from\",\n",
    "    \"prior to\",\n",
    "    \"pursuant to\",\n",
    "    \"rather than\",\n",
    "    \"subsequent to\",\n",
    "    \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8361b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_article_noun_p = [\n",
    "    \"by way of\",\n",
    "    \"for lack of\",\n",
    "    \"from want of\",\n",
    "    \"for want of\",\n",
    "    \"in contact with\",\n",
    "    \"in line with\",\n",
    "    \"in place of\",\n",
    "    \"in point of\",\n",
    "    \"in relation to\",\n",
    "    \"with regard to\",\n",
    "    \"in regard to\",\n",
    "    \"with respect to\",\n",
    "    \"in respect to\",\n",
    "    \"in touch with\",\n",
    "    \"on grounds of\",\n",
    "    \"on the part of\",\n",
    "    \"on top of\",\n",
    "    \"with a view to\",\n",
    "    \n",
    "]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
