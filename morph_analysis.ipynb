{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c9e6a762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import json\n",
    "import csv\n",
    "from nltk.corpus import wordnet as wn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62d31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['above', 'across', 'against', 'along', 'among', 'around', 'away', 'behind', 'below', 'beside', 'between', 'beyond', 'down', 'in', 'in front of', 'inside', 'left', 'near', 'next to', 'off', 'on', 'out', 'outside', 'over', 'past', 'right', 'through', 'under', 'up', 'upon']\n"
     ]
    }
   ],
   "source": [
    "with open('atomic_p.json', 'r') as f:\n",
    "    pp = json.load(f)\n",
    "    \n",
    "# print(pp['atomic_p'].keys())\n",
    "# sort pp['atomic_p'].keys() alphabetically\n",
    "keys_sorted = sorted(pp['atomic_p'].keys())\n",
    "print(keys_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9f2f3284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keys_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "497f8c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'above': ['above',\n",
       "  'supra',\n",
       "  'higher_up',\n",
       "  'in_a_higher_place',\n",
       "  'to_a_higher_place'],\n",
       " 'across': ['across', 'crosswise', 'crossways'],\n",
       " 'against': [],\n",
       " 'along': ['along', 'on'],\n",
       " 'among': [],\n",
       " 'around': ['about',\n",
       "  'around',\n",
       "  'approximately',\n",
       "  'close_to',\n",
       "  'just_about',\n",
       "  'some',\n",
       "  'roughly',\n",
       "  'more_or_less',\n",
       "  'or_so',\n",
       "  'round'],\n",
       " 'away': ['away', 'outside', 'off', 'forth', 'out', 'aside', 'by'],\n",
       " 'behind': ['buttocks',\n",
       "  'nates',\n",
       "  'arse',\n",
       "  'butt',\n",
       "  'backside',\n",
       "  'bum',\n",
       "  'buns',\n",
       "  'can',\n",
       "  'fundament',\n",
       "  'hindquarters',\n",
       "  'hind_end',\n",
       "  'keister',\n",
       "  'posterior',\n",
       "  'prat',\n",
       "  'rear',\n",
       "  'rear_end',\n",
       "  'rump',\n",
       "  'stern',\n",
       "  'seat',\n",
       "  'tail',\n",
       "  'tail_end',\n",
       "  'tooshie',\n",
       "  'tush',\n",
       "  'bottom',\n",
       "  'behind',\n",
       "  'derriere',\n",
       "  'fanny',\n",
       "  'ass',\n",
       "  'slow',\n",
       "  'behindhand',\n",
       "  'in_arrears'],\n",
       " 'below': ['below',\n",
       "  'at_a_lower_place',\n",
       "  'to_a_lower_place',\n",
       "  'beneath',\n",
       "  'infra',\n",
       "  'downstairs',\n",
       "  'down_the_stairs',\n",
       "  'on_a_lower_floor',\n",
       "  'under'],\n",
       " 'beside': [],\n",
       " 'between': ['between', 'betwixt', \"'tween\"],\n",
       " 'beyond': ['beyond'],\n",
       " 'down': ['down',\n",
       "  'down_feather',\n",
       "  'Down',\n",
       "  'John_L._H._Down',\n",
       "  'pile',\n",
       "  'toss_off',\n",
       "  'pop',\n",
       "  'bolt_down',\n",
       "  'belt_down',\n",
       "  'pour_down',\n",
       "  'drink_down',\n",
       "  'kill',\n",
       "  'devour',\n",
       "  'consume',\n",
       "  'go_through',\n",
       "  'shoot_down',\n",
       "  'land',\n",
       "  'knock_down',\n",
       "  'cut_down',\n",
       "  'push_down',\n",
       "  'pull_down',\n",
       "  'polish',\n",
       "  'refine',\n",
       "  'fine-tune',\n",
       "  'downward',\n",
       "  'down_pat',\n",
       "  'mastered',\n",
       "  'depressed',\n",
       "  'gloomy',\n",
       "  'grim',\n",
       "  'blue',\n",
       "  'dispirited',\n",
       "  'downcast',\n",
       "  'downhearted',\n",
       "  'down_in_the_mouth',\n",
       "  'low',\n",
       "  'low-spirited',\n",
       "  'downwards',\n",
       "  'downwardly'],\n",
       " 'in': ['inch',\n",
       "  'in',\n",
       "  'indium',\n",
       "  'In',\n",
       "  'atomic_number_49',\n",
       "  'Indiana',\n",
       "  'Hoosier_State',\n",
       "  'IN',\n",
       "  'inwards',\n",
       "  'inward'],\n",
       " 'in front of': [],\n",
       " 'inside': ['inside',\n",
       "  'interior',\n",
       "  'inner',\n",
       "  'privileged',\n",
       "  'indoors',\n",
       "  'within',\n",
       "  'inwardly',\n",
       "  'at_heart',\n",
       "  'at_bottom',\n",
       "  'deep_down',\n",
       "  'in_spite_of_appearance'],\n",
       " 'left': ['left',\n",
       "  'left_wing',\n",
       "  'left_hand',\n",
       "  'left_field',\n",
       "  'leftfield',\n",
       "  'leave',\n",
       "  'go_forth',\n",
       "  'go_away',\n",
       "  'leave_alone',\n",
       "  'leave_behind',\n",
       "  'exit',\n",
       "  'go_out',\n",
       "  'get_out',\n",
       "  'allow_for',\n",
       "  'allow',\n",
       "  'provide',\n",
       "  'result',\n",
       "  'lead',\n",
       "  'depart',\n",
       "  'pull_up_stakes',\n",
       "  'entrust',\n",
       "  'bequeath',\n",
       "  'will',\n",
       "  'impart',\n",
       "  'give',\n",
       "  'pass_on',\n",
       "  'forget',\n",
       "  'leftover',\n",
       "  'left_over',\n",
       "  'odd',\n",
       "  'remaining',\n",
       "  'unexpended',\n",
       "  'left-hand'],\n",
       " 'near': ['approach',\n",
       "  'near',\n",
       "  'come_on',\n",
       "  'go_up',\n",
       "  'draw_near',\n",
       "  'draw_close',\n",
       "  'come_near',\n",
       "  'close',\n",
       "  'nigh',\n",
       "  'cheeseparing',\n",
       "  'penny-pinching',\n",
       "  'skinny',\n",
       "  'dear',\n",
       "  'good',\n",
       "  'approximate',\n",
       "  'about',\n",
       "  'almost',\n",
       "  'most',\n",
       "  'nearly',\n",
       "  'virtually',\n",
       "  'well-nigh'],\n",
       " 'next to': [],\n",
       " 'off': ['murder',\n",
       "  'slay',\n",
       "  'hit',\n",
       "  'dispatch',\n",
       "  'bump_off',\n",
       "  'off',\n",
       "  'polish_off',\n",
       "  'remove',\n",
       "  'cancelled',\n",
       "  'sour',\n",
       "  'turned',\n",
       "  'away',\n",
       "  'forth'],\n",
       " 'on': ['on', 'along'],\n",
       " 'out': ['out',\n",
       "  'come_out_of_the_closet',\n",
       "  'come_out',\n",
       "  'extinct',\n",
       "  'forbidden',\n",
       "  'prohibited',\n",
       "  'proscribed',\n",
       "  'taboo',\n",
       "  'tabu',\n",
       "  'verboten',\n",
       "  'knocked_out',\n",
       "  'kayoed',\n",
       "  \"KO'd\",\n",
       "  'stunned',\n",
       "  'away'],\n",
       " 'outside': ['outside',\n",
       "  'exterior',\n",
       "  'external',\n",
       "  'extraneous',\n",
       "  'outdoor',\n",
       "  'out-of-door',\n",
       "  'international',\n",
       "  'remote',\n",
       "  'away',\n",
       "  'outdoors',\n",
       "  'out_of_doors',\n",
       "  'alfresco'],\n",
       " 'over': ['over',\n",
       "  'complete',\n",
       "  'concluded',\n",
       "  'ended',\n",
       "  'all_over',\n",
       "  'terminated',\n",
       "  \"o'er\"],\n",
       " 'past': ['past',\n",
       "  'past_times',\n",
       "  'yesteryear',\n",
       "  'past_tense',\n",
       "  'preceding',\n",
       "  'retiring',\n",
       "  'by'],\n",
       " 'right': ['right',\n",
       "  'right_field',\n",
       "  'rightfield',\n",
       "  'right_wing',\n",
       "  'right_hand',\n",
       "  'rightfulness',\n",
       "  'compensate',\n",
       "  'redress',\n",
       "  'correct',\n",
       "  'rectify',\n",
       "  'proper',\n",
       "  'right-hand',\n",
       "  'good',\n",
       "  'ripe',\n",
       "  'veracious',\n",
       "  'flop',\n",
       "  'properly',\n",
       "  'decently',\n",
       "  'decent',\n",
       "  'in_good_order',\n",
       "  'the_right_way',\n",
       "  'right_on',\n",
       "  'mighty',\n",
       "  'mightily',\n",
       "  'powerful',\n",
       "  'justly',\n",
       "  'correctly',\n",
       "  'aright'],\n",
       " 'through': ['done', 'through', 'through_with', 'through_and_through'],\n",
       " 'under': ['nether', 'under', 'below'],\n",
       " 'up': ['up', 'astir', 'improving', 'upward', 'upwards', 'upwardly'],\n",
       " 'upon': []}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms_atomic_p = {}\n",
    "for key in keys_sorted:\n",
    "    # collect all lemma names in a flat list\n",
    "    all_lemmas = []\n",
    "    for syn in wn.synsets(key):\n",
    "        all_lemmas.extend(syn.lemma_names())\n",
    "    # remove duplicates while preserving order\n",
    "    synonyms_atomic_p[key] = list(dict.fromkeys(all_lemmas))\n",
    "\n",
    "# inspect result\n",
    "synonyms_atomic_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f9e4da73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # synonyms atomic_p to json\n",
    "# with open('synonyms_atomic_p.json', 'w') as f:\n",
    "#     json.dump(synonyms_atomic_p, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0f49238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_atomic_p_prop(prop='', counter=5):\n",
    "# pp is a dict, access preposition as key\n",
    "    try:\n",
    "        if prop is not None and prop in ['isAtomicMorph', 'class', 'spellOutHEAD', 'path_p_morphology', 'measure_allowed']:\n",
    "            for key, value in pp['atomic_p'].items():\n",
    "                # print(f\"key: {key}\")\n",
    "                for el in value:\n",
    "                    if el == prop:\n",
    "                        print(f\"{key}: {pp['atomic_p'][key][el]} \")\n",
    "                        counter += 1\n",
    "                        if counter == 5:\n",
    "                            break\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e} not found in atomic_p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b81715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_wordnet_wiki_pop = []\n",
    "with open('dictionaries/pp_wordnet_wiki_pop.csv', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile, quotechar='|', dialect='excel')\n",
    "    for row in reader:\n",
    "        if row['preposition'] == '':\n",
    "            continue\n",
    "        pp_wordnet_wiki_pop.append({\n",
    "            'preposition': row['preposition'],\n",
    "            'isAtomic': row.get('is_atomic'),\n",
    "            'isSpatial': row.get('is_spatial')\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7877221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'preposition': 'a', 'isAtomic': 'TRUE', 'isSpatial': 'TRUE'},\n",
       " {'preposition': 'aboard', 'isAtomic': 'TRUE', 'isSpatial': 'TRUE'},\n",
       " {'preposition': 'about', 'isAtomic': 'FALSE', 'isSpatial': 'FALSE'},\n",
       " {'preposition': 'above', 'isAtomic': 'TRUE', 'isSpatial': 'TRUE'},\n",
       " {'preposition': 'absent', 'isAtomic': 'FALSE', 'isSpatial': 'FALSE'}]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pp_wordnet_wiki_pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a5578b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of unique_tokens before: 153\n",
      "numbers of non overlapping unique_tokens: 125\n",
      "back\n",
      "base\n",
      "opposed\n",
      "well\n",
      "astride\n",
      "adjacent\n",
      "center\n",
      "with\n",
      "respect\n",
      "higher\n",
      "top\n",
      "addition\n",
      "front\n",
      "except\n",
      "heart\n",
      "behest\n",
      "underside\n",
      "atop\n",
      "concerning\n",
      "of\n",
      "nigh\n",
      "opposite\n",
      "point\n",
      "regardless\n",
      "means\n",
      "ahead\n",
      "from\n",
      "most\n",
      "corner\n",
      "into\n",
      "within\n",
      "behalf\n",
      "virtue\n",
      "circa\n",
      "following\n",
      "besides\n",
      "minus\n",
      "regard\n",
      "view\n",
      "account\n",
      "face\n",
      "aboard\n",
      "save\n",
      "to\n",
      "via\n",
      "aside\n",
      "middle\n",
      "versus\n",
      "nearest\n",
      "bottom\n",
      "end\n",
      "afore\n",
      "flank\n",
      "apart\n",
      "soon\n",
      "subsequent\n",
      "least\n",
      "instead\n",
      "notwithstanding\n",
      "far\n",
      "foot\n",
      "worth\n",
      "cross\n",
      "such\n",
      "tween\n",
      "onto\n",
      "per\n",
      "edge\n",
      "regards\n",
      "throughout\n",
      "plus\n",
      "before\n",
      "prior\n",
      "thanks\n",
      "underneath\n",
      "case\n",
      "pursuant\n",
      "rather\n",
      "for\n",
      "rear\n",
      "betwixt\n",
      "sake\n",
      "during\n",
      "despite\n",
      "including\n",
      "spite\n",
      "according\n",
      "side\n",
      "a\n",
      "after\n",
      "owing\n",
      "rim\n",
      "next\n",
      "skin\n",
      "towards\n",
      "at\n",
      "until\n",
      "but\n",
      "upside\n",
      "nearer\n",
      "amongst\n",
      "by\n",
      "without\n",
      "because\n",
      "since\n",
      "due\n",
      "as\n",
      "beneath\n",
      "less\n",
      "like\n",
      "surface\n",
      "astern\n",
      "absent\n",
      "close\n",
      "about\n",
      "the\n",
      "core\n",
      "toward\n",
      "amid\n",
      "amidst\n",
      "alongside\n",
      "place\n",
      "accordance\n",
      "lieu\n",
      "than\n"
     ]
    }
   ],
   "source": [
    "unique_tokens = set() # unique token is defined as set of unique words in preposition\n",
    "def tokenize_preposition(preposition):\n",
    "    return preposition.split(' ')\n",
    "\n",
    "# Example usage\n",
    "for pp in pp_wordnet_wiki_pop:\n",
    "    tokens = tokenize_preposition(pp['preposition'])\n",
    "    # print(f\"Tokens for '{pp['preposition']}': {tokens}\")    \n",
    "    unique_tokens.update(tokens)\n",
    "\n",
    "print(f\"length of unique_tokens before: {len(unique_tokens)}\")\n",
    "\n",
    "c = 0\n",
    "unique_tokens_copy = unique_tokens.copy()\n",
    "for k in keys_sorted:\n",
    "    if k in unique_tokens_copy:\n",
    "        unique_tokens_copy.remove(k)\n",
    "        c += 1\n",
    "        # print(f\"{k} is in unique tokens\")\n",
    "    else:\n",
    "        # print(f\"{k} is NOT in unique tokens\")\n",
    "        continue\n",
    "print(f\"numbers of non overlapping unique_tokens: {len(unique_tokens_copy)}\")\n",
    "\n",
    "for k in unique_tokens_copy:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed47a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3164a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english_word(w):\n",
    "    w = w.lower()\n",
    "    return bool(wn.synsets(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baad4423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_preposition(preposition, unique_tokens, method='substring'):\n",
    "    \n",
    "    result = {}\n",
    "    p = preposition.lower()\n",
    "\n",
    "    if method == 'substring':\n",
    "        for token in unique_tokens:\n",
    "            t = token.lower()\n",
    "            if p not in t:\n",
    "                continue\n",
    "\n",
    "            count = t.count(p)\n",
    "\n",
    "            remainder = t.replace(p, \"\", 1)\n",
    "\n",
    "            # if remainder == '':\n",
    "            #     continue\n",
    "            if is_english_word(p) and is_english_word(remainder):\n",
    "                result[token] = {\n",
    "                    'decomposition': [p, remainder],\n",
    "                    'occurrence': count\n",
    "            }\n",
    "\n",
    "        return result\n",
    "    \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "60f0b66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "atomic = list(pp['atomic_p'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0fb5094e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preposition</th>\n",
       "      <th>token</th>\n",
       "      <th>decomposition</th>\n",
       "      <th>occurrence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>near</td>\n",
       "      <td>nearest</td>\n",
       "      <td>[near, est]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>near</td>\n",
       "      <td>nearer</td>\n",
       "      <td>[near, er]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>along</td>\n",
       "      <td>alongside</td>\n",
       "      <td>[along, side]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>through</td>\n",
       "      <td>throughout</td>\n",
       "      <td>[through, out]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>under</td>\n",
       "      <td>underside</td>\n",
       "      <td>[under, side]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>up</td>\n",
       "      <td>upside</td>\n",
       "      <td>[up, side]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>on</td>\n",
       "      <td>soon</td>\n",
       "      <td>[on, so]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>in</td>\n",
       "      <td>point</td>\n",
       "      <td>[in, pot]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>in</td>\n",
       "      <td>minus</td>\n",
       "      <td>[in, mus]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>in</td>\n",
       "      <td>instead</td>\n",
       "      <td>[in, stead]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>out</td>\n",
       "      <td>throughout</td>\n",
       "      <td>[out, through]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>out</td>\n",
       "      <td>about</td>\n",
       "      <td>[out, ab]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   preposition       token   decomposition  occurrence\n",
       "0         near     nearest     [near, est]           1\n",
       "1         near      nearer      [near, er]           1\n",
       "2        along   alongside   [along, side]           1\n",
       "3      through  throughout  [through, out]           1\n",
       "4        under   underside   [under, side]           1\n",
       "5           up      upside      [up, side]           1\n",
       "6           on        soon        [on, so]           1\n",
       "7           in       point       [in, pot]           1\n",
       "8           in       minus       [in, mus]           1\n",
       "9           in     instead     [in, stead]           1\n",
       "10         out  throughout  [out, through]           1\n",
       "11         out       about       [out, ab]           1"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect all decompositions\n",
    "result_decompose = {}\n",
    "for pp in atomic:\n",
    "    if pp in unique_tokens:\n",
    "        comps = decompose_preposition(pp, unique_tokens_copy, method='substring')\n",
    "        if comps:\n",
    "            result_decompose[pp] = comps\n",
    "\n",
    "# turn it into a flat table\n",
    "rows = []\n",
    "for preposition, comps in result_decompose.items():\n",
    "    for token, details in comps.items():\n",
    "        rows.append({\n",
    "            'preposition': preposition,\n",
    "            'token': token,\n",
    "            'decomposition': details['decomposition'],\n",
    "            'occurrence': details['occurrence']\n",
    "        })\n",
    "\n",
    "# dataFrame of all decompositions\n",
    "df_decompose = pandas.DataFrame(rows)\n",
    "df_decompose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5a5c1efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "\n",
    "# Create instances of each stemmer\n",
    "porter   = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "50646d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            index                0           1\n",
      "0            back             back        None\n",
      "1            base             base        None\n",
      "2           oppos          opposed        None\n",
      "3          behind           behind        None\n",
      "4            past             past        None\n",
      "5            well             well        None\n",
      "6          astrid          astride        None\n",
      "7           adjac         adjacent        None\n",
      "8          center           center        None\n",
      "9            with             with        None\n",
      "10        respect          respect        None\n",
      "11          along            along        None\n",
      "12            out              out        None\n",
      "13         higher           higher        None\n",
      "14            top              top        None\n",
      "15          addit         addition        None\n",
      "16          front            front        None\n",
      "17         except           except        None\n",
      "18          heart            heart        None\n",
      "19         behest           behest        None\n",
      "20       undersid        underside        None\n",
      "21          besid           beside     besides\n",
      "22           atop             atop        None\n",
      "23        concern       concerning        None\n",
      "24             of               of        None\n",
      "25           nigh             nigh        None\n",
      "26        opposit         opposite        None\n",
      "27          point            point        None\n",
      "28     regardless       regardless        None\n",
      "29           mean            means        None\n",
      "30          ahead            ahead        None\n",
      "31           from             from        None\n",
      "32           most             most        None\n",
      "33         corner           corner        None\n",
      "34           into             into        None\n",
      "35         within           within        None\n",
      "36         behalf           behalf        None\n",
      "37          virtu           virtue        None\n",
      "38          circa            circa        None\n",
      "39         follow        following        None\n",
      "40           minu            minus        None\n",
      "41           over             over        None\n",
      "42         regard           regard     regards\n",
      "43           down             down        None\n",
      "44           near             near        None\n",
      "45           view             view        None\n",
      "46        account          account        None\n",
      "47           face             face        None\n",
      "48          insid           inside        None\n",
      "49         aboard           aboard        None\n",
      "50           save             save        None\n",
      "51             to               to        None\n",
      "52            via              via        None\n",
      "53           asid            aside        None\n",
      "54          middl           middle        None\n",
      "55         outsid          outside        None\n",
      "56          versu           versus        None\n",
      "57        nearest          nearest        None\n",
      "58         bottom           bottom        None\n",
      "59             in               in        None\n",
      "60            end              end        None\n",
      "61           afor            afore        None\n",
      "62          flank            flank        None\n",
      "63          apart            apart        None\n",
      "64           soon             soon        None\n",
      "65        subsequ       subsequent        None\n",
      "66          least            least        None\n",
      "67        instead          instead        None\n",
      "68   notwithstand  notwithstanding        None\n",
      "69            far              far        None\n",
      "70           foot             foot        None\n",
      "71          worth            worth        None\n",
      "72          cross            cross        None\n",
      "73           such             such        None\n",
      "74          tween            tween        None\n",
      "75           onto             onto        None\n",
      "76            per              per        None\n",
      "77            edg             edge        None\n",
      "78             on               on        None\n",
      "79         across           across        None\n",
      "80     throughout       throughout        None\n",
      "81            plu             plus        None\n",
      "82          befor           before        None\n",
      "83          prior            prior        None\n",
      "84          thank           thanks        None\n",
      "85     underneath       underneath        None\n",
      "86           case             case        None\n",
      "87       pursuant         pursuant        None\n",
      "88          right            right        None\n",
      "89         rather           rather        None\n",
      "90        against          against        None\n",
      "91            for              for        None\n",
      "92        through          through        None\n",
      "93         beyond           beyond        None\n",
      "94           rear             rear        None\n",
      "95        betwixt          betwixt        None\n",
      "96           upon             upon        None\n",
      "97           sake             sake        None\n",
      "98           dure           during        None\n",
      "99         despit          despite        None\n",
      "100        includ        including        None\n",
      "101         spite            spite        None\n",
      "102        accord        according  accordance\n",
      "103         below            below        None\n",
      "104          side             side        None\n",
      "105             a                a        None\n",
      "106         after            after        None\n",
      "107          away             away        None\n",
      "108          left             left        None\n",
      "109           owe            owing        None\n",
      "110           rim              rim        None\n",
      "111       between          between        None\n",
      "112          next             next        None\n",
      "113         under            under        None\n",
      "114          skin             skin        None\n",
      "115        toward          towards      toward\n",
      "116            at               at        None\n",
      "117         until            until        None\n",
      "118         among            among        None\n",
      "119           but              but        None\n",
      "120         upsid           upside        None\n",
      "121        nearer           nearer        None\n",
      "122       amongst          amongst        None\n",
      "123            by               by        None\n",
      "124       without          without        None\n",
      "125          abov            above        None\n",
      "126        becaus          because        None\n",
      "127          sinc            since        None\n",
      "128           due              due        None\n",
      "129            as               as        None\n",
      "130       beneath          beneath        None\n",
      "131          less             less        None\n",
      "132          like             like        None\n",
      "133           off              off        None\n",
      "134        surfac          surface        None\n",
      "135        astern           astern        None\n",
      "136        absent           absent        None\n",
      "137         close            close        None\n",
      "138            up               up        None\n",
      "139         about            about        None\n",
      "140           the              the        None\n",
      "141          core             core        None\n",
      "142        around           around        None\n",
      "143          amid             amid        None\n",
      "144        amidst           amidst        None\n",
      "145      alongsid        alongside        None\n",
      "146         place            place        None\n",
      "147          lieu             lieu        None\n",
      "148          than             than        None\n"
     ]
    }
   ],
   "source": [
    "stemmer = [porter, lancaster, snowball]\n",
    "# build mapping token -> stem for each stemmer\n",
    "def build_stem_mapping(tokens, stemmer):\n",
    "    stem_mapping = {}\n",
    "    for token in tokens:\n",
    "        stem = stemmer.stem(token)\n",
    "        if stem not in stem_mapping:\n",
    "            stem_mapping[stem] = []\n",
    "        stem_mapping[stem].append(token)\n",
    "    return stem_mapping\n",
    "\n",
    "df_stem_porter = build_stem_mapping(unique_tokens, porter)\n",
    "df_stem_lancaster = build_stem_mapping(unique_tokens, lancaster)\n",
    "df_stem_snowball = build_stem_mapping(unique_tokens, snowball)\n",
    "# Convert the stem mappings to DataFrames\n",
    "df_stem_porter = pandas.DataFrame.from_dict(df_stem_porter, orient='index').reset_index()\n",
    "\n",
    "print(df_stem_porter.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b89189b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'back': ['back'],\n",
       " 'bas': ['base'],\n",
       " 'oppos': ['opposed'],\n",
       " 'behind': ['behind'],\n",
       " 'past': ['past'],\n",
       " 'wel': ['well'],\n",
       " 'astrid': ['astride'],\n",
       " 'adjac': ['adjacent'],\n",
       " 'cent': ['center'],\n",
       " 'with': ['with'],\n",
       " 'respect': ['respect'],\n",
       " 'along': ['along'],\n",
       " 'out': ['out'],\n",
       " 'high': ['higher'],\n",
       " 'top': ['top'],\n",
       " 'addit': ['addition'],\n",
       " 'front': ['front'],\n",
       " 'exceiv': ['except'],\n",
       " 'heart': ['heart'],\n",
       " 'behest': ['behest'],\n",
       " 'undersid': ['underside'],\n",
       " 'besid': ['beside', 'besides'],\n",
       " 'atop': ['atop'],\n",
       " 'concern': ['concerning'],\n",
       " 'of': ['of'],\n",
       " 'nigh': ['nigh'],\n",
       " 'opposit': ['opposite'],\n",
       " 'point': ['point'],\n",
       " 'regardless': ['regardless'],\n",
       " 'mean': ['means'],\n",
       " 'ahead': ['ahead'],\n",
       " 'from': ['from'],\n",
       " 'most': ['most'],\n",
       " 'corn': ['corner'],\n",
       " 'into': ['into'],\n",
       " 'within': ['within'],\n",
       " 'behalf': ['behalf'],\n",
       " 'virtu': ['virtue'],\n",
       " 'circ': ['circa'],\n",
       " 'follow': ['following'],\n",
       " 'min': ['minus'],\n",
       " 'ov': ['over'],\n",
       " 'regard': ['regard', 'regards'],\n",
       " 'down': ['down'],\n",
       " 'near': ['near', 'nearer'],\n",
       " 'view': ['view'],\n",
       " 'account': ['account'],\n",
       " 'fac': ['face'],\n",
       " 'insid': ['inside'],\n",
       " 'aboard': ['aboard'],\n",
       " 'sav': ['save'],\n",
       " 'to': ['to'],\n",
       " 'via': ['via'],\n",
       " 'asid': ['aside'],\n",
       " 'middl': ['middle'],\n",
       " 'outsid': ['outside'],\n",
       " 'vers': ['versus'],\n",
       " 'nearest': ['nearest'],\n",
       " 'bottom': ['bottom'],\n",
       " 'in': ['in'],\n",
       " 'end': ['end'],\n",
       " 'af': ['afore'],\n",
       " 'flank': ['flank'],\n",
       " 'apart': ['apart'],\n",
       " 'soon': ['soon'],\n",
       " 'subsequ': ['subsequent'],\n",
       " 'least': ['least'],\n",
       " 'instead': ['instead'],\n",
       " 'notwithstand': ['notwithstanding'],\n",
       " 'far': ['far'],\n",
       " 'foot': ['foot'],\n",
       " 'wor': ['worth'],\n",
       " 'cross': ['cross'],\n",
       " 'such': ['such'],\n",
       " 'tween': ['tween'],\n",
       " 'onto': ['onto'],\n",
       " 'per': ['per'],\n",
       " 'edg': ['edge'],\n",
       " 'on': ['on'],\n",
       " 'across': ['across'],\n",
       " 'throughout': ['throughout'],\n",
       " 'plu': ['plus'],\n",
       " 'bef': ['before'],\n",
       " 'pri': ['prior'],\n",
       " 'thank': ['thanks'],\n",
       " 'undernea': ['underneath'],\n",
       " 'cas': ['case'],\n",
       " 'pursu': ['pursuant'],\n",
       " 'right': ['right'],\n",
       " 'rath': ['rather'],\n",
       " 'against': ['against'],\n",
       " 'for': ['for'],\n",
       " 'through': ['through'],\n",
       " 'beyond': ['beyond'],\n",
       " 'rear': ['rear'],\n",
       " 'betwixt': ['betwixt'],\n",
       " 'upon': ['upon'],\n",
       " 'sak': ['sake'],\n",
       " 'dur': ['during'],\n",
       " 'despit': ['despite'],\n",
       " 'includ': ['including'],\n",
       " 'spit': ['spite'],\n",
       " 'accord': ['according', 'accordance'],\n",
       " 'below': ['below'],\n",
       " 'sid': ['side'],\n",
       " 'a': ['a'],\n",
       " 'aft': ['after'],\n",
       " 'away': ['away'],\n",
       " 'left': ['left'],\n",
       " 'ow': ['owing'],\n",
       " 'rim': ['rim'],\n",
       " 'between': ['between'],\n",
       " 'next': ['next'],\n",
       " 'und': ['under'],\n",
       " 'skin': ['skin'],\n",
       " 'toward': ['towards', 'toward'],\n",
       " 'at': ['at'],\n",
       " 'until': ['until'],\n",
       " 'among': ['among'],\n",
       " 'but': ['but'],\n",
       " 'upsid': ['upside'],\n",
       " 'amongst': ['amongst'],\n",
       " 'by': ['by'],\n",
       " 'without': ['without'],\n",
       " 'abov': ['above'],\n",
       " 'becaus': ['because'],\n",
       " 'sint': ['since'],\n",
       " 'due': ['due'],\n",
       " 'as': ['as'],\n",
       " 'benea': ['beneath'],\n",
       " 'less': ['less'],\n",
       " 'lik': ['like'],\n",
       " 'off': ['off'],\n",
       " 'surfac': ['surface'],\n",
       " 'astern': ['astern'],\n",
       " 'abs': ['absent'],\n",
       " 'clos': ['close'],\n",
       " 'up': ['up'],\n",
       " 'about': ['about'],\n",
       " 'the': ['the'],\n",
       " 'cor': ['core'],\n",
       " 'around': ['around'],\n",
       " 'amid': ['amid'],\n",
       " 'amidst': ['amidst'],\n",
       " 'alongsid': ['alongside'],\n",
       " 'plac': ['place'],\n",
       " 'lieu': ['lieu'],\n",
       " 'than': ['than']}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stem_lancaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "dd5cd9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'back': ['back'],\n",
       " 'base': ['base'],\n",
       " 'oppos': ['opposed'],\n",
       " 'behind': ['behind'],\n",
       " 'past': ['past'],\n",
       " 'well': ['well'],\n",
       " 'astrid': ['astride'],\n",
       " 'adjac': ['adjacent'],\n",
       " 'center': ['center'],\n",
       " 'with': ['with'],\n",
       " 'respect': ['respect'],\n",
       " 'along': ['along'],\n",
       " 'out': ['out'],\n",
       " 'higher': ['higher'],\n",
       " 'top': ['top'],\n",
       " 'addit': ['addition'],\n",
       " 'front': ['front'],\n",
       " 'except': ['except'],\n",
       " 'heart': ['heart'],\n",
       " 'behest': ['behest'],\n",
       " 'undersid': ['underside'],\n",
       " 'besid': ['beside', 'besides'],\n",
       " 'atop': ['atop'],\n",
       " 'concern': ['concerning'],\n",
       " 'of': ['of'],\n",
       " 'nigh': ['nigh'],\n",
       " 'opposit': ['opposite'],\n",
       " 'point': ['point'],\n",
       " 'regardless': ['regardless'],\n",
       " 'mean': ['means'],\n",
       " 'ahead': ['ahead'],\n",
       " 'from': ['from'],\n",
       " 'most': ['most'],\n",
       " 'corner': ['corner'],\n",
       " 'into': ['into'],\n",
       " 'within': ['within'],\n",
       " 'behalf': ['behalf'],\n",
       " 'virtu': ['virtue'],\n",
       " 'circa': ['circa'],\n",
       " 'follow': ['following'],\n",
       " 'minus': ['minus'],\n",
       " 'over': ['over'],\n",
       " 'regard': ['regard', 'regards'],\n",
       " 'down': ['down'],\n",
       " 'near': ['near'],\n",
       " 'view': ['view'],\n",
       " 'account': ['account'],\n",
       " 'face': ['face'],\n",
       " 'insid': ['inside'],\n",
       " 'aboard': ['aboard'],\n",
       " 'save': ['save'],\n",
       " 'to': ['to'],\n",
       " 'via': ['via'],\n",
       " 'asid': ['aside'],\n",
       " 'middl': ['middle'],\n",
       " 'outsid': ['outside'],\n",
       " 'versus': ['versus'],\n",
       " 'nearest': ['nearest'],\n",
       " 'bottom': ['bottom'],\n",
       " 'in': ['in'],\n",
       " 'end': ['end'],\n",
       " 'afor': ['afore'],\n",
       " 'flank': ['flank'],\n",
       " 'apart': ['apart'],\n",
       " 'soon': ['soon'],\n",
       " 'subsequ': ['subsequent'],\n",
       " 'least': ['least'],\n",
       " 'instead': ['instead'],\n",
       " 'notwithstand': ['notwithstanding'],\n",
       " 'far': ['far'],\n",
       " 'foot': ['foot'],\n",
       " 'worth': ['worth'],\n",
       " 'cross': ['cross'],\n",
       " 'such': ['such'],\n",
       " 'tween': ['tween'],\n",
       " 'onto': ['onto'],\n",
       " 'per': ['per'],\n",
       " 'edg': ['edge'],\n",
       " 'on': ['on'],\n",
       " 'across': ['across'],\n",
       " 'throughout': ['throughout'],\n",
       " 'plus': ['plus'],\n",
       " 'befor': ['before'],\n",
       " 'prior': ['prior'],\n",
       " 'thank': ['thanks'],\n",
       " 'underneath': ['underneath'],\n",
       " 'case': ['case'],\n",
       " 'pursuant': ['pursuant'],\n",
       " 'right': ['right'],\n",
       " 'rather': ['rather'],\n",
       " 'against': ['against'],\n",
       " 'for': ['for'],\n",
       " 'through': ['through'],\n",
       " 'beyond': ['beyond'],\n",
       " 'rear': ['rear'],\n",
       " 'betwixt': ['betwixt'],\n",
       " 'upon': ['upon'],\n",
       " 'sake': ['sake'],\n",
       " 'dure': ['during'],\n",
       " 'despit': ['despite'],\n",
       " 'includ': ['including'],\n",
       " 'spite': ['spite'],\n",
       " 'accord': ['according', 'accordance'],\n",
       " 'below': ['below'],\n",
       " 'side': ['side'],\n",
       " 'a': ['a'],\n",
       " 'after': ['after'],\n",
       " 'away': ['away'],\n",
       " 'left': ['left'],\n",
       " 'owe': ['owing'],\n",
       " 'rim': ['rim'],\n",
       " 'between': ['between'],\n",
       " 'next': ['next'],\n",
       " 'under': ['under'],\n",
       " 'skin': ['skin'],\n",
       " 'toward': ['towards', 'toward'],\n",
       " 'at': ['at'],\n",
       " 'until': ['until'],\n",
       " 'among': ['among'],\n",
       " 'but': ['but'],\n",
       " 'upsid': ['upside'],\n",
       " 'nearer': ['nearer'],\n",
       " 'amongst': ['amongst'],\n",
       " 'by': ['by'],\n",
       " 'without': ['without'],\n",
       " 'abov': ['above'],\n",
       " 'becaus': ['because'],\n",
       " 'sinc': ['since'],\n",
       " 'due': ['due'],\n",
       " 'as': ['as'],\n",
       " 'beneath': ['beneath'],\n",
       " 'less': ['less'],\n",
       " 'like': ['like'],\n",
       " 'off': ['off'],\n",
       " 'surfac': ['surface'],\n",
       " 'astern': ['astern'],\n",
       " 'absent': ['absent'],\n",
       " 'close': ['close'],\n",
       " 'up': ['up'],\n",
       " 'about': ['about'],\n",
       " 'the': ['the'],\n",
       " 'core': ['core'],\n",
       " 'around': ['around'],\n",
       " 'amid': ['amid'],\n",
       " 'amidst': ['amidst'],\n",
       " 'alongsid': ['alongside'],\n",
       " 'place': ['place'],\n",
       " 'lieu': ['lieu'],\n",
       " 'than': ['than']}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stem_snowball"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
