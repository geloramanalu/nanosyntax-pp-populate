{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eef0c69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from nltk.grammar import CFG, Nonterminal, Production\n",
    "from nltk.parse import EarleyChartParser\n",
    "import re\n",
    "from nltk.tree import Tree\n",
    "from collections import defaultdict, deque\n",
    "from benepar import Parser\n",
    "from nltk.grammar import induce_pcfg, PCFG, ProbabilisticProduction\n",
    "from nltk.parse.viterbi import ViterbiParser\n",
    "from tqdm.notebook import tqdm\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('averaged_perceptron_tagger')  \n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10a9a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<benepar.integrations.spacy_plugin.BeneparComponent at 0x15f732dc0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# install the neural parser backend (T5Model etc.)\n",
    "\n",
    "# pip install benepar | collecting huggingface-hub<1.0,>=0.16.4\n",
    "import spacy, benepar\n",
    "\n",
    "\n",
    "# benepar.download(\"benepar_en3\")\n",
    "\n",
    "# load and init spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\") #python -m spacy download en_core_web_sm\n",
    "\n",
    "# add benepar to the pipeline\n",
    "nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8934ae",
   "metadata": {},
   "source": [
    "# Derivative Decomposition of Complex Prepositional Phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8ab47b",
   "metadata": {},
   "source": [
    "## Step 1: initialize master lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c446a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_pp_lexicon = {}\n",
    "# Populate from atomic_p.json\n",
    "with open('pp_lexicon/atomic_p.json', 'r') as f:\n",
    "    atomic_p = json.load(f)\n",
    "    for preposition, features in atomic_p.items():\n",
    "        master_pp_lexicon[preposition] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "522f4038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate from p_lexicon.json\n",
    "with open('pp_lexicon/p_lexicon.json', 'r') as f:\n",
    "    p_lexicon = json.load(f)\n",
    "    for preposition, features in p_lexicon.items():\n",
    "            master_pp_lexicon[preposition] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "644b165c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(master_pp_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc253361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['behind', 'above', 'below', 'beyond', 'in front of', 'inside', 'outside', 'left', 'right', 'against', 'among', 'beside', 'between', 'near', 'next to', 'upon', 'across', 'along', 'around', 'over', 'past', 'through', 'under', 'up', 'down', 'on', 'off', 'in', 'out', 'away', 'top', 'astride', 'corner', 'by', 'edge', 'after', 'but', 'end', 'for', 'base', 'higher', 'high', 'bottom', 'via', 'back', 'at', 'astern', 'following', 'center', 'prior', 'front', 'before', 'opposite', 'from', 'beneath', 'with', 'rear', 'apart', 'next', 'side', 'of', 'far', 'amid', 'skin', 'flank', 'part', 'to', 'board', 'surface', 'a', 'amidst', 'middle', 'rim', 'face', 'close', 'core', 'foot', 'subsequent', 'heart'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_pp_lexicon.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8d3c32",
   "metadata": {},
   "source": [
    "## Step 2: add complex preposition list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29bfd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp_wordnet = pd.read_csv('dictionaries/pp_wordnet_dict_wiki_pop_fix.csv', sep=',')\n",
    "pp_wordnet_wiki = list(df_pp_wordnet['preposition'])\n",
    "# # drop nan in pp_wordnet_wiki\n",
    "pp_wordnet_wiki = [x for x in pp_wordnet_wiki if x is not None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe699622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_pp_list = []\n",
    "# check pp_wordnet_wiki against master_pp_lexicon keys\n",
    "for preposition in pp_wordnet_wiki:\n",
    "    if preposition not in master_pp_lexicon:\n",
    "        complex_pp_list.append(preposition)\n",
    "\n",
    "# remove nan type() float in complex_pp_list\n",
    "complex_pp_list = [x for x in complex_pp_list if type(x) is str]\n",
    "complex_pp_list = list(set(complex_pp_list))  # remove duplicates\n",
    "complex_pp_list.sort()\n",
    "len(complex_pp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b6624fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# add empty p_lexicon.json entry to complex_pp_list\n",
    "counter = 0\n",
    "for key, entry in p_lexicon.items():\n",
    "    if entry[\"isAtomicMorph\"] == \"\" and entry[\"class\"] == \"\" and entry[\"path_p_morphology\"] == \"\" and entry[\"measure_allowed\"] == \"\" and entry[\"spellOutHEAD\"] == [\"\"]:\n",
    "        complex_pp_list.append(key)\n",
    "        counter += 1\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56161d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('pp_lexicon/complex_pp_repop.json', 'w') as f:\n",
    "    json.dump(complex_pp_list, f, indent=4)\n",
    "    \n",
    "len(complex_pp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "972565ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['until',\n",
       " 'up to',\n",
       " 'upside',\n",
       " 'versus',\n",
       " 'with a view to',\n",
       " 'with regard to',\n",
       " 'with respect to',\n",
       " 'within',\n",
       " 'without',\n",
       " 'worth']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'the' in complex_pp_list:\n",
    "    complex_pp_list.remove('the')\n",
    "complex_pp_list[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2ad8dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_result = {} # dictionary to store deomposition results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b3cb0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "underside\n",
      "cross\n"
     ]
    }
   ],
   "source": [
    "with open('pp_lexicon/axial_words.json', 'r') as f:\n",
    "    axial_words = json.load(f)\n",
    "\n",
    "for ax in axial_words:\n",
    "    if ax not in master_pp_lexicon.keys():\n",
    "        print(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f293509d",
   "metadata": {},
   "source": [
    "### get empty list lexicon in p_lexicon.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d81a781f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# get empty or \"\" entries from p_lexicon.json\n",
    "empty_entries = []\n",
    "for key, entry in p_lexicon.items():\n",
    "    if entry[\"isAtomicMorph\"] == \"\" and entry[\"class\"] == \"\" and entry[\"path_p_morphology\"] == \"\" and entry[\"measure_allowed\"] == \"\" and entry[\"spellOutHEAD\"] == [\"\"]:\n",
    "        empty_entries.append(key)\n",
    "    \n",
    "print(len(empty_entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e26e3c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy p_lexicon.json to dictionary\n",
    "filled_p_lexicon = {}\n",
    "\n",
    "for key, entry in p_lexicon.items():\n",
    "    if entry[\"isAtomicMorph\"] == \"\" and entry[\"class\"] == \"\" and entry[\"path_p_morphology\"] == \"\" and entry[\"measure_allowed\"] == \"\" and entry[\"spellOutHEAD\"] == [\"\"]:\n",
    "        continue\n",
    "    else:\n",
    "        filled_p_lexicon[key] = entry\n",
    "        \n",
    "len(filled_p_lexicon.keys())\n",
    "\n",
    "# with open('pp_lexicon/p_lexicon.json', 'w') as f:\n",
    "#     json.dump(filled_p_lexicon, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172ce435",
   "metadata": {},
   "source": [
    "## Step 3: Decomposition Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "471dafa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abstract_domain_classes import PPFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c169fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aboard': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED'],\n",
       "  'spellOutHEAD': ['*K',\n",
       "   {'AxPart': '[SURFACE]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   '*p',\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC'],\n",
       "  'measure_allowed': False},\n",
       " 'about': {'isAtomicMorph': False,\n",
       "  'class': ['NOT_SPATIAL'],\n",
       "  'spellOutHEAD': [],\n",
       "  'path_p_morphology': ['none'],\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['bout']},\n",
       " 'absent': {'isAtomicMorph': False,\n",
       "  'class': ['NOT_SPATIAL'],\n",
       "  'spellOutHEAD': [],\n",
       "  'path_p_morphology': ['none'],\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['bsent']},\n",
       " 'according to': {'isAtomicMorph': False,\n",
       "  'class': ['EXTENDED'],\n",
       "  'spellOutHEAD': ['*K', '*AxPart', '*Deg', '*Proj', '*p', {'Path': 'to'}],\n",
       "  'path_p_morphology': ['GOAL'],\n",
       "  'measure_allowed': True,\n",
       "  'unlexicalized': ['ccording']},\n",
       " 'adjacent': {'isAtomicMorph': False,\n",
       "  'class': ['NOT_SPATIAL'],\n",
       "  'spellOutHEAD': [],\n",
       "  'path_p_morphology': ['none'],\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['djacent']},\n",
       " 'adjacent to': {'isAtomicMorph': False,\n",
       "  'class': ['EXTENDED'],\n",
       "  'spellOutHEAD': ['*K', '*AxPart', '*Deg', '*Proj', '*p', {'Path': 'to'}],\n",
       "  'path_p_morphology': ['GOAL'],\n",
       "  'measure_allowed': True,\n",
       "  'unlexicalized': ['djacent']},\n",
       " 'afore': {'isAtomicMorph': False,\n",
       "  'class': ['EXTENDED', 'BOUNDED'],\n",
       "  'spellOutHEAD': ['*K',\n",
       "   {'AxPart': '[BIFURCATION]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'astride'},\n",
       "   {'Path': 'for'}],\n",
       "  'path_p_morphology': ['GOAL', 'LOC'],\n",
       "  'measure_allowed': True},\n",
       " 'ahead': {'isAtomicMorph': False,\n",
       "  'class': ['NOT_SPATIAL'],\n",
       "  'spellOutHEAD': [],\n",
       "  'path_p_morphology': ['none'],\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['head']},\n",
       " 'ahead of': {'isAtomicMorph': False,\n",
       "  'class': ['NOT_SPATIAL', 'NOT_SPATIAL'],\n",
       "  'spellOutHEAD': [{'K': 'of'}],\n",
       "  'path_p_morphology': ['none'],\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['head']},\n",
       " 'alongside': {'isAtomicMorph': False,\n",
       "  'class': ['EXTENDED', 'PROJECTIVE'],\n",
       "  'spellOutHEAD': [{'K': 'along'},\n",
       "   {'K': 'side'},\n",
       "   {'AxPart': '[EDGE]'},\n",
       "   {'AxPart': '[SIDE]'},\n",
       "   '*Deg',\n",
       "   '*Proj'],\n",
       "  'path_p_morphology': ['LOC'],\n",
       "  'measure_allowed': True},\n",
       " 'amongst': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED', 'BOUNDED'],\n",
       "  'spellOutHEAD': [{'*Deg': '[PROXIMAL]'},\n",
       "   {'K': 'among'},\n",
       "   {'AxPart': '[BIFURCATION]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'among'},\n",
       "   {'p': 'astride'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC'],\n",
       "  'measure_allowed': False},\n",
       " 'apart from': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED', 'EXTENDED'],\n",
       "  'spellOutHEAD': ['*K',\n",
       "   {'AxPart': '[OUT]'},\n",
       "   {'Deg': '[DISTAL]'},\n",
       "   '*Proj',\n",
       "   {'p': 'apart'},\n",
       "   {'Path': 'from'}],\n",
       "  'path_p_morphology': ['LOC', 'SOURCE'],\n",
       "  'measure_allowed': True},\n",
       " 'as': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED'],\n",
       "  'spellOutHEAD': ['*K',\n",
       "   {'AxPart': '[BIFURCATION]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'astride'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC'],\n",
       "  'measure_allowed': False},\n",
       " 'as far as': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED', 'BOUNDED', 'BOUNDED'],\n",
       "  'spellOutHEAD': [{'K': 'far'},\n",
       "   {'AxPart': '[BIFURCATION]'},\n",
       "   {'Deg': '[DISTAL]'},\n",
       "   '*Proj',\n",
       "   {'p': 'astride'},\n",
       "   {'p': 'far'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC'],\n",
       "  'measure_allowed': False},\n",
       " 'as of': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED'],\n",
       "  'spellOutHEAD': [{'K': 'of'},\n",
       "   {'AxPart': '[BIFURCATION]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'astride'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC'],\n",
       "  'measure_allowed': False},\n",
       " 'as opposed to': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED', 'EXTENDED'],\n",
       "  'spellOutHEAD': ['*K',\n",
       "   {'AxPart': '[BIFURCATION]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'astride'},\n",
       "   {'Path': 'to'}],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': True,\n",
       "  'unlexicalized': ['opposed']},\n",
       " 'as per': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED'],\n",
       "  'spellOutHEAD': ['*K',\n",
       "   {'AxPart': '[BIFURCATION]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'astride'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC'],\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['per']},\n",
       " 'as regards': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED'],\n",
       "  'spellOutHEAD': ['*K',\n",
       "   {'AxPart': '[BIFURCATION]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'astride'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC'],\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['regards']},\n",
       " 'as soon as': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED', 'BOUNDED'],\n",
       "  'spellOutHEAD': ['*K',\n",
       "   {'AxPart': '[BIFURCATION]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'astride'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC'],\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['soon']},\n",
       " 'as well as': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED', 'BOUNDED'],\n",
       "  'spellOutHEAD': ['*K',\n",
       "   {'AxPart': '[BIFURCATION]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'astride'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC'],\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['well']},\n",
       " 'aside': {'isAtomicMorph': False,\n",
       "  'class': ['PROJECTIVE'],\n",
       "  'spellOutHEAD': [{'K': 'side'}, {'AxPart': '[SIDE]'}, '*Deg', '*Proj'],\n",
       "  'path_p_morphology': ['LOC'],\n",
       "  'measure_allowed': True},\n",
       " 'aside from': {'isAtomicMorph': False,\n",
       "  'class': ['PROJECTIVE', 'EXTENDED'],\n",
       "  'spellOutHEAD': [{'K': 'side'},\n",
       "   {'AxPart': '[SIDE]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   '*p',\n",
       "   {'Path': 'from'}],\n",
       "  'path_p_morphology': ['LOC', 'SOURCE'],\n",
       "  'measure_allowed': True},\n",
       " 'astern of': {'isAtomicMorph': False,\n",
       "  'class': ['PROJECTIVE'],\n",
       "  'spellOutHEAD': [{'K': 'astern'},\n",
       "   {'K': 'of'},\n",
       "   {'AxPart': '[BACK_NAUTICAL]'},\n",
       "   '*Deg',\n",
       "   '*Proj'],\n",
       "  'path_p_morphology': ['LOC'],\n",
       "  'measure_allowed': True},\n",
       " 'at least': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED'],\n",
       "  'spellOutHEAD': ['*K', '*AxPart', '*Deg', '*Proj', {'p': 'at'}, '*Path'],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['least']},\n",
       " 'at most': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED'],\n",
       "  'spellOutHEAD': ['*K', '*AxPart', '*Deg', '*Proj', {'p': 'at'}, '*Path'],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['most']},\n",
       " 'at the back of': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED', 'PROJECTIVE'],\n",
       "  'spellOutHEAD': [{'K': 'back'},\n",
       "   {'K': 'of'},\n",
       "   {'AxPart': '[BACK]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'at'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': True,\n",
       "  'unlexicalized': ['the']},\n",
       " 'at the behest of': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED'],\n",
       "  'spellOutHEAD': [{'K': 'of'},\n",
       "   '*AxPart',\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'at'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['the', 'behest']},\n",
       " 'at the rear of': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED', 'PROJECTIVE'],\n",
       "  'spellOutHEAD': [{'K': 'rear'},\n",
       "   {'K': 'of'},\n",
       "   {'AxPart': '[BACK]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'at'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': True,\n",
       "  'unlexicalized': ['the']},\n",
       " 'atop': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED', 'BOUNDED'],\n",
       "  'spellOutHEAD': ['*K',\n",
       "   {'AxPart': '[TOP]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'at'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': False},\n",
       " 'back to': {'isAtomicMorph': False,\n",
       "  'class': ['PROJECTIVE', 'EXTENDED'],\n",
       "  'spellOutHEAD': [{'K': 'back'},\n",
       "   {'AxPart': '[BACK]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   '*p',\n",
       "   {'Path': 'to'}],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': True},\n",
       " 'because of': {'isAtomicMorph': False,\n",
       "  'class': ['NOT_SPATIAL'],\n",
       "  'spellOutHEAD': [{'K': 'of'}],\n",
       "  'path_p_morphology': ['none'],\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['because']},\n",
       " 'besides': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED', 'BOUNDED'],\n",
       "  'spellOutHEAD': [{'*Deg': '[PROXIMAL]'},\n",
       "   {'K': 'beside'},\n",
       "   {'AxPart': '[SIDE]'},\n",
       "   {'AxPart': '[BIFURCATION]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'astride'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC'],\n",
       "  'measure_allowed': False},\n",
       " 'betwixt': {'isAtomicMorph': False,\n",
       "  'class': None,\n",
       "  'spellOutHEAD': [],\n",
       "  'path_p_morphology': None,\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['betwixt']},\n",
       " 'by means of': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED'],\n",
       "  'spellOutHEAD': [{'K': 'of'},\n",
       "   '*AxPart',\n",
       "   {'Deg': '[PROXIMAL]'},\n",
       "   '*Proj',\n",
       "   {'p': 'by'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC', 'ROUTE'],\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['means']},\n",
       " 'by virtue of': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED'],\n",
       "  'spellOutHEAD': [{'K': 'of'},\n",
       "   '*AxPart',\n",
       "   {'Deg': '[PROXIMAL]'},\n",
       "   '*Proj',\n",
       "   {'p': 'by'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC', 'ROUTE'],\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['virtue']},\n",
       " 'circa': {'isAtomicMorph': False,\n",
       "  'class': None,\n",
       "  'spellOutHEAD': [],\n",
       "  'path_p_morphology': None,\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['circa']},\n",
       " 'close to': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED', 'EXTENDED'],\n",
       "  'spellOutHEAD': [{'K': 'close'},\n",
       "   '*AxPart',\n",
       "   {'Deg': '[PROXIMAL]'},\n",
       "   '*Proj',\n",
       "   {'p': 'close'},\n",
       "   {'Path': 'to'}],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': True},\n",
       " 'concerning': {'isAtomicMorph': False,\n",
       "  'class': None,\n",
       "  'spellOutHEAD': [],\n",
       "  'path_p_morphology': None,\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['concerning']},\n",
       " 'cross': {'isAtomicMorph': False,\n",
       "  'class': ['EXTENDED'],\n",
       "  'spellOutHEAD': [{'K': 'across'}, {'AxPart': '[EDGE]'}, '*Deg', '*Proj'],\n",
       "  'path_p_morphology': ['GOAL'],\n",
       "  'measure_allowed': True},\n",
       " 'despite': {'isAtomicMorph': False,\n",
       "  'class': None,\n",
       "  'spellOutHEAD': [],\n",
       "  'path_p_morphology': None,\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['despite']},\n",
       " 'due to': {'isAtomicMorph': False,\n",
       "  'class': ['EXTENDED'],\n",
       "  'spellOutHEAD': ['*K', '*AxPart', '*Deg', '*Proj', '*p', {'Path': 'to'}],\n",
       "  'path_p_morphology': ['GOAL'],\n",
       "  'measure_allowed': True,\n",
       "  'unlexicalized': ['due']},\n",
       " 'during': {'isAtomicMorph': False,\n",
       "  'class': None,\n",
       "  'spellOutHEAD': [],\n",
       "  'path_p_morphology': None,\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['during']},\n",
       " 'except': {'isAtomicMorph': False,\n",
       "  'class': None,\n",
       "  'spellOutHEAD': [],\n",
       "  'path_p_morphology': None,\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['except']},\n",
       " 'except for': {'isAtomicMorph': False,\n",
       "  'class': ['EXTENDED'],\n",
       "  'spellOutHEAD': ['*K', '*AxPart', '*Deg', '*Proj', '*p', {'Path': 'for'}],\n",
       "  'path_p_morphology': ['GOAL'],\n",
       "  'measure_allowed': True,\n",
       "  'unlexicalized': ['except']},\n",
       " 'far from': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED', 'EXTENDED'],\n",
       "  'spellOutHEAD': [{'K': 'far'},\n",
       "   '*AxPart',\n",
       "   {'Deg': '[DISTAL]'},\n",
       "   '*Proj',\n",
       "   {'p': 'far'},\n",
       "   {'Path': 'from'}],\n",
       "  'path_p_morphology': ['LOC', 'SOURCE'],\n",
       "  'measure_allowed': True},\n",
       " 'for the sake of': {'isAtomicMorph': False,\n",
       "  'class': ['EXTENDED'],\n",
       "  'spellOutHEAD': [{'K': 'of'},\n",
       "   '*AxPart',\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   '*p',\n",
       "   {'Path': 'for'}],\n",
       "  'path_p_morphology': ['GOAL'],\n",
       "  'measure_allowed': True,\n",
       "  'unlexicalized': ['the', 'sake']},\n",
       " 'from behind': {'isAtomicMorph': False,\n",
       "  'class': ['EXTENDED', 'PROJECTIVE'],\n",
       "  'spellOutHEAD': [{'K': 'behind'},\n",
       "   {'AxPart': '[BACK]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   '*p',\n",
       "   {'Path': 'from'}],\n",
       "  'path_p_morphology': ['SOURCE', 'LOC', 'GOAL'],\n",
       "  'measure_allowed': True},\n",
       " 'from under': {'isAtomicMorph': False,\n",
       "  'class': ['EXTENDED', 'EXTENDED'],\n",
       "  'spellOutHEAD': [{'K': 'under'},\n",
       "   {'AxPart': '[DOWN]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   '*p',\n",
       "   {'Path': 'from'}],\n",
       "  'path_p_morphology': ['SOURCE', 'LOC'],\n",
       "  'measure_allowed': True},\n",
       " 'in a higher place': {'isAtomicMorph': False,\n",
       "  'class': ['PARTICLE', 'PROJECTIVE'],\n",
       "  'spellOutHEAD': ['*Det',\n",
       "   {'K': 'higher'},\n",
       "   {'AxPart': '[UP]'},\n",
       "   {'Deg': ['[GRADIENT]', '[POSITIVE]']},\n",
       "   '*Proj',\n",
       "   {'p': 'in'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': True,\n",
       "  'unlexicalized': ['place']},\n",
       " 'in accordance with': {'isAtomicMorph': False,\n",
       "  'class': ['PARTICLE', 'BOUNDED'],\n",
       "  'spellOutHEAD': ['*Det',\n",
       "   {'K': 'with'},\n",
       "   '*AxPart',\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'in'},\n",
       "   {'p': 'with'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['ccordance']},\n",
       " 'in addition to': {'isAtomicMorph': False,\n",
       "  'class': ['PARTICLE', 'EXTENDED'],\n",
       "  'spellOutHEAD': ['*Det',\n",
       "   '*K',\n",
       "   '*AxPart',\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'in'},\n",
       "   {'Path': 'to'}],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': True,\n",
       "  'unlexicalized': ['ddition']},\n",
       " 'in case of': {'isAtomicMorph': False,\n",
       "  'class': ['PARTICLE'],\n",
       "  'spellOutHEAD': ['*Det',\n",
       "   {'K': 'of'},\n",
       "   '*AxPart',\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'in'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['case']},\n",
       " 'in lieu of': {'isAtomicMorph': False,\n",
       "  'class': ['PARTICLE'],\n",
       "  'spellOutHEAD': ['*Det',\n",
       "   {'K': 'of'},\n",
       "   '*AxPart',\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'in'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['lieu']},\n",
       " 'in place of': {'isAtomicMorph': False,\n",
       "  'class': ['PARTICLE'],\n",
       "  'spellOutHEAD': ['*Det',\n",
       "   {'K': 'of'},\n",
       "   '*AxPart',\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'in'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['place']},\n",
       " 'in point of': {'isAtomicMorph': False,\n",
       "  'class': ['PARTICLE'],\n",
       "  'spellOutHEAD': ['*Det',\n",
       "   {'K': 'of'},\n",
       "   '*AxPart',\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'in'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['point']},\n",
       " 'in spite of': {'isAtomicMorph': False,\n",
       "  'class': ['PARTICLE'],\n",
       "  'spellOutHEAD': ['*Det',\n",
       "   {'K': 'of'},\n",
       "   '*AxPart',\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'in'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['spite']},\n",
       " 'including': {'isAtomicMorph': False,\n",
       "  'class': ['PARTICLE'],\n",
       "  'spellOutHEAD': ['*Det',\n",
       "   '*K',\n",
       "   '*AxPart',\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'in'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['cluding']},\n",
       " 'inside of': {'isAtomicMorph': False,\n",
       "  'class': ['PROJECTIVE'],\n",
       "  'spellOutHEAD': [{'K': 'inside'},\n",
       "   {'K': 'of'},\n",
       "   {'AxPart': '[IN]'},\n",
       "   '*Deg',\n",
       "   '*Proj'],\n",
       "  'path_p_morphology': ['GOAL'],\n",
       "  'measure_allowed': True},\n",
       " 'instead of': {'isAtomicMorph': False,\n",
       "  'class': ['PARTICLE'],\n",
       "  'spellOutHEAD': ['*Det',\n",
       "   {'K': 'of'},\n",
       "   '*AxPart',\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'in'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['stead']},\n",
       " 'into': {'isAtomicMorph': False,\n",
       "  'class': ['PARTICLE', 'EXTENDED'],\n",
       "  'spellOutHEAD': ['*Det',\n",
       "   '*K',\n",
       "   '*AxPart',\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'in'},\n",
       "   {'Path': 'to'}],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': True},\n",
       " 'left of': {'isAtomicMorph': False,\n",
       "  'class': ['PROJECTIVE'],\n",
       "  'spellOutHEAD': [{'K': 'left'},\n",
       "   {'K': 'of'},\n",
       "   {'AxPart': '[LEFT]'},\n",
       "   '*Deg',\n",
       "   '*Proj'],\n",
       "  'path_p_morphology': [''],\n",
       "  'measure_allowed': True},\n",
       " 'less': {'isAtomicMorph': False,\n",
       "  'class': None,\n",
       "  'spellOutHEAD': [],\n",
       "  'path_p_morphology': None,\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['less']},\n",
       " 'like': {'isAtomicMorph': False,\n",
       "  'class': None,\n",
       "  'spellOutHEAD': [],\n",
       "  'path_p_morphology': None,\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['like']},\n",
       " 'minus': {'isAtomicMorph': False,\n",
       "  'class': None,\n",
       "  'spellOutHEAD': [],\n",
       "  'path_p_morphology': None,\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['minus']},\n",
       " 'near to': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED', 'EXTENDED'],\n",
       "  'spellOutHEAD': [{'K': 'near'},\n",
       "   '*AxPart',\n",
       "   {'Deg': '[PROXIMAL]'},\n",
       "   '*Proj',\n",
       "   {'p': 'near'},\n",
       "   {'Path': 'to'}],\n",
       "  'path_p_morphology': ['GOAL'],\n",
       "  'measure_allowed': True},\n",
       " 'nearer': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED', 'BOUNDED'],\n",
       "  'spellOutHEAD': [{'K': 'near'},\n",
       "   {'AxPart': '[EDGE]'},\n",
       "   {'Deg': '[PROXIMAL]'},\n",
       "   '*Proj',\n",
       "   {'p': 'near'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC'],\n",
       "  'measure_allowed': False},\n",
       " 'nearest': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED'],\n",
       "  'spellOutHEAD': [{'K': 'near'},\n",
       "   '*AxPart',\n",
       "   {'Deg': '[PROXIMAL]'},\n",
       "   '*Proj',\n",
       "   {'p': 'near'}],\n",
       "  'path_p_morphology': ['none'],\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['est']},\n",
       " 'nigh': {'isAtomicMorph': False,\n",
       "  'class': None,\n",
       "  'spellOutHEAD': [],\n",
       "  'path_p_morphology': None,\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['nigh']},\n",
       " 'notwithstanding': {'isAtomicMorph': False,\n",
       "  'class': None,\n",
       "  'spellOutHEAD': [],\n",
       "  'path_p_morphology': None,\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['notwithstanding']},\n",
       " 'on account of': {'isAtomicMorph': False,\n",
       "  'class': ['PARTICLE'],\n",
       "  'spellOutHEAD': ['*Det',\n",
       "   {'K': 'of'},\n",
       "   '*AxPart',\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'on'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC'],\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['ccount']},\n",
       " 'on behalf of': {'isAtomicMorph': False,\n",
       "  'class': ['PARTICLE'],\n",
       "  'spellOutHEAD': ['*Det',\n",
       "   {'K': 'of'},\n",
       "   '*AxPart',\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'on'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC'],\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['behalf']},\n",
       " 'on top of': {'isAtomicMorph': False,\n",
       "  'class': ['PARTICLE', 'BOUNDED'],\n",
       "  'spellOutHEAD': ['*Det',\n",
       "   {'K': 'of'},\n",
       "   {'AxPart': '[TOP]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'on'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC'],\n",
       "  'measure_allowed': False},\n",
       " 'onto': {'isAtomicMorph': False,\n",
       "  'class': ['PARTICLE', 'EXTENDED'],\n",
       "  'spellOutHEAD': ['*Det',\n",
       "   '*K',\n",
       "   '*AxPart',\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'on'},\n",
       "   {'Path': 'to'}],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': True},\n",
       " 'opposite of': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED'],\n",
       "  'spellOutHEAD': [{'K': 'opposite'},\n",
       "   {'K': 'of'},\n",
       "   {'AxPart': '[FRONT]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'opposite'}],\n",
       "  'path_p_morphology': ['LOC'],\n",
       "  'measure_allowed': False},\n",
       " 'opposite to': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED', 'EXTENDED'],\n",
       "  'spellOutHEAD': [{'K': 'opposite'},\n",
       "   {'AxPart': '[FRONT]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'opposite'},\n",
       "   {'Path': 'to'}],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': True},\n",
       " 'out from': {'isAtomicMorph': False,\n",
       "  'class': ['PARTICLE', 'EXTENDED'],\n",
       "  'spellOutHEAD': ['*Det',\n",
       "   '*K',\n",
       "   '*AxPart',\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'out'},\n",
       "   {'Path': 'from'}],\n",
       "  'path_p_morphology': ['LOC', 'SOURCE'],\n",
       "  'measure_allowed': True},\n",
       " 'out of': {'isAtomicMorph': False,\n",
       "  'class': ['PARTICLE'],\n",
       "  'spellOutHEAD': ['*Det',\n",
       "   {'K': 'of'},\n",
       "   '*AxPart',\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'out'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC'],\n",
       "  'measure_allowed': False},\n",
       " 'outside of': {'isAtomicMorph': False,\n",
       "  'class': ['PROJECTIVE'],\n",
       "  'spellOutHEAD': [{'K': 'outside'},\n",
       "   {'K': 'of'},\n",
       "   {'AxPart': '[OUT]'},\n",
       "   '*Deg',\n",
       "   '*Proj'],\n",
       "  'path_p_morphology': ['GOAL'],\n",
       "  'measure_allowed': True},\n",
       " 'owing to': {'isAtomicMorph': False,\n",
       "  'class': ['PROJECTIVE', 'EXTENDED'],\n",
       "  'spellOutHEAD': [{'K': 'following'},\n",
       "   {'AxPart': '[BACK_SEQUENTIAL_DYNAMIC]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   '*p',\n",
       "   {'Path': 'to'}],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': True},\n",
       " 'per': {'isAtomicMorph': False,\n",
       "  'class': None,\n",
       "  'spellOutHEAD': [],\n",
       "  'path_p_morphology': None,\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['per']},\n",
       " 'plus': {'isAtomicMorph': False,\n",
       "  'class': None,\n",
       "  'spellOutHEAD': [],\n",
       "  'path_p_morphology': None,\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['plus']},\n",
       " 'prior to': {'isAtomicMorph': False,\n",
       "  'class': ['PROJECTIVE', 'EXTENDED'],\n",
       "  'spellOutHEAD': [{'K': 'prior'},\n",
       "   {'AxPart': '[fRONT]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   '*p',\n",
       "   {'Path': 'to'}],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': True},\n",
       " 'pursuant to': {'isAtomicMorph': False,\n",
       "  'class': ['EXTENDED'],\n",
       "  'spellOutHEAD': ['*K', '*AxPart', '*Deg', '*Proj', '*p', {'Path': 'to'}],\n",
       "  'path_p_morphology': ['GOAL'],\n",
       "  'measure_allowed': True,\n",
       "  'unlexicalized': ['pursuant']},\n",
       " 'rather than': {'isAtomicMorph': False,\n",
       "  'class': None,\n",
       "  'spellOutHEAD': [],\n",
       "  'path_p_morphology': None,\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['rather', 'than']},\n",
       " 'regardless of': {'isAtomicMorph': False,\n",
       "  'class': ['NOT_SPATIAL'],\n",
       "  'spellOutHEAD': [{'K': 'of'}],\n",
       "  'path_p_morphology': ['none'],\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['regardless']},\n",
       " 'right of': {'isAtomicMorph': False,\n",
       "  'class': ['PROJECTIVE'],\n",
       "  'spellOutHEAD': [{'K': 'right'},\n",
       "   {'K': 'of'},\n",
       "   {'AxPart': '[RIGHT]'},\n",
       "   '*Deg',\n",
       "   '*Proj'],\n",
       "  'path_p_morphology': [''],\n",
       "  'measure_allowed': True},\n",
       " 'save': {'isAtomicMorph': False,\n",
       "  'class': None,\n",
       "  'spellOutHEAD': [],\n",
       "  'path_p_morphology': None,\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['save']},\n",
       " 'since': {'isAtomicMorph': False,\n",
       "  'class': None,\n",
       "  'spellOutHEAD': [],\n",
       "  'path_p_morphology': None,\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['since']},\n",
       " 'subsequent to': {'isAtomicMorph': False,\n",
       "  'class': ['PROJECTIVE', 'EXTENDED'],\n",
       "  'spellOutHEAD': [{'K': 'subsequent'},\n",
       "   {'AxPart': '[BACK]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   '*p',\n",
       "   {'Path': 'to'}],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': True},\n",
       " 'such as': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED'],\n",
       "  'spellOutHEAD': ['*K',\n",
       "   {'AxPart': '[BIFURCATION]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'astride'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC'],\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['such']},\n",
       " 'thanks to': {'isAtomicMorph': False,\n",
       "  'class': ['EXTENDED'],\n",
       "  'spellOutHEAD': ['*K', '*AxPart', '*Deg', '*Proj', '*p', {'Path': 'to'}],\n",
       "  'path_p_morphology': ['GOAL'],\n",
       "  'measure_allowed': True,\n",
       "  'unlexicalized': ['thanks']},\n",
       " 'throughout': {'isAtomicMorph': False,\n",
       "  'class': ['EXTENDED', 'PARTICLE'],\n",
       "  'spellOutHEAD': ['*Det',\n",
       "   {'K': 'through'},\n",
       "   {'AxPart': '[IN]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'out'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['ROUTE', 'LOC'],\n",
       "  'measure_allowed': True},\n",
       " 'to a higher place': {'isAtomicMorph': False,\n",
       "  'class': ['EXTENDED', 'PROJECTIVE'],\n",
       "  'spellOutHEAD': [{'K': 'higher'},\n",
       "   {'AxPart': '[UP]'},\n",
       "   {'Deg': ['[GRADIENT]', '[POSITIVE]']},\n",
       "   '*Proj',\n",
       "   '*p',\n",
       "   {'Path': 'to'}],\n",
       "  'path_p_morphology': ['GOAL', 'LOC'],\n",
       "  'measure_allowed': True,\n",
       "  'unlexicalized': ['place']},\n",
       " 'to behind': {'isAtomicMorph': False,\n",
       "  'class': ['EXTENDED', 'PROJECTIVE'],\n",
       "  'spellOutHEAD': [{'K': 'behind'},\n",
       "   {'AxPart': '[BACK]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   '*p',\n",
       "   {'Path': 'to'}],\n",
       "  'path_p_morphology': ['GOAL', 'LOC', 'SOURCE'],\n",
       "  'measure_allowed': True},\n",
       " 'to under': {'isAtomicMorph': False,\n",
       "  'class': ['EXTENDED', 'EXTENDED'],\n",
       "  'spellOutHEAD': [{'K': 'under'},\n",
       "   {'AxPart': '[DOWN]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   '*p',\n",
       "   {'Path': 'to'}],\n",
       "  'path_p_morphology': ['GOAL', 'LOC'],\n",
       "  'measure_allowed': True},\n",
       " 'toward': {'isAtomicMorph': False,\n",
       "  'class': ['EXTENDED'],\n",
       "  'spellOutHEAD': ['*K', '*AxPart', '*Deg', '*Proj', '*p', {'Path': 'to'}],\n",
       "  'path_p_morphology': ['GOAL'],\n",
       "  'measure_allowed': True,\n",
       "  'unlexicalized': ['ward']},\n",
       " 'towards': {'isAtomicMorph': False,\n",
       "  'class': ['EXTENDED'],\n",
       "  'spellOutHEAD': ['*K', '*AxPart', '*Deg', '*Proj', '*p', {'Path': 'to'}],\n",
       "  'path_p_morphology': ['GOAL'],\n",
       "  'measure_allowed': True,\n",
       "  'unlexicalized': ['wards']},\n",
       " 'tween': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED'],\n",
       "  'spellOutHEAD': [{'K': 'between'},\n",
       "   {'AxPart': '[BIFURCATION]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'between'}],\n",
       "  'path_p_morphology': ['none'],\n",
       "  'measure_allowed': False},\n",
       " 'underneath': {'isAtomicMorph': False,\n",
       "  'class': ['EXTENDED', 'PROJECTIVE'],\n",
       "  'spellOutHEAD': [{'K': 'under'},\n",
       "   {'K': 'beneath'},\n",
       "   {'AxPart': '[DOWN]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'beneath'}],\n",
       "  'path_p_morphology': ['LOC'],\n",
       "  'measure_allowed': True},\n",
       " 'underside': {'isAtomicMorph': False,\n",
       "  'class': ['EXTENDED', 'PROJECTIVE'],\n",
       "  'spellOutHEAD': [{'K': 'under'},\n",
       "   {'K': 'side'},\n",
       "   {'AxPart': '[DOWN]'},\n",
       "   {'AxPart': '[SIDE]'},\n",
       "   '*Deg',\n",
       "   '*Proj'],\n",
       "  'path_p_morphology': ['LOC'],\n",
       "  'measure_allowed': True},\n",
       " 'until': {'isAtomicMorph': False,\n",
       "  'class': None,\n",
       "  'spellOutHEAD': [],\n",
       "  'path_p_morphology': None,\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['until']},\n",
       " 'up to': {'isAtomicMorph': False,\n",
       "  'class': ['PARTICLE', 'EXTENDED'],\n",
       "  'spellOutHEAD': ['*K',\n",
       "   '*AxPart',\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'up'},\n",
       "   {'Path': 'to'}],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': True},\n",
       " 'upside': {'isAtomicMorph': False,\n",
       "  'class': ['PARTICLE', 'PROJECTIVE'],\n",
       "  'spellOutHEAD': [{'K': 'side'},\n",
       "   {'AxPart': '[SIDE]'},\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'up'}],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': True},\n",
       " 'versus': {'isAtomicMorph': False,\n",
       "  'class': None,\n",
       "  'spellOutHEAD': [],\n",
       "  'path_p_morphology': None,\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['versus']},\n",
       " 'with a view to': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED', 'EXTENDED'],\n",
       "  'spellOutHEAD': [{'K': 'with'},\n",
       "   '*AxPart',\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'with'},\n",
       "   {'Path': 'to'}],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': True,\n",
       "  'unlexicalized': ['view']},\n",
       " 'with regard to': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED', 'EXTENDED'],\n",
       "  'spellOutHEAD': [{'K': 'with'},\n",
       "   '*AxPart',\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'with'},\n",
       "   {'Path': 'to'}],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': True,\n",
       "  'unlexicalized': ['regard']},\n",
       " 'with respect to': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED', 'EXTENDED'],\n",
       "  'spellOutHEAD': [{'K': 'with'},\n",
       "   '*AxPart',\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'with'},\n",
       "   {'Path': 'to'}],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': True,\n",
       "  'unlexicalized': ['respect']},\n",
       " 'within': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED', 'PARTICLE'],\n",
       "  'spellOutHEAD': ['*Det',\n",
       "   {'K': 'with'},\n",
       "   '*AxPart',\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'with'},\n",
       "   {'p': 'in'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC', 'GOAL'],\n",
       "  'measure_allowed': False},\n",
       " 'without': {'isAtomicMorph': False,\n",
       "  'class': ['BOUNDED', 'PARTICLE'],\n",
       "  'spellOutHEAD': ['*Det',\n",
       "   {'K': 'with'},\n",
       "   '*AxPart',\n",
       "   '*Deg',\n",
       "   '*Proj',\n",
       "   {'p': 'with'},\n",
       "   {'p': 'out'},\n",
       "   '*Path'],\n",
       "  'path_p_morphology': ['LOC'],\n",
       "  'measure_allowed': False},\n",
       " 'worth': {'isAtomicMorph': False,\n",
       "  'class': None,\n",
       "  'spellOutHEAD': [],\n",
       "  'path_p_morphology': None,\n",
       "  'measure_allowed': False,\n",
       "  'unlexicalized': ['worth']}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH={'to','from','into','onto','through','across','toward','past'}\n",
    "PLACE={'in','on','at','under','beside','near','between','among'}\n",
    "factory = PPFactory('pp_lexicon/atomic_p.json', 'pp_lexicon/p_lexicon.json', 'pp_lexicon/complex_pp_repop.json', PATH, PLACE)\n",
    "factory.create_classes()\n",
    "\n",
    "decomposed_result = factory.export_complex_pp()\n",
    "decomposed_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb43c56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unlexicalized entries: 65\n",
      "['about', 'absent', 'according to', 'adjacent', 'adjacent to', 'ahead', 'ahead of', 'as opposed to', 'as per', 'as regards', 'as soon as', 'as well as', 'at least', 'at most', 'at the back of', 'at the behest of', 'at the rear of', 'because of', 'betwixt', 'by means of', 'by virtue of', 'circa', 'concerning', 'despite', 'due to', 'during', 'except', 'except for', 'for the sake of', 'in a higher place', 'in accordance with', 'in addition to', 'in case of', 'in lieu of', 'in place of', 'in point of', 'in spite of', 'including', 'instead of', 'less', 'like', 'minus', 'nearest', 'nigh', 'notwithstanding', 'on account of', 'on behalf of', 'per', 'plus', 'pursuant to', 'rather than', 'regardless of', 'save', 'since', 'such as', 'thanks to', 'to a higher place', 'toward', 'towards', 'until', 'versus', 'with a view to', 'with regard to', 'with respect to', 'worth']\n"
     ]
    }
   ],
   "source": [
    "unlexicalized_count = 0\n",
    "for entry in decomposed_result.values():\n",
    "    unlex = entry.get('unlexicalized')  \n",
    "    if unlex:                           \n",
    "        unlexicalized_count += 1\n",
    "\n",
    "print(f\"Number of unlexicalized entries: {unlexicalized_count}\")\n",
    "\n",
    "keys_with_unlex = [\n",
    "    k\n",
    "    for k, entry in decomposed_result.items()\n",
    "    if entry.get(\"unlexicalized\")\n",
    "]\n",
    "\n",
    "print(keys_with_unlex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5540853",
   "metadata": {},
   "source": [
    "## Merge with base atomic lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d4e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_pp_lexicon.update(decomposed_result)\n",
    "# with open('pp_lexicon/master_pp_lexicon.json', 'w') as f:\n",
    "#     json.dump(master_pp_lexicon, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a681ed",
   "metadata": {},
   "source": [
    "## Step 4: Acquire Spatial Sentences Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28a3974",
   "metadata": {},
   "source": [
    "### Refclef dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e226d398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_ids_list</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>ref_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>split</th>\n",
       "      <th>category_id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>raw</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>19135_1</td>\n",
       "      <td>0</td>\n",
       "      <td>19135</td>\n",
       "      <td>train</td>\n",
       "      <td>60</td>\n",
       "      <td>sky</td>\n",
       "      <td>sky</td>\n",
       "      <td>0</td>\n",
       "      <td>sky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>19135_2</td>\n",
       "      <td>1</td>\n",
       "      <td>19135</td>\n",
       "      <td>train</td>\n",
       "      <td>235</td>\n",
       "      <td>statue</td>\n",
       "      <td>statue</td>\n",
       "      <td>1</td>\n",
       "      <td>statue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>23412_4</td>\n",
       "      <td>2</td>\n",
       "      <td>23412</td>\n",
       "      <td>train</td>\n",
       "      <td>258</td>\n",
       "      <td>anywhere,except,the,people</td>\n",
       "      <td>anywhere except the people</td>\n",
       "      <td>2</td>\n",
       "      <td>anywhere except the people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>23412_1</td>\n",
       "      <td>3</td>\n",
       "      <td>23412</td>\n",
       "      <td>train</td>\n",
       "      <td>160</td>\n",
       "      <td>person,in,front</td>\n",
       "      <td>person in front</td>\n",
       "      <td>3</td>\n",
       "      <td>person in front</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>23412_2</td>\n",
       "      <td>4</td>\n",
       "      <td>23412</td>\n",
       "      <td>train</td>\n",
       "      <td>120</td>\n",
       "      <td>person,all,the,way,in,back</td>\n",
       "      <td>person all the way in back</td>\n",
       "      <td>4</td>\n",
       "      <td>person all the way in back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130359</th>\n",
       "      <td>130358,130359</td>\n",
       "      <td>7380_2</td>\n",
       "      <td>99291</td>\n",
       "      <td>7380</td>\n",
       "      <td>train</td>\n",
       "      <td>96</td>\n",
       "      <td>two,birds,on,left</td>\n",
       "      <td>two birds on left</td>\n",
       "      <td>130359</td>\n",
       "      <td>two birds on left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130360</th>\n",
       "      <td>130360</td>\n",
       "      <td>7380_5</td>\n",
       "      <td>99292</td>\n",
       "      <td>7380</td>\n",
       "      <td>train</td>\n",
       "      <td>182</td>\n",
       "      <td>palm,tree,to,the,right</td>\n",
       "      <td>palm tree to the right</td>\n",
       "      <td>130360</td>\n",
       "      <td>palm tree to the right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130361</th>\n",
       "      <td>130361</td>\n",
       "      <td>7380_4</td>\n",
       "      <td>99293</td>\n",
       "      <td>7380</td>\n",
       "      <td>train</td>\n",
       "      <td>182</td>\n",
       "      <td>tree,on,left,side</td>\n",
       "      <td>tree on left side</td>\n",
       "      <td>130361</td>\n",
       "      <td>tree on left side</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130362</th>\n",
       "      <td>130362</td>\n",
       "      <td>38047_4</td>\n",
       "      <td>99294</td>\n",
       "      <td>38047</td>\n",
       "      <td>train</td>\n",
       "      <td>34</td>\n",
       "      <td>bush,bottom,left</td>\n",
       "      <td>bush bottom left</td>\n",
       "      <td>130362</td>\n",
       "      <td>bush bottom left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130363</th>\n",
       "      <td>130363</td>\n",
       "      <td>7380_6</td>\n",
       "      <td>99295</td>\n",
       "      <td>7380</td>\n",
       "      <td>train</td>\n",
       "      <td>224</td>\n",
       "      <td>sky</td>\n",
       "      <td>sky</td>\n",
       "      <td>130363</td>\n",
       "      <td>sky</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130364 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sent_ids_list   ann_id  ref_id  image_id  split  category_id                      tokens                         raw  sent_id                        sent\n",
       "0                   0  19135_1       0     19135  train           60                         sky                         sky        0                         sky\n",
       "1                   1  19135_2       1     19135  train          235                      statue                      statue        1                      statue\n",
       "2                   2  23412_4       2     23412  train          258  anywhere,except,the,people  anywhere except the people        2  anywhere except the people\n",
       "3                   3  23412_1       3     23412  train          160             person,in,front             person in front        3             person in front\n",
       "4                   4  23412_2       4     23412  train          120  person,all,the,way,in,back  person all the way in back        4  person all the way in back\n",
       "...               ...      ...     ...       ...    ...          ...                         ...                         ...      ...                         ...\n",
       "130359  130358,130359   7380_2   99291      7380  train           96           two,birds,on,left           two birds on left   130359           two birds on left\n",
       "130360         130360   7380_5   99292      7380  train          182      palm,tree,to,the,right      palm tree to the right   130360      palm tree to the right\n",
       "130361         130361   7380_4   99293      7380  train          182           tree,on,left,side           tree on left side   130361           tree on left side\n",
       "130362         130362  38047_4   99294     38047  train           34            bush,bottom,left            bush bottom left   130362            bush bottom left\n",
       "130363         130363   7380_6   99295      7380  train          224                         sky                         sky   130363                         sky\n",
       "\n",
       "[130364 rows x 10 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_refer = pd.read_csv('dataset/refer/referitdataset/refclef_unc_flattened.csv')\n",
    "df_refer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "07c58387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                sky\n",
       "1                             statue\n",
       "2         anywhere except the people\n",
       "3                    person in front\n",
       "4         person all the way in back\n",
       "                     ...            \n",
       "130359             two birds on left\n",
       "130360        palm tree to the right\n",
       "130361             tree on left side\n",
       "130362              bush bottom left\n",
       "130363                           sky\n",
       "Name: sent, Length: 130364, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_refer['sent']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04e7ebc",
   "metadata": {},
   "source": [
    "#### Inspect most frequent tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53047f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bottom,right,corner', 476), ('bottom,left,corner', 446), ('bottom,right', 413), ('bottom,left', 399), ('the,sky', 386), ('top,left,corner', 347), ('top,right,corner', 338), ('top,right', 291), ('top,left', 271), ('blue,sky', 255), ('bike', 233), ('any,person', 217), ('tree,on,left', 162), ('tree,on,right', 155), ('top,sky', 147), ('tree,left', 138), ('anywhere', 127), ('left,building', 127), ('tree,right', 119), ('sky,top,left', 118)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# ensure Brown corpus and the universal tagset are available\n",
    "nltk.download('brown', quiet=True)\n",
    "nltk.download('universal_tagset', quiet=True)\n",
    "\n",
    "# build a set of lowercased nouns from the Brown corpus\n",
    "noun_set = {w.lower() for w, t in brown.tagged_words(tagset='universal') if t == 'NOUN'}\n",
    "\n",
    "# explode your tokens, drop missing, lowercase them\n",
    "tokens = df_refer['tokens'].str.split().explode().dropna().str.lower()\n",
    "\n",
    "# keep only those tokens that are NOT in the Brown corpus noun set\n",
    "filtered_tokens = tokens[~tokens.isin(noun_set)]\n",
    "\n",
    "from collections import Counter\n",
    "most_common_tokens = Counter(filtered_tokens).most_common(20)\n",
    "print((most_common_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "341ddd75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bottom,right,corner', 476),\n",
       " ('bottom,left,corner', 446),\n",
       " ('bottom,right', 413),\n",
       " ('bottom,left', 399),\n",
       " ('the,sky', 386),\n",
       " ('top,left,corner', 347),\n",
       " ('top,right,corner', 338),\n",
       " ('top,right', 291),\n",
       " ('top,left', 271),\n",
       " ('blue,sky', 255),\n",
       " ('bike', 233),\n",
       " ('any,person', 217),\n",
       " ('tree,on,left', 162),\n",
       " ('tree,on,right', 155),\n",
       " ('top,sky', 147),\n",
       " ('tree,left', 138),\n",
       " ('anywhere', 127),\n",
       " ('left,building', 127),\n",
       " ('tree,right', 119),\n",
       " ('sky,top,left', 118)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb99456",
   "metadata": {},
   "source": [
    "### Random Sampling for RefClef dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b1652d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences longer than 2 tokens: 51338\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_ids_list</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>ref_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>split</th>\n",
       "      <th>category_id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>raw</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21586</th>\n",
       "      <td>21586</td>\n",
       "      <td>23058_2</td>\n",
       "      <td>15848</td>\n",
       "      <td>23058</td>\n",
       "      <td>train</td>\n",
       "      <td>273</td>\n",
       "      <td>second,person,from,right</td>\n",
       "      <td>second person from right</td>\n",
       "      <td>21586</td>\n",
       "      <td>second person from right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67160</th>\n",
       "      <td>67160</td>\n",
       "      <td>15940_1</td>\n",
       "      <td>50386</td>\n",
       "      <td>15940</td>\n",
       "      <td>train</td>\n",
       "      <td>60</td>\n",
       "      <td>any,part,of,sky</td>\n",
       "      <td>any part of sky</td>\n",
       "      <td>67160</td>\n",
       "      <td>any part of sky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66823</th>\n",
       "      <td>66823</td>\n",
       "      <td>3048_10</td>\n",
       "      <td>50123</td>\n",
       "      <td>3048</td>\n",
       "      <td>train</td>\n",
       "      <td>90</td>\n",
       "      <td>fence,on,the,right</td>\n",
       "      <td>fence on the right</td>\n",
       "      <td>66823</td>\n",
       "      <td>fence on the right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95212</th>\n",
       "      <td>95212,95213,95214</td>\n",
       "      <td>11547_5</td>\n",
       "      <td>71964</td>\n",
       "      <td>11547</td>\n",
       "      <td>val</td>\n",
       "      <td>114</td>\n",
       "      <td>bottom,most,glass,front,center</td>\n",
       "      <td>bottom most glass front center</td>\n",
       "      <td>95212</td>\n",
       "      <td>bottom most glass front center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54317</th>\n",
       "      <td>54317</td>\n",
       "      <td>30580_2</td>\n",
       "      <td>40684</td>\n",
       "      <td>30580</td>\n",
       "      <td>train</td>\n",
       "      <td>66</td>\n",
       "      <td>any,of,the,two,people</td>\n",
       "      <td>Any of the two people</td>\n",
       "      <td>54317</td>\n",
       "      <td>any of the two people</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sent_ids_list   ann_id  ref_id  image_id  split  category_id                          tokens                             raw  sent_id                            sent\n",
       "21586              21586  23058_2   15848     23058  train          273        second,person,from,right        second person from right    21586        second person from right\n",
       "67160              67160  15940_1   50386     15940  train           60                 any,part,of,sky                 any part of sky    67160                 any part of sky\n",
       "66823              66823  3048_10   50123      3048  train           90              fence,on,the,right              fence on the right    66823              fence on the right\n",
       "95212  95212,95213,95214  11547_5   71964     11547    val          114  bottom,most,glass,front,center  bottom most glass front center    95212  bottom most glass front center\n",
       "54317              54317  30580_2   40684     30580  train           66           any,of,the,two,people           Any of the two people    54317           any of the two people"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter df_refer to only sentences longer than 2 tokens\n",
    "mask = df_refer['sent'].str.split().str.len() > 3\n",
    "df_refer_filtered = df_refer.loc[mask]\n",
    "print(f\"Sentences longer than 2 tokens: {len(df_refer_filtered)}\")\n",
    "\n",
    "# now sample from the filtered dataframe\n",
    "# replace n and random_state with your desired values\n",
    "sampled_df_refer = df_refer_filtered.sample(n=1185, random_state=42)\n",
    "sampled_df_refer.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "41268be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['second person from right',\n",
       " 'any part of sky',\n",
       " 'fence on the right',\n",
       " 'bottom most glass front center',\n",
       " 'any of the two people',\n",
       " 'water to left of object in center screen',\n",
       " 'fourth flag from left',\n",
       " 'girl on right with red shirt',\n",
       " 'the black horse on the right hand side with 2 peeps on it',\n",
       " 'sky right above the mountain']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refer_sent = sampled_df_refer['sent'].tolist()\n",
    "refer_sent[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6ab4fc",
   "metadata": {},
   "source": [
    "### Acquiring REAL_Corpus Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4feefab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>photoid</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>annotation</th>\n",
       "      <th>status</th>\n",
       "      <th>phrase length</th>\n",
       "      <th>validator_userid</th>\n",
       "      <th>validator_age</th>\n",
       "      <th>validator_gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>male</td>\n",
       "      <td>img23</td>\n",
       "      <td>495</td>\n",
       "      <td>157</td>\n",
       "      <td>Pub called 'Deacon Brodie's Tavern. Black and white traditional building with sign hanging outside and 3 big arched windows</td>\n",
       "      <td>correct</td>\n",
       "      <td>123</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>male</td>\n",
       "      <td>img23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Pub called 'Deacon Brodie's Tavern. Black and white traditional building with sign hanging outside and 3 big arched windows</td>\n",
       "      <td>cantfind</td>\n",
       "      <td>123</td>\n",
       "      <td>41</td>\n",
       "      <td>5</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>male</td>\n",
       "      <td>img23</td>\n",
       "      <td>383</td>\n",
       "      <td>164</td>\n",
       "      <td>Pub called 'Deacon Brodie's Tavern. Black and white traditional building with sign hanging outside and 3 big arched windows</td>\n",
       "      <td>correct</td>\n",
       "      <td>123</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>male</td>\n",
       "      <td>img31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Traditional early Victorian terrace on 3 floors + dormer. Third in from the left of the end terrace.  Royal Blue Door, Black, spiked iron railings.</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>147</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>male</td>\n",
       "      <td>img31</td>\n",
       "      <td>210</td>\n",
       "      <td>421</td>\n",
       "      <td>Traditional early Victorian terrace on 3 floors + dormer. Third in from the left of the end terrace.  Royal Blue Door, Black, spiked iron railings.</td>\n",
       "      <td>correct</td>\n",
       "      <td>147</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userid  age gender photoid    x    y                                                                                                                                           annotation     status  phrase length  validator_userid  validator_age validator_gender\n",
       "0       2    4   male   img23  495  157                          Pub called 'Deacon Brodie's Tavern. Black and white traditional building with sign hanging outside and 3 big arched windows    correct            123                26              6             male\n",
       "1       2    4   male   img23    0    0                          Pub called 'Deacon Brodie's Tavern. Black and white traditional building with sign hanging outside and 3 big arched windows   cantfind            123                41              5           female\n",
       "2       2    4   male   img23  383  164                          Pub called 'Deacon Brodie's Tavern. Black and white traditional building with sign hanging outside and 3 big arched windows    correct            123                42              4           female\n",
       "3       2    4   male   img31    0    0  Traditional early Victorian terrace on 3 floors + dormer. Third in from the left of the end terrace.  Royal Blue Door, Black, spiked iron railings.  ambiguous            147                 7              2           female\n",
       "4       2    4   male   img31  210  421  Traditional early Victorian terrace on 3 floors + dormer. Third in from the left of the end terrace.  Royal Blue Door, Black, spiked iron railings.    correct            147                 8              3           female"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_real = pd.read_csv('dataset/REAL_Corpus/REAL_Corpus_ReferringExpressionsData_withValidationDetails.csv', delimiter=';')\n",
    "df_real.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a67381f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>photoid</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>annotation</th>\n",
       "      <th>status</th>\n",
       "      <th>phrase length</th>\n",
       "      <th>validator_userid</th>\n",
       "      <th>validator_age</th>\n",
       "      <th>validator_gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>male</td>\n",
       "      <td>img23</td>\n",
       "      <td>495</td>\n",
       "      <td>157</td>\n",
       "      <td>Pub called 'Deacon Brodie's Tavern. Black and white traditional building with sign hanging outside and 3 big arched windows</td>\n",
       "      <td>correct</td>\n",
       "      <td>123</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>male</td>\n",
       "      <td>img23</td>\n",
       "      <td>383</td>\n",
       "      <td>164</td>\n",
       "      <td>Pub called 'Deacon Brodie's Tavern. Black and white traditional building with sign hanging outside and 3 big arched windows</td>\n",
       "      <td>correct</td>\n",
       "      <td>123</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>male</td>\n",
       "      <td>img31</td>\n",
       "      <td>210</td>\n",
       "      <td>421</td>\n",
       "      <td>Traditional early Victorian terrace on 3 floors + dormer. Third in from the left of the end terrace.  Royal Blue Door, Black, spiked iron railings.</td>\n",
       "      <td>correct</td>\n",
       "      <td>147</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>male</td>\n",
       "      <td>img31</td>\n",
       "      <td>82</td>\n",
       "      <td>371</td>\n",
       "      <td>Traditional early Victorian terrace on 3 floors + dormer. Third in from the left of the end terrace.  Royal Blue Door, Black, spiked iron railings.</td>\n",
       "      <td>correct</td>\n",
       "      <td>147</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>male</td>\n",
       "      <td>img21</td>\n",
       "      <td>222</td>\n",
       "      <td>305</td>\n",
       "      <td>Large, modern glass fronted building, butted up against traditional victorian terrace, slightly set back from road, and with facing bowed frontage.</td>\n",
       "      <td>correct</td>\n",
       "      <td>147</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userid  age gender photoid    x    y                                                                                                                                           annotation   status  phrase length  validator_userid  validator_age validator_gender\n",
       "0       2    4   male   img23  495  157                          Pub called 'Deacon Brodie's Tavern. Black and white traditional building with sign hanging outside and 3 big arched windows  correct            123                26              6             male\n",
       "2       2    4   male   img23  383  164                          Pub called 'Deacon Brodie's Tavern. Black and white traditional building with sign hanging outside and 3 big arched windows  correct            123                42              4           female\n",
       "4       2    4   male   img31  210  421  Traditional early Victorian terrace on 3 floors + dormer. Third in from the left of the end terrace.  Royal Blue Door, Black, spiked iron railings.  correct            147                 8              3           female\n",
       "5       2    4   male   img31   82  371  Traditional early Victorian terrace on 3 floors + dormer. Third in from the left of the end terrace.  Royal Blue Door, Black, spiked iron railings.  correct            147                 9              2             male\n",
       "6       2    4   male   img21  222  305  Large, modern glass fronted building, butted up against traditional victorian terrace, slightly set back from road, and with facing bowed frontage.  correct            147                 6              2             male"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_real[df_real['status'] == 'correct'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ec90b9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userid                                                                                                                                        2\n",
       "age                                                                                                                                           4\n",
       "gender                                                                                                                                     male\n",
       "photoid                                                                                                                                   img23\n",
       "x                                                                                                                                           495\n",
       "y                                                                                                                                           157\n",
       "annotation          Pub called 'Deacon Brodie's Tavern. Black and white traditional building with sign hanging outside and 3 big arched windows\n",
       "status                                                                                                                                  correct\n",
       "phrase length                                                                                                                               123\n",
       "validator_userid                                                                                                                             26\n",
       "validator_age                                                                                                                                 6\n",
       "validator_gender                                                                                                                           male\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep the first row for each unique annotation,\n",
    "# but carry along every other column\n",
    "df_real_unique = df_real.drop_duplicates(subset=['annotation'], keep='first') \\\n",
    "                        .reset_index(drop=True)\n",
    "df_real_unique.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0c0394a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             Pub called 'Deacon Brodie's Tavern. Black and white traditional building with sign hanging outside and 3 big arched windows\n",
       "1     Traditional early Victorian terrace on 3 floors + dormer. Third in from the left of the end terrace.  Royal Blue Door, Black, spiked iron railings.\n",
       "2     Large, modern glass fronted building, butted up against traditional victorian terrace, slightly set back from road, and with facing bowed frontage.\n",
       "3                           Two story traditional stone building, on the left of the street as faced, just before the bed.  Two large white double doors.\n",
       "4                  Church in the middle of a small square/island - no spire but with large stained glass window in gable end, slightly obscured by trees.\n",
       "5                                                                      Building with columns behind the one in front, also in similar architecture style.\n",
       "6                                                                   End centre of the street, straight ahead, big building with a columns and a dome roof\n",
       "7                                                             Grey building block with statues above the entrance, Amarone restaurant on the bottom floor\n",
       "8                                                                                        Big historical building with huge columns and statue at the top.\n",
       "9                                 Middle vertical section of the house above the brown door of the housing block with Eigin Cashmere on the ground floor.\n",
       "10                                                                                                                              Small shed with lead roof\n",
       "11                                                                                                            Red and gold coffee shop next to the bridge\n",
       "12                                                                              Small stone capped monument/fountain with a plaque built on the pavement.\n",
       "13                                                                                                Enormous Gothic tower in the park that is stained black\n",
       "14                                                                                                        Coffee shop to the left of the converted church\n",
       "15                                                        The windows above the restaurant with the black street furniture next to the RBS on the corner.\n",
       "16                                                                         The Austrian looking white house with the dark wooden beams at the water side.\n",
       "17                                                                The castle- like tower on the corner next to the Tartan Weaving Mill on the Royal Mile.\n",
       "18                                                        The green- ish statue standing in front on the square in front of the church on the Royal Mile.\n",
       "19                                                          The square building on the right hand side with the three tall pillars next to the Pizza Hut.\n",
       "Name: annotation, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "df_real_unique['annotation'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a814e99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(878, 12)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_real_unique.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4d2e7c",
   "metadata": {},
   "source": [
    "There's datas which contains multiple sentences. We should split those based on ending punctuation (.?!) occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0e372a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "878"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_real_unique['annotation'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aa65fb59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                                                                                     Pub called 'Deacon Brodie's Tavern.\n",
       "1                                                                 Black and white traditional building with sign hanging outside and 3 big arched windows\n",
       "2                                                                                               Traditional early Victorian terrace on 3 floors + dormer.\n",
       "3                                                                                                              Third in from the left of the end terrace.\n",
       "4                                                                                                           Royal Blue Door, Black, spiked iron railings.\n",
       "5     Large, modern glass fronted building, butted up against traditional victorian terrace, slightly set back from road, and with facing bowed frontage.\n",
       "6                                                          Two story traditional stone building, on the left of the street as faced, just before the bed.\n",
       "7                                                                                                                           Two large white double doors.\n",
       "8                  Church in the middle of a small square/island - no spire but with large stained glass window in gable end, slightly obscured by trees.\n",
       "9                                                                      Building with columns behind the one in front, also in similar architecture style.\n",
       "10                                                                  End centre of the street, straight ahead, big building with a columns and a dome roof\n",
       "11                                                            Grey building block with statues above the entrance, Amarone restaurant on the bottom floor\n",
       "12                                                                                       Big historical building with huge columns and statue at the top.\n",
       "13                                Middle vertical section of the house above the brown door of the housing block with Eigin Cashmere on the ground floor.\n",
       "14                                                                                                                              Small shed with lead roof\n",
       "15                                                                                                            Red and gold coffee shop next to the bridge\n",
       "16                                                                              Small stone capped monument/fountain with a plaque built on the pavement.\n",
       "17                                                                                                Enormous Gothic tower in the park that is stained black\n",
       "18                                                                                                        Coffee shop to the left of the converted church\n",
       "19                                                        The windows above the restaurant with the black street furniture next to the RBS on the corner.\n",
       "Name: annotation, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_real_split = df_real_unique.copy()\n",
    "\n",
    "# split the original annotation string once into sentence lists if contains multiple sentences with ?!.\n",
    "df_real_split['annotation'] = df_real_split['annotation'].apply(\n",
    "    lambda s: re.split(r'(?<=[\\.\\?\\!])\\s+', s) if isinstance(s, str) else []\n",
    ")\n",
    "\n",
    "df_real_split = df_real_split.explode('annotation').reset_index(drop=True)\n",
    "\n",
    "df_real_split['annotation'] = df_real_split['annotation'].str.strip()\n",
    "df_real_split = df_real_split[df_real_split['annotation'] != '']\n",
    "# 4. Join back to the original\n",
    "\n",
    "df_real_split['annotation'].head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "262550fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1244"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_real_split['annotation'].nunique()\n",
    "\n",
    "# to csv\n",
    "# df_real_split['annotation'].to_csv('dataset/REAL_Corpus/unique_split_annotations.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "71f0bdfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1185, 12)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# detect df_real_split['annotation'] which only contain less than 3 tokens\n",
    "mask = df_real_split['annotation'].str.split().str.len() < 3\n",
    "df_real_split_filtered = df_real_split[~mask].reset_index(drop=True)\n",
    "df_real_split_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ca4ccbf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                        Pub called 'Deacon Brodie's Tavern.\n",
       "1    Black and white traditional building with sign hanging outside and 3 big arched windows\n",
       "2                                  Traditional early Victorian terrace on 3 floors + dormer.\n",
       "3                                                 Third in from the left of the end terrace.\n",
       "4                                              Royal Blue Door, Black, spiked iron railings.\n",
       "Name: annotation, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_real_split_filtered['annotation'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2684b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('dataset/REAL_Corpus/unique_split_annotations_filtered.json', 'r') as f:\n",
    "    unique_split_annotations_filtered = json.load(f)\n",
    "    \n",
    "df_real_split_filtered['annotation'] = unique_split_annotations_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb790298",
   "metadata": {},
   "source": [
    "### Real corpus cfg generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fad2fe",
   "metadata": {},
   "source": [
    "#### CFG + earley "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01f69ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/glora/projects/populate-ns-lex/.venv-benepar/lib/python3.9/site-packages/benepar/parse_chart.py:169: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n"
     ]
    }
   ],
   "source": [
    "parser = Parser(\"benepar_en3\")\n",
    "\n",
    "def create_structural_cfg(sentences):\n",
    "    trees = []\n",
    "    for sent in sentences:\n",
    "        try:\n",
    "            # parse string  nltk.Tree\n",
    "            t = parser.parse(sent)\n",
    "            trees.append(t)\n",
    "        except Exception as e:\n",
    "            # optional: log or skip unparseable sentences\n",
    "            print(f\"  skipping (parse failed): {sent!r}   {e!r}\")\n",
    "\n",
    "    if not trees:\n",
    "        raise ValueError(\"No parse trees generated  check your sentences\")\n",
    "\n",
    "    # extract & dedupe productions\n",
    "    all_prods   = [p for t in trees for p in t.productions()]\n",
    "    unique_prods = list(dict.fromkeys(all_prods))\n",
    "\n",
    "    # derive a start symbol that actually appears in LHS\n",
    "    start_label = trees[0].label()\n",
    "    start_nt    = Nonterminal(start_label)\n",
    "\n",
    "    # build a quick lookup for reachability\n",
    "    prods_by_lhs = defaultdict(list)\n",
    "    for p in unique_prods:\n",
    "        prods_by_lhs[p.lhs()].append(p)\n",
    "\n",
    "    # BFS from the start symbol\n",
    "    reachable = {start_nt}\n",
    "    queue     = deque([start_nt])\n",
    "    while queue:\n",
    "        lhs = queue.popleft()\n",
    "        for p in prods_by_lhs[lhs]:\n",
    "            for sym in p.rhs():\n",
    "                if isinstance(sym, Nonterminal) and sym not in reachable:\n",
    "                    reachable.add(sym)\n",
    "                    queue.append(sym)\n",
    "\n",
    "    pruned = [p for p in unique_prods if p.lhs() in reachable]\n",
    "    if not pruned:\n",
    "        raise ValueError(f\"No productions reachable from {start_nt!r}\")\n",
    "\n",
    "    return CFG(start=start_nt, productions=pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10f52667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c838acfddf14c2091a65db99df98ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building CFG:   0%|          | 0/1185 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "/Users/glora/projects/populate-ns-lex/.venv-benepar/lib/python3.9/site-packages/torch/distributions/distribution.py:55: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "sentences_clean = [re.sub(r\"[^\\w\\s]\", \"\", s) for s in df_real_split_filtered['annotation'].tolist() if isinstance(s, str)]\n",
    "\n",
    "# build the CFG and parser\n",
    "cfg = create_structural_cfg(\n",
    "    tqdm(\n",
    "        sentences_clean,\n",
    "        desc=\"Building CFG\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1f301",
   "metadata": {},
   "source": [
    "#### Derive CFG -> PCFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "99b00660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP -> NP [0.0833333]  [0.08]\n",
      "NP -> NP VP [0.00273973]  [0.00]\n",
      "NP -> NN [0.00273973]  [0.00]\n",
      "NN -> 'Pub' [0.00176678]  [0.00]\n",
      "VP -> VBN S [0.00675676]  [0.01]\n",
      "VBN -> 'called' [0.0126582]  [0.01]\n",
      "S -> NP [0.02]  [0.02]\n",
      "NP -> NNP NNP NNP [0.00273973]  [0.00]\n",
      "NNP -> 'Deacon' [0.00680272]  [0.01]\n",
      "NNP -> 'Brodies' [0.00680272]  [0.01]\n"
     ]
    }
   ],
   "source": [
    "from nltk.grammar import induce_pcfg\n",
    "\n",
    "# assume youve already loaded:\n",
    "#   cfg = CFG.fromstring(cfg_text)\n",
    "start = cfg.start()\n",
    "# start = 'TOP'\n",
    "productions = cfg.productions()\n",
    "\n",
    "# this will count each production once, then for each nonterminal A\n",
    "# assign P(rhs | A) = 1 / (# of productions with LHS A)\n",
    "pcfg = induce_pcfg(start, productions)\n",
    "\n",
    "# inspect a few rules\n",
    "for p in pcfg.productions()[:10]:\n",
    "    print(f\"{p}  [{p.prob():.2f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "00e0b078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rules: 2793\n",
      "Pruned rules:   2793\n"
     ]
    }
   ],
   "source": [
    "# threshold = 0.01\n",
    "\n",
    "# 2. filter\n",
    "# pruned_prods = [p for p in pcfg.productions() if p.prob() >= threshold]\n",
    "\n",
    "# 3. (optional) re-normalize each LHS so the probabilities sum to 1 again\n",
    "from collections import defaultdict\n",
    "grouped = defaultdict(list)\n",
    "for p in pcfg.productions():\n",
    "    grouped[p.lhs()].append(p)\n",
    "\n",
    "repl_prods = []\n",
    "for lhs, ps in grouped.items():\n",
    "    total = sum(p.prob() for p in ps)\n",
    "    for p in ps:\n",
    "        normalized = p.prob() / total\n",
    "        # NOTE the `prob=` keyword here\n",
    "        repl_prods.append(ProbabilisticProduction(lhs, p.rhs(), prob=normalized))\n",
    "\n",
    "\n",
    "# 4. build the smaller PCFG\n",
    "small_pcfg = PCFG(pcfg.start(), repl_prods)\n",
    "\n",
    "print(f\"Original rules: {len(pcfg.productions())}\")\n",
    "print(f\"Pruned rules:   {len(small_pcfg.productions())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a99779aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pcfg_from_pos_tagged(sentences, min_prob=0.001):\n",
    "    \"\"\"\n",
    "    Build a robust PCFG from POS-tagged sentences without external parser.\n",
    "    \n",
    "    Args:\n",
    "        sentences: List of raw sentences\n",
    "        min_prob: Minimum probability threshold\n",
    "    \n",
    "    Returns:\n",
    "        Robust PCFG based on POS patterns with better coverage\n",
    "    \"\"\"\n",
    "    import nltk\n",
    "    from nltk import word_tokenize, pos_tag\n",
    "    \n",
    "    # Download required NLTK data if not present\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "    \n",
    "    try:\n",
    "        nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "    except LookupError:\n",
    "        nltk.download('averaged_perceptron_tagger')\n",
    "    \n",
    "    production_counts = Counter()\n",
    "    word_tag_counts = Counter()\n",
    "    all_pos_tags = set()\n",
    "    \n",
    "    print(\"Building robust PCFG from POS tags...\")\n",
    "    for sent in tqdm(sentences):\n",
    "        try:\n",
    "            # Tokenize and POS tag\n",
    "            tokens = word_tokenize(sent)\n",
    "            if not tokens:\n",
    "                continue\n",
    "                \n",
    "            pos_tags = pos_tag(tokens)\n",
    "            \n",
    "            # Create word -> POS productions (keep original case for proper nouns)\n",
    "            for word, pos in pos_tags:\n",
    "                all_pos_tags.add(pos)\n",
    "                # Keep original case for proper nouns, lowercase for others\n",
    "                if pos.startswith('NNP'):\n",
    "                    word_clean = word\n",
    "                else:\n",
    "                    word_clean = word.lower()\n",
    "                word_tag_counts[(pos, word_clean)] += 1\n",
    "            \n",
    "            # Create phrase structure rules\n",
    "            if len(pos_tags) > 0:\n",
    "                pos_sequence = [pos for word, pos in pos_tags]\n",
    "                \n",
    "                # S -> sequence of POS tags\n",
    "                s_production = (Nonterminal('S'), tuple(Nonterminal(pos) for pos in pos_sequence))\n",
    "                production_counts[s_production] += 1\n",
    "                \n",
    "                # Create more flexible rules for common patterns\n",
    "                # Basic sentence patterns\n",
    "                if len(pos_sequence) >= 2:\n",
    "                    # S -> NP VP (if we can identify NP and VP regions)\n",
    "                    np_end = -1\n",
    "                    vp_start = -1\n",
    "                    \n",
    "                    # Find potential NP end and VP start\n",
    "                    for i, pos in enumerate(pos_sequence):\n",
    "                        if pos.startswith('N') and np_end == -1:\n",
    "                            np_end = i\n",
    "                        elif pos.startswith('V') and vp_start == -1:\n",
    "                            vp_start = i\n",
    "                            break\n",
    "                    \n",
    "                    if np_end >= 0 and vp_start > np_end:\n",
    "                        # Add S -> NP VP rule\n",
    "                        production_counts[(Nonterminal('S'), (Nonterminal('NP'), Nonterminal('VP')))] += 1\n",
    "                        \n",
    "                        # Add NP -> sequence from start to np_end\n",
    "                        np_seq = pos_sequence[:np_end + 1]\n",
    "                        if len(np_seq) > 0:\n",
    "                            production_counts[(Nonterminal('NP'), tuple(Nonterminal(pos) for pos in np_seq))] += 1\n",
    "                        \n",
    "                        # Add VP -> sequence from vp_start to end\n",
    "                        vp_seq = pos_sequence[vp_start:]\n",
    "                        if len(vp_seq) > 0:\n",
    "                            production_counts[(Nonterminal('VP'), tuple(Nonterminal(pos) for pos in vp_seq))] += 1\n",
    "                \n",
    "                # Add some common short patterns\n",
    "                common_patterns = [\n",
    "                    # Determiner + Noun patterns\n",
    "                    (['DT', 'NN'], 'NP'),\n",
    "                    (['DT', 'NNS'], 'NP'),\n",
    "                    (['DT', 'NNP'], 'NP'),\n",
    "                    (['DT', 'JJ', 'NN'], 'NP'),\n",
    "                    (['DT', 'JJ', 'NNS'], 'NP'),\n",
    "                    # Verb patterns\n",
    "                    (['VBZ', 'DT', 'NN'], 'VP'),\n",
    "                    (['VBD', 'DT', 'NN'], 'VP'),\n",
    "                    (['VB', 'DT', 'NN'], 'VP'),\n",
    "                    # Prepositional phrases\n",
    "                    (['IN', 'DT', 'NN'], 'PP'),\n",
    "                    (['IN', 'NNP'], 'PP'),\n",
    "                ]\n",
    "                \n",
    "                # Check for these patterns in the sentence\n",
    "                for pattern_pos, pattern_lhs in common_patterns:\n",
    "                    for i in range(len(pos_sequence) - len(pattern_pos) + 1):\n",
    "                        if pos_sequence[i:i+len(pattern_pos)] == pattern_pos:\n",
    "                            pattern_rhs = tuple(Nonterminal(pos) for pos in pattern_pos)\n",
    "                            production_counts[(Nonterminal(pattern_lhs), pattern_rhs)] += 1\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sentence: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to probabilistic productions\n",
    "    prob_productions = []\n",
    "    \n",
    "    # Word -> POS productions (most important - these must be comprehensive)\n",
    "    pos_counts = defaultdict(int)\n",
    "    for (pos, word), count in word_tag_counts.items():\n",
    "        pos_counts[pos] += count\n",
    "    \n",
    "    # Add ALL word productions (no filtering by count for lexical rules)\n",
    "    for (pos, word), count in word_tag_counts.items():\n",
    "        prob = count / pos_counts[pos]\n",
    "        prob_prod = ProbabilisticProduction(Nonterminal(pos), [word], prob=prob)\n",
    "        prob_productions.append(prob_prod)\n",
    "    \n",
    "    # Add unknown word handling - create low-probability productions for common POS tags\n",
    "    common_pos_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', \n",
    "                       'JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS', 'DT', 'IN', 'CC', 'TO', 'PRP', 'MD']\n",
    "    \n",
    "    unknown_prob = 0.0001  # Very low probability for unknown words\n",
    "    for pos in common_pos_tags:\n",
    "        if pos in all_pos_tags:  # Only add if we've seen this POS tag\n",
    "            # Add a generic unknown word production\n",
    "            prob_prod = ProbabilisticProduction(Nonterminal(pos), ['<UNK>'], prob=unknown_prob)\n",
    "            prob_productions.append(prob_prod)\n",
    "    \n",
    "    # Phrase structure productions\n",
    "    lhs_counts = defaultdict(int)\n",
    "    for (lhs, rhs), count in production_counts.items():\n",
    "        lhs_counts[lhs] += count\n",
    "    \n",
    "    for (lhs, rhs), count in production_counts.items():\n",
    "        prob = count / lhs_counts[lhs]\n",
    "        if prob >= min_prob:\n",
    "            prob_prod = ProbabilisticProduction(lhs, list(rhs), prob=prob)\n",
    "            prob_productions.append(prob_prod)\n",
    "    \n",
    "    # Add some basic fallback rules if we don't have enough structure\n",
    "    if len([p for p in prob_productions if not all(isinstance(sym, str) for sym in p.rhs())]) < 10:\n",
    "        print(\"Adding fallback structural rules...\")\n",
    "        # Add very basic fallback rules\n",
    "        fallback_rules = [\n",
    "            (Nonterminal('S'), [Nonterminal('NP')], 0.1),\n",
    "            (Nonterminal('S'), [Nonterminal('VP')], 0.1),\n",
    "            (Nonterminal('NP'), [Nonterminal('NN')], 0.3),\n",
    "            (Nonterminal('NP'), [Nonterminal('NNP')], 0.3),\n",
    "            (Nonterminal('VP'), [Nonterminal('VB')], 0.3),\n",
    "            (Nonterminal('VP'), [Nonterminal('VBZ')], 0.3),\n",
    "        ]\n",
    "        \n",
    "        for lhs, rhs, prob in fallback_rules:\n",
    "            prob_prod = ProbabilisticProduction(lhs, rhs, prob=prob)\n",
    "            prob_productions.append(prob_prod)\n",
    "    \n",
    "    print(f\"Robust PCFG: {len(prob_productions)} productions\")\n",
    "    print(f\"Lexical productions: {len([p for p in prob_productions if all(isinstance(sym, str) for sym in p.rhs())])}\")\n",
    "    print(f\"Structural productions: {len([p for p in prob_productions if not all(isinstance(sym, str) for sym in p.rhs())])}\")\n",
    "    \n",
    "    return PCFG(Nonterminal('S'), prob_productions)\n",
    "\n",
    "# Enhanced parsing function with unknown word handling\n",
    "def parse_with_unknown_word_handling(parser, tokens, pcfg):\n",
    "    \"\"\"\n",
    "    Parse tokens with unknown word handling by replacing unknown words with <UNK>\n",
    "    \"\"\"\n",
    "    # Get all words in the grammar\n",
    "    grammar_words = set()\n",
    "    for prod in pcfg.productions():\n",
    "        if all(isinstance(sym, str) for sym in prod.rhs()):\n",
    "            grammar_words.update(prod.rhs())\n",
    "    \n",
    "    # Replace unknown words with <UNK>\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.lower() in grammar_words or token in grammar_words:\n",
    "            processed_tokens.append(token.lower() if not token[0].isupper() else token)\n",
    "        else:\n",
    "            processed_tokens.append('<UNK>')\n",
    "    \n",
    "    return list(parser.parse(processed_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "862d76f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from nltk.parse.pchart import InsideChartParser\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d0191e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. build the beam parser once (tune beam_size as you like)\n",
    "# beam_parser = InsideChartParser(pcfg, beam_size=50)\n",
    "\n",
    "# with open('dataset/REAL_Corpus/real_corpus_no_symbol.pcfg', 'w', encoding='utf-8') as fout:\n",
    "#     for sent in tqdm(sentences_clean, desc=\"Beam parsing\"):\n",
    "#         tokens = sent.split()\n",
    "#         # parse(tokens) yields all parses that survived the beam, in prob order\n",
    "#         # here we grab just the 1 best; to get top K, change `1` to K\n",
    "#         best_trees = list(islice(beam_parser.parse(tokens), 1))\n",
    "#         if best_trees:\n",
    "#             # write the single best bracketed tree\n",
    "#             fout.write(best_trees[0].pformat() + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0bc57d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.grammar import PCFG, ProbabilisticProduction, Nonterminal\n",
    "from nltk.parse import ViterbiParser, InsideChartParser\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3ea3add8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building robust PCFG from POS tags...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094b6f4680454790ae424df725186ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust PCFG: 1888 productions\n",
      "Lexical productions: 1234\n",
      "Structural productions: 654\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Productions for NP do not sum to 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m sentences_clean \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m     re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, s)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m df_real_split_filtered[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m      7\u001b[0m ]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# build your robust PCFG\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m pcfg \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_pcfg_from_pos_tagged\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msentences_clean\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m viterbi \u001b[38;5;241m=\u001b[39m ViterbiParser(pcfg)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# now parse with unknown word handling\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[100], line 172\u001b[0m, in \u001b[0;36mbuild_pcfg_from_pos_tagged\u001b[0;34m(sentences, min_prob)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLexical productions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m([p\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mprob_productions\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(sym,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39msym\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;241m.\u001b[39mrhs())])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStructural productions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m([p\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mprob_productions\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(sym,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39msym\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;241m.\u001b[39mrhs())])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPCFG\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNonterminal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprob_productions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/populate-ns-lex/.venv-benepar/lib/python3.9/site-packages/nltk/grammar.py:1237\u001b[0m, in \u001b[0;36mPCFG.__init__\u001b[0;34m(self, start, productions, calculate_leftcorners)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lhs, p \u001b[38;5;129;01min\u001b[39;00m probs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m PCFG\u001b[38;5;241m.\u001b[39mEPSILON) \u001b[38;5;241m<\u001b[39m p \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m PCFG\u001b[38;5;241m.\u001b[39mEPSILON)):\n\u001b[0;32m-> 1237\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProductions for \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m do not sum to 1\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m lhs)\n",
      "\u001b[0;31mValueError\u001b[0m: Productions for NP do not sum to 1"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "# clean your sentences\n",
    "sentences_clean = [\n",
    "    re.sub(r\"[^\\w\\s]\", \"\", s)\n",
    "    for s in df_real_split_filtered['annotation']\n",
    "    if isinstance(s, str)\n",
    "]\n",
    "\n",
    "# build your robust PCFG\n",
    "pcfg = build_pcfg_from_pos_tagged(\n",
    "    sentences_clean[:500],\n",
    "    min_prob=0.001\n",
    ")\n",
    "\n",
    "viterbi = ViterbiParser(pcfg)\n",
    "\n",
    "# now parse with unknown word handling\n",
    "for sent in tqdm(sentences_clean[500:550], desc=\"Viterbi parsing\"):\n",
    "    tokens = sent.split()\n",
    "    \n",
    "    # Use the enhanced parsing function\n",
    "    trees = parse_with_unknown_word_handling(viterbi, tokens, pcfg)\n",
    "    \n",
    "    if trees:\n",
    "        print(f\"\\n{sent!r} \", trees[0])\n",
    "    else:\n",
    "        print(f\"\\n{sent!r}  No parse found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8203a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('dataset/REAL_Corpus/real_corpus_no_symbol.cfg', 'w', encoding='utf-8') as f:\n",
    "#     for prod in cfg.productions():\n",
    "#         f.write(str(prod) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0c58b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33e8260c52b464589b12259d8533bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing sentences:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentence: Pub called Deacon Brodies Tavern\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x10749a250>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/glora/projects/populate-ns-lex/.venv-benepar/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "parser = EarleyChartParser(cfg)\n",
    "\n",
    "# parse first 10 sentences with a tqdm progress bar\n",
    "for sentence in tqdm(sentences_clean[:10],\n",
    "                     desc=\"Parsing sentences\"):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    print(f\"Parsing sentence: {sentence}\")\n",
    "    for tree in parser.parse(tokens):\n",
    "        print(tree)\n",
    "        tree.pretty_print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e75efd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a8a341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be82e748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67f2cc69",
   "metadata": {},
   "source": [
    "Should I sample refclef with 1245 sentences as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca9209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f59b972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1a5d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e9ff43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272c9c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527a6fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f70ec7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8ee0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c774c58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.to_csv of       userid  age  gender photoid    x    y                                                                                                                                    annotation     status  phrase length  validator_userid  validator_age validator_gender\n",
       "0          2    4    male   img23  495  157                                                                                                            Pub called 'Deacon Brodie's Tavern    correct            123                26              6             male\n",
       "1          2    4    male   img23  495  157                                                       Black and white traditional building with sign hanging outside and 3 big arched windows    correct            123                26              6             male\n",
       "2          2    4    male   img31    0    0                                                                                      Traditional early Victorian terrace on 3 floors + dormer  ambiguous            147                 7              2           female\n",
       "3          2    4    male   img31    0    0                                                                                                     Third in from the left of the end terrace  ambiguous            147                 7              2           female\n",
       "4          2    4    male   img31    0    0                                                                                                  Royal Blue Door, Black, spiked iron railings  ambiguous            147                 7              2           female\n",
       "...      ...  ...     ...     ...  ...  ...                                                                                                                                           ...        ...            ...               ...            ...              ...\n",
       "1709     234    4  female   img15  611  379  There is a woman walking on the pavement with a blue-gray shirt wearing weird orange pants, or is she standing behind an orange wheelie bin?  incorrect            140               238              2             male\n",
       "1710     234    4  female   img10  240  330                                                                                                     The cream painted shop next to the church  incorrect             87               240              2             male\n",
       "1711     234    4  female   img10  240  330                                                                                                   It is just opposite the pedestrian crossing  incorrect             87               240              2             male\n",
       "1713     234    4  female   img14  273  427                                                    the statue at the end of the road, just in front of the building with the dome shaped roof    correct             90               241              2           female\n",
       "1714     234    4  female   img24  800  415                                                                                               The small, dark entrance just below the Saltire  incorrect             47               239              2             male\n",
       "\n",
       "[1245 rows x 12 columns]>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_real_split.to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832af27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     nltk.data.find('taggers/averaged_perceptron_tagger_eng')\n",
    "#     print(\"averaged_perceptron_tagger_eng is installed \")\n",
    "# except LookupError:\n",
    "#     print(\"averaged_perceptron_tagger is NOT installed  (LookupError raised)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fe00c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagger works!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nltk.pos_tag(\"This is a test\".split())\n",
    "    print(\"Tagger works!\")\n",
    "except LookupError as e:\n",
    "    print(\"LookupError:\", e)\n",
    "    print(\"So the tagger isnt installed yet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03078f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.pos_tag() uses the Penn Treebank Tag Set.\n",
    "unique_sents = df_real_split['annotation'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395e9a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NNP Black)\n",
      "  (CC and)\n",
      "  (JJ white)\n",
      "  (JJ traditional)\n",
      "  (NN building)\n",
      "  (IN with)\n",
      "  (NN sign)\n",
      "  (VBG hanging)\n",
      "  (JJ outside)\n",
      "  (CC and)\n",
      "  (CD 3)\n",
      "  (JJ big)\n",
      "  (VBD arched)\n",
      "  (NNS windows))\n"
     ]
    }
   ],
   "source": [
    "# # generate cfg rule dump constituent tree as grammar:\n",
    "\n",
    "# S = Nonterminal('S')\n",
    "# prod_set = set()\n",
    "\n",
    "# for sent in unique_sents:\n",
    "#     tokens = sent.split()\n",
    "\n",
    "#     tagged = nltk.pos_tag(tokens)  \n",
    "#     # build the S  POS1 POS2  production\n",
    "#     rhs_pos = [Nonterminal(pos) for (_tok, pos) in tagged]\n",
    "#     prod_set.add(Production(S, rhs_pos))\n",
    "\n",
    "#     # build each POS  \"word\" production\n",
    "#     for word, pos in tagged:\n",
    "#         prod_set.add(Production(Nonterminal(pos), [word]))\n",
    "\n",
    "\n",
    "# grammar = CFG(S, list(prod_set))\n",
    "# # print(grammar)\n",
    "# parser = EarleyChartParser(grammar)\n",
    "\n",
    "# # start_sym = trees[0].label()           # usually 'S'\n",
    "\n",
    "# # prods = list({ p for p in all_prods })  # set-dedupe, original producing 2000 cfg rules\n",
    "# # grammar = CFG(Nonterminal(start_sym), prods)\n",
    "\n",
    "\n",
    "# # test on the first sentence\n",
    "# test_sent = unique_sents_unpunct[1].split()\n",
    "# for tree in parser.parse(test_sent):\n",
    "#     print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b53a61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S = Nonterminal('S')\n",
    "# prods = []\n",
    "\n",
    "# for sent in unique_sents:\n",
    "#     tagged = nltk.pos_tag(sent.split())\n",
    "#     prev_nt = S\n",
    "\n",
    "#     # for each token except the last, build NT_i  POS_i NT_{i+1}\n",
    "#     for i, (w, pos) in enumerate(tagged[:-1]):\n",
    "#         curr_nt = Nonterminal(f\"X{i}\")\n",
    "#         prods.append(Production(prev_nt, [Nonterminal(pos), curr_nt]))\n",
    "#         prods.append(Production(Nonterminal(pos), [w]))\n",
    "#         prev_nt = curr_nt\n",
    "\n",
    "#     # final link: X_{n-1}  POS_n\n",
    "#     last_word, last_pos = tagged[-1]\n",
    "#     prods.append(Production(prev_nt, [Nonterminal(last_pos)]))\n",
    "#     prods.append(Production(Nonterminal(last_pos), [last_word]))\n",
    "\n",
    "# grammar = CFG(S, prods)\n",
    "# parser = EarleyChartParser(grammar)\n",
    "\n",
    "# print(grammar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0615b9d",
   "metadata": {},
   "source": [
    "#### spaCy+benepar PCFG Neural Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9974a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d7b5007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Pub called 'Deacon Brodie's Tavern.\", 'Black and white traditional building with sign hanging outside and 3 big arched windows', 'Traditional early Victorian terrace on 3 floors + dormer.', 'Third in from the left of the end terrace.', 'Royal Blue Door, Black, spiked iron railings.', 'Large, modern glass fronted building, butted up against traditional victorian terrace, slightly set back from road, and with facing bowed frontage.', 'Two story traditional stone building, on the left of the street as faced, just before the bed.', 'Two large white double doors.', 'Church in the middle of a small square/island - no spire but with large stained glass window in gable end, slightly obscured by trees.', 'Building with columns behind the one in front, also in similar architecture style.']\n"
     ]
    }
   ],
   "source": [
    "df_real_split_filtered['annotation'].shape\n",
    "real_split_annotations = df_real_split_filtered['annotation'].tolist()\n",
    "\n",
    "print(real_split_annotations[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ff2db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/glora/projects/populate-ns-lex/.venv-benepar/lib/python3.9/site-packages/benepar/parse_chart.py:169: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<benepar.integrations.spacy_plugin.BeneparComponent at 0x17cd24eb0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# benepar.download(\"benepar_en3\")\n",
    "nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9c4b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# punct_tags = {\",\", \":\", \"``\", \"''\", \".\", \"(\", \")\", \"--\", \"``\", \"+\"}\n",
    "\n",
    "# unique_sents_unpunct = [\n",
    "#     sent for sent in unique_sents if not any(tag in sent for tag in punct_tags) # reduced 50% rules\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de46f959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b3c1238a1f4c55b2b4bae0e959ac97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing docs: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "/Users/glora/projects/populate-ns-lex/.venv-benepar/lib/python3.9/site-packages/torch/distributions/distribution.py:55: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 2. collect all the parse trees with a progress bar\n",
    "clean_annotations = [\" \".join(s.split()) for s in real_split_annotations]\n",
    "\n",
    "trees = []\n",
    "for doc in tqdm(nlp.pipe(clean_annotations), desc=\"Parsing docs\"):\n",
    "    for sent in doc.sents:\n",
    "        bracketed = sent._.parse_string\n",
    "        trees.append(Tree.fromstring(bracketed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b30a361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NP -> NP VP .,\n",
       " NP -> NN,\n",
       " NN -> 'Pub',\n",
       " VP -> VBN S,\n",
       " VBN -> 'called',\n",
       " S -> `` NP,\n",
       " `` -> \"'\",\n",
       " NP -> NP NNP,\n",
       " NP -> NNP NNP POS,\n",
       " NNP -> 'Deacon',\n",
       " NNP -> 'Brodie',\n",
       " POS -> \"'s\",\n",
       " NNP -> 'Tavern',\n",
       " . -> '.',\n",
       " NP -> NP PP,\n",
       " NP -> ADJP JJ NN,\n",
       " ADJP -> JJ CC JJ,\n",
       " JJ -> 'Black',\n",
       " CC -> 'and',\n",
       " JJ -> 'white',\n",
       " JJ -> 'traditional',\n",
       " NN -> 'building',\n",
       " PP -> IN NP,\n",
       " IN -> 'with',\n",
       " NP -> NP CC NP,\n",
       " NP -> NP VP,\n",
       " NP -> NN,\n",
       " NN -> 'sign',\n",
       " VP -> VBG ADVP,\n",
       " VBG -> 'hanging',\n",
       " ADVP -> RB,\n",
       " RB -> 'outside',\n",
       " CC -> 'and',\n",
       " NP -> CD JJ JJ NNS,\n",
       " CD -> '3',\n",
       " JJ -> 'big',\n",
       " JJ -> 'arched',\n",
       " NNS -> 'windows',\n",
       " NP -> NP PP .,\n",
       " NP -> JJ ADJP NN,\n",
       " JJ -> 'Traditional',\n",
       " ADJP -> JJ JJ,\n",
       " JJ -> 'early',\n",
       " JJ -> 'Victorian',\n",
       " NN -> 'terrace',\n",
       " PP -> IN NP,\n",
       " IN -> 'on',\n",
       " NP -> NP CC NP,\n",
       " NP -> CD NNS,\n",
       " CD -> '3',\n",
       " NNS -> 'floors',\n",
       " CC -> '+',\n",
       " NP -> JJR,\n",
       " JJR -> 'dormer',\n",
       " . -> '.',\n",
       " FRAG -> ADVP PP .,\n",
       " ADVP -> JJ RB,\n",
       " JJ -> 'Third',\n",
       " RB -> 'in',\n",
       " PP -> IN NP,\n",
       " IN -> 'from',\n",
       " NP -> NP PP,\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'left',\n",
       " PP -> IN NP,\n",
       " IN -> 'of',\n",
       " NP -> DT NN NN,\n",
       " DT -> 'the',\n",
       " NN -> 'end',\n",
       " NN -> 'terrace',\n",
       " . -> '.',\n",
       " NP -> NP , NP , NP .,\n",
       " NP -> NNP NNP NNP,\n",
       " NNP -> 'Royal',\n",
       " NNP -> 'Blue',\n",
       " NNP -> 'Door',\n",
       " , -> ',',\n",
       " NP -> JJ,\n",
       " JJ -> 'Black',\n",
       " , -> ',',\n",
       " NP -> JJ NN NNS,\n",
       " JJ -> 'spiked',\n",
       " NN -> 'iron',\n",
       " NNS -> 'railings',\n",
       " . -> '.',\n",
       " NP -> NP , VP .,\n",
       " NP -> JJ , JJ ADJP NN,\n",
       " JJ -> 'Large',\n",
       " , -> ',',\n",
       " JJ -> 'modern',\n",
       " ADJP -> NN VBN,\n",
       " NN -> 'glass',\n",
       " VBN -> 'fronted',\n",
       " NN -> 'building',\n",
       " , -> ',',\n",
       " VP -> VP , CC PP,\n",
       " VP -> VBN PRT PP,\n",
       " VBN -> 'butted',\n",
       " PRT -> RP,\n",
       " RP -> 'up',\n",
       " PP -> PP , VP,\n",
       " PP -> IN NP,\n",
       " IN -> 'against',\n",
       " NP -> JJ JJ NN,\n",
       " JJ -> 'traditional',\n",
       " JJ -> 'victorian',\n",
       " NN -> 'terrace',\n",
       " , -> ',',\n",
       " VP -> ADVP VBN ADVP,\n",
       " ADVP -> RB,\n",
       " RB -> 'slightly',\n",
       " VBN -> 'set',\n",
       " ADVP -> ADVP PP,\n",
       " ADVP -> RB,\n",
       " RB -> 'back',\n",
       " PP -> IN NP,\n",
       " IN -> 'from',\n",
       " NP -> NN,\n",
       " NN -> 'road',\n",
       " , -> ',',\n",
       " CC -> 'and',\n",
       " PP -> IN NP,\n",
       " IN -> 'with',\n",
       " NP -> VBG JJ NN,\n",
       " VBG -> 'facing',\n",
       " JJ -> 'bowed',\n",
       " NN -> 'frontage',\n",
       " . -> '.',\n",
       " NP -> NP , PP , PP .,\n",
       " NP -> NML JJ NN NN,\n",
       " NML -> CD NN,\n",
       " CD -> 'Two',\n",
       " NN -> 'story',\n",
       " JJ -> 'traditional',\n",
       " NN -> 'stone',\n",
       " NN -> 'building',\n",
       " , -> ',',\n",
       " PP -> IN NP,\n",
       " IN -> 'on',\n",
       " NP -> NP PP SBAR,\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'left',\n",
       " PP -> IN NP,\n",
       " IN -> 'of',\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'street',\n",
       " SBAR -> IN S,\n",
       " IN -> 'as',\n",
       " S -> VP,\n",
       " VP -> VBN,\n",
       " VBN -> 'faced',\n",
       " , -> ',',\n",
       " PP -> ADVP IN NP,\n",
       " ADVP -> RB,\n",
       " RB -> 'just',\n",
       " IN -> 'before',\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'bed',\n",
       " . -> '.',\n",
       " NP -> CD JJ JJ JJ NNS .,\n",
       " CD -> 'Two',\n",
       " JJ -> 'large',\n",
       " JJ -> 'white',\n",
       " JJ -> 'double',\n",
       " NNS -> 'doors',\n",
       " . -> '.',\n",
       " NP -> NP : NP CC PP .,\n",
       " NP -> NP PP,\n",
       " NP -> NN,\n",
       " NN -> 'Church',\n",
       " PP -> IN NP,\n",
       " IN -> 'in',\n",
       " NP -> NP PP,\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'middle',\n",
       " PP -> IN NP,\n",
       " IN -> 'of',\n",
       " NP -> DT JJ NN SYM NN,\n",
       " DT -> 'a',\n",
       " JJ -> 'small',\n",
       " NN -> 'square',\n",
       " SYM -> '/',\n",
       " NN -> 'island',\n",
       " : -> '-',\n",
       " NP -> DT NN,\n",
       " DT -> 'no',\n",
       " NN -> 'spire',\n",
       " CC -> 'but',\n",
       " PP -> IN NP,\n",
       " IN -> 'with',\n",
       " NP -> NP , VP,\n",
       " NP -> NP PP,\n",
       " NP -> JJ NML NN,\n",
       " JJ -> 'large',\n",
       " NML -> VBN NN,\n",
       " VBN -> 'stained',\n",
       " NN -> 'glass',\n",
       " NN -> 'window',\n",
       " PP -> IN NP,\n",
       " IN -> 'in',\n",
       " NP -> JJ NN,\n",
       " JJ -> 'gable',\n",
       " NN -> 'end',\n",
       " , -> ',',\n",
       " VP -> ADVP VBN PP,\n",
       " ADVP -> RB,\n",
       " RB -> 'slightly',\n",
       " VBN -> 'obscured',\n",
       " PP -> IN NP,\n",
       " IN -> 'by',\n",
       " NP -> NNS,\n",
       " NNS -> 'trees',\n",
       " . -> '.',\n",
       " NP -> NP PP .,\n",
       " NP -> NN,\n",
       " NN -> 'Building',\n",
       " PP -> PP , PP,\n",
       " PP -> IN NP,\n",
       " IN -> 'with',\n",
       " NP -> NP PP,\n",
       " NP -> NNS,\n",
       " NNS -> 'columns',\n",
       " PP -> IN NP,\n",
       " IN -> 'behind',\n",
       " NP -> NP PP,\n",
       " NP -> DT CD,\n",
       " DT -> 'the',\n",
       " CD -> 'one',\n",
       " PP -> IN NP,\n",
       " IN -> 'in',\n",
       " NP -> NN,\n",
       " NN -> 'front',\n",
       " , -> ',',\n",
       " PP -> ADVP IN NP,\n",
       " ADVP -> RB,\n",
       " RB -> 'also',\n",
       " IN -> 'in',\n",
       " NP -> JJ NN NN,\n",
       " JJ -> 'similar',\n",
       " NN -> 'architecture',\n",
       " NN -> 'style',\n",
       " . -> '.',\n",
       " NP -> NP , ADVP , NP,\n",
       " NP -> NP PP,\n",
       " NP -> JJ NN,\n",
       " JJ -> 'End',\n",
       " NN -> 'centre',\n",
       " PP -> IN NP,\n",
       " IN -> 'of',\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'street',\n",
       " , -> ',',\n",
       " ADVP -> RB RB,\n",
       " RB -> 'straight',\n",
       " RB -> 'ahead',\n",
       " , -> ',',\n",
       " NP -> NP PP,\n",
       " NP -> JJ NN,\n",
       " JJ -> 'big',\n",
       " NN -> 'building',\n",
       " PP -> IN NP,\n",
       " IN -> 'with',\n",
       " NP -> NP CC NP,\n",
       " NP -> DT NNS,\n",
       " DT -> 'a',\n",
       " NNS -> 'columns',\n",
       " CC -> 'and',\n",
       " NP -> DT NN NN,\n",
       " DT -> 'a',\n",
       " NN -> 'dome',\n",
       " NN -> 'roof',\n",
       " NP -> NP , NP,\n",
       " NP -> NP PP,\n",
       " NP -> JJ NN NN,\n",
       " JJ -> 'Grey',\n",
       " NN -> 'building',\n",
       " NN -> 'block',\n",
       " PP -> IN NP,\n",
       " IN -> 'with',\n",
       " NP -> NP PP,\n",
       " NP -> NNS,\n",
       " NNS -> 'statues',\n",
       " PP -> IN NP,\n",
       " IN -> 'above',\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'entrance',\n",
       " , -> ',',\n",
       " NP -> NP PP,\n",
       " NP -> NNP NN,\n",
       " NNP -> 'Amarone',\n",
       " NN -> 'restaurant',\n",
       " PP -> IN NP,\n",
       " IN -> 'on',\n",
       " NP -> DT JJ NN,\n",
       " DT -> 'the',\n",
       " JJ -> 'bottom',\n",
       " NN -> 'floor',\n",
       " NP -> NP PP .,\n",
       " NP -> JJ JJ NN,\n",
       " JJ -> 'Big',\n",
       " JJ -> 'historical',\n",
       " NN -> 'building',\n",
       " PP -> IN NP,\n",
       " IN -> 'with',\n",
       " NP -> NP PP,\n",
       " NP -> JJ NNS CC NN,\n",
       " JJ -> 'huge',\n",
       " NNS -> 'columns',\n",
       " CC -> 'and',\n",
       " NN -> 'statue',\n",
       " PP -> IN NP,\n",
       " IN -> 'at',\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'top',\n",
       " . -> '.',\n",
       " NP -> NP PP PP PP .,\n",
       " NP -> JJ JJ NN,\n",
       " JJ -> 'Middle',\n",
       " JJ -> 'vertical',\n",
       " NN -> 'section',\n",
       " PP -> IN NP,\n",
       " IN -> 'of',\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'house',\n",
       " PP -> PP PP,\n",
       " PP -> IN NP,\n",
       " IN -> 'above',\n",
       " NP -> DT JJ NN,\n",
       " DT -> 'the',\n",
       " JJ -> 'brown',\n",
       " NN -> 'door',\n",
       " PP -> IN NP,\n",
       " IN -> 'of',\n",
       " NP -> DT NN NN,\n",
       " DT -> 'the',\n",
       " NN -> 'housing',\n",
       " NN -> 'block',\n",
       " PP -> IN NP,\n",
       " IN -> 'with',\n",
       " NP -> NP PP,\n",
       " NP -> NNP NNP,\n",
       " NNP -> 'Eigin',\n",
       " NNP -> 'Cashmere',\n",
       " PP -> IN NP,\n",
       " IN -> 'on',\n",
       " NP -> DT JJ NN,\n",
       " DT -> 'the',\n",
       " JJ -> 'ground',\n",
       " NN -> 'floor',\n",
       " . -> '.',\n",
       " NP -> NP PP,\n",
       " NP -> JJ NN,\n",
       " JJ -> 'Small',\n",
       " NN -> 'shed',\n",
       " PP -> IN NP,\n",
       " IN -> 'with',\n",
       " NP -> NN NN,\n",
       " NN -> 'lead',\n",
       " NN -> 'roof',\n",
       " NP -> NP ADVP,\n",
       " NP -> ADJP NN NN,\n",
       " ADJP -> JJ CC JJ,\n",
       " JJ -> 'Red',\n",
       " CC -> 'and',\n",
       " JJ -> 'gold',\n",
       " NN -> 'coffee',\n",
       " NN -> 'shop',\n",
       " ADVP -> JJ PP,\n",
       " JJ -> 'next',\n",
       " PP -> IN NP,\n",
       " IN -> 'to',\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'bridge',\n",
       " NP -> NP PP .,\n",
       " NP -> JJ ADJP NN SYM NN,\n",
       " JJ -> 'Small',\n",
       " ADJP -> NN VBN,\n",
       " NN -> 'stone',\n",
       " VBN -> 'capped',\n",
       " NN -> 'monument',\n",
       " SYM -> '/',\n",
       " NN -> 'fountain',\n",
       " PP -> IN NP,\n",
       " IN -> 'with',\n",
       " NP -> NP VP,\n",
       " NP -> DT NN,\n",
       " DT -> 'a',\n",
       " NN -> 'plaque',\n",
       " VP -> VBN PP,\n",
       " VBN -> 'built',\n",
       " PP -> IN NP,\n",
       " IN -> 'on',\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'pavement',\n",
       " . -> '.',\n",
       " NP -> NP SBAR,\n",
       " NP -> NP PP,\n",
       " NP -> JJ JJ NN,\n",
       " JJ -> 'Enormous',\n",
       " JJ -> 'Gothic',\n",
       " NN -> 'tower',\n",
       " PP -> IN NP,\n",
       " IN -> 'in',\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'park',\n",
       " SBAR -> WHNP S,\n",
       " WHNP -> WDT,\n",
       " WDT -> 'that',\n",
       " S -> VP,\n",
       " VP -> VBZ VP,\n",
       " VBZ -> 'is',\n",
       " VP -> VBN ADVP,\n",
       " VBN -> 'stained',\n",
       " ADVP -> JJ,\n",
       " JJ -> 'black',\n",
       " NP -> NP PP,\n",
       " NP -> NN NN,\n",
       " NN -> 'Coffee',\n",
       " NN -> 'shop',\n",
       " PP -> IN NP,\n",
       " IN -> 'to',\n",
       " NP -> NP PP,\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'left',\n",
       " PP -> IN NP,\n",
       " IN -> 'of',\n",
       " NP -> DT VBN NN,\n",
       " DT -> 'the',\n",
       " VBN -> 'converted',\n",
       " NN -> 'church',\n",
       " NP -> NP PP PP .,\n",
       " NP -> DT NNS,\n",
       " DT -> 'The',\n",
       " NNS -> 'windows',\n",
       " PP -> IN NP,\n",
       " IN -> 'above',\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'restaurant',\n",
       " PP -> IN NP,\n",
       " IN -> 'with',\n",
       " NP -> NP ADVP,\n",
       " NP -> DT JJ NN NN,\n",
       " DT -> 'the',\n",
       " JJ -> 'black',\n",
       " NN -> 'street',\n",
       " NN -> 'furniture',\n",
       " ADVP -> JJ PP,\n",
       " JJ -> 'next',\n",
       " PP -> PP PP,\n",
       " PP -> IN NP,\n",
       " IN -> 'to',\n",
       " NP -> DT NNP,\n",
       " DT -> 'the',\n",
       " NNP -> 'RBS',\n",
       " PP -> IN NP,\n",
       " IN -> 'on',\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'corner',\n",
       " . -> '.',\n",
       " NP -> NP PP .,\n",
       " NP -> DT ADJP JJ NN,\n",
       " DT -> 'The',\n",
       " ADJP -> JJ VBG,\n",
       " JJ -> 'Austrian',\n",
       " VBG -> 'looking',\n",
       " JJ -> 'white',\n",
       " NN -> 'house',\n",
       " PP -> IN NP,\n",
       " IN -> 'with',\n",
       " NP -> NP PP,\n",
       " NP -> DT JJ JJ NNS,\n",
       " DT -> 'the',\n",
       " JJ -> 'dark',\n",
       " JJ -> 'wooden',\n",
       " NNS -> 'beams',\n",
       " PP -> IN NP,\n",
       " IN -> 'at',\n",
       " NP -> DT NN NN,\n",
       " DT -> 'the',\n",
       " NN -> 'water',\n",
       " NN -> 'side',\n",
       " . -> '.',\n",
       " S -> NP JJ VP .,\n",
       " NP -> DT NN,\n",
       " DT -> 'The',\n",
       " NN -> 'castle',\n",
       " JJ -> 'like',\n",
       " VP -> VBP PP,\n",
       " VBP -> 'tower',\n",
       " PP -> IN NP,\n",
       " IN -> 'on',\n",
       " NP -> NP ADVP,\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'corner',\n",
       " ADVP -> JJ PP,\n",
       " JJ -> 'next',\n",
       " PP -> IN NP,\n",
       " IN -> 'to',\n",
       " NP -> NP PP,\n",
       " NP -> DT NNP NNP NNP,\n",
       " DT -> 'the',\n",
       " NNP -> 'Tartan',\n",
       " NNP -> 'Weaving',\n",
       " NNP -> 'Mill',\n",
       " PP -> IN NP,\n",
       " IN -> 'on',\n",
       " NP -> DT NNP NNP,\n",
       " DT -> 'the',\n",
       " NNP -> 'Royal',\n",
       " NNP -> 'Mile',\n",
       " . -> '.',\n",
       " NP -> NP VP .,\n",
       " NP -> DT JJ NN,\n",
       " DT -> 'The',\n",
       " JJ -> 'greenish',\n",
       " NN -> 'statue',\n",
       " VP -> VBG PP PP,\n",
       " VBG -> 'standing',\n",
       " PP -> IN NP,\n",
       " IN -> 'in',\n",
       " NP -> NN,\n",
       " NN -> 'front',\n",
       " PP -> IN NP,\n",
       " IN -> 'on',\n",
       " NP -> NP PP,\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'square',\n",
       " PP -> IN NP,\n",
       " IN -> 'in',\n",
       " NP -> NP PP,\n",
       " NP -> NN,\n",
       " NN -> 'front',\n",
       " PP -> IN NP,\n",
       " IN -> 'of',\n",
       " NP -> NP PP,\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'church',\n",
       " PP -> IN NP,\n",
       " IN -> 'on',\n",
       " NP -> DT NNP NNP,\n",
       " DT -> 'the',\n",
       " NNP -> 'Royal',\n",
       " NNP -> 'Mile',\n",
       " . -> '.',\n",
       " NP -> NP PP PP .,\n",
       " NP -> DT JJ NN,\n",
       " DT -> 'The',\n",
       " JJ -> 'square',\n",
       " NN -> 'building',\n",
       " PP -> IN NP,\n",
       " IN -> 'on',\n",
       " NP -> DT JJ NN NN,\n",
       " DT -> 'the',\n",
       " JJ -> 'right',\n",
       " NN -> 'hand',\n",
       " NN -> 'side',\n",
       " PP -> IN NP,\n",
       " IN -> 'with',\n",
       " NP -> NP ADVP,\n",
       " NP -> DT CD JJ NNS,\n",
       " DT -> 'the',\n",
       " CD -> 'three',\n",
       " JJ -> 'tall',\n",
       " NNS -> 'pillars',\n",
       " ADVP -> JJ PP,\n",
       " JJ -> 'next',\n",
       " PP -> IN NP,\n",
       " IN -> 'to',\n",
       " NP -> DT NNP NNP,\n",
       " DT -> 'the',\n",
       " NNP -> 'Pizza',\n",
       " NNP -> 'Hut',\n",
       " . -> '.',\n",
       " NP -> NP CC NP .,\n",
       " NP -> DT NML NN,\n",
       " DT -> 'The',\n",
       " NML -> NN NN,\n",
       " NN -> 'elephant',\n",
       " NN -> 'house',\n",
       " NN -> 'pub',\n",
       " CC -> 'and',\n",
       " NP -> NP PP,\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'flat',\n",
       " PP -> IN NP,\n",
       " IN -> 'above',\n",
       " NP -> PRP,\n",
       " PRP -> 'it',\n",
       " . -> '.',\n",
       " S -> NP VP .,\n",
       " NP -> PRP,\n",
       " PRP -> 'It',\n",
       " VP -> VP CC VP,\n",
       " VP -> VBZ NP,\n",
       " VBZ -> 'has',\n",
       " NP -> DT JJ NN,\n",
       " DT -> 'a',\n",
       " JJ -> 'red',\n",
       " NN -> 'facade',\n",
       " CC -> 'and',\n",
       " VP -> VBZ ADVP,\n",
       " VBZ -> 'is',\n",
       " ADVP -> JJ PP,\n",
       " JJ -> 'next',\n",
       " PP -> IN NP,\n",
       " IN -> 'to',\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'patisserie',\n",
       " . -> '.',\n",
       " NP -> NP , PP .,\n",
       " NP -> NP VP,\n",
       " NP -> DT JJ NN,\n",
       " DT -> 'A',\n",
       " JJ -> 'small',\n",
       " NN -> 'restaurant',\n",
       " VP -> VBN S,\n",
       " VBN -> 'called',\n",
       " S -> NP,\n",
       " NP -> NNP,\n",
       " NNP -> 'Suruchi',\n",
       " , -> ',',\n",
       " PP -> IN NP,\n",
       " IN -> 'with',\n",
       " NP -> DT ADJP NN,\n",
       " DT -> 'a',\n",
       " ADJP -> JJ CC JJ,\n",
       " JJ -> 'red',\n",
       " CC -> 'and',\n",
       " JJ -> 'white',\n",
       " NN -> 'front',\n",
       " . -> '.',\n",
       " S -> NP VP .,\n",
       " NP -> PRP,\n",
       " PRP -> 'It',\n",
       " VP -> VBZ PP,\n",
       " VBZ -> \"'s\",\n",
       " PP -> IN NP,\n",
       " IN -> 'between',\n",
       " NP -> NP CC NP,\n",
       " NP -> DT NNP NN NN,\n",
       " DT -> 'the',\n",
       " NNP -> 'Subway',\n",
       " NN -> 'sandwich',\n",
       " NN -> 'shop',\n",
       " CC -> 'and',\n",
       " NP -> NP VP,\n",
       " NP -> DT JJ NN,\n",
       " DT -> 'another',\n",
       " JJ -> 'small',\n",
       " NN -> 'restaurant',\n",
       " VP -> VBN S,\n",
       " VBN -> 'called',\n",
       " S -> NP,\n",
       " NP -> NNP NNP,\n",
       " NNP -> 'Thai',\n",
       " NNP -> 'Basil',\n",
       " . -> '.',\n",
       " S -> NP VP .,\n",
       " NP -> PRP,\n",
       " PRP -> 'It',\n",
       " VP -> VBZ NP,\n",
       " VBZ -> \"'s\",\n",
       " NP -> DT JJ NN,\n",
       " DT -> 'a',\n",
       " JJ -> 'Georgian',\n",
       " NN -> 'building',\n",
       " . -> '.',\n",
       " S -> NP VP .,\n",
       " NP -> PRP,\n",
       " PRP -> 'It',\n",
       " VP -> VBZ NP,\n",
       " VBZ -> \"'s\",\n",
       " NP -> NP PP , PP,\n",
       " NP -> DT JJ NN,\n",
       " DT -> 'the',\n",
       " JJ -> 'second',\n",
       " NN -> 'door',\n",
       " PP -> IN NP,\n",
       " IN -> 'from',\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'corner',\n",
       " , -> ',',\n",
       " PP -> IN NP,\n",
       " IN -> 'with',\n",
       " NP -> DT JJ NN,\n",
       " DT -> 'a',\n",
       " JJ -> 'white',\n",
       " NN -> 'door',\n",
       " . -> '.',\n",
       " NP -> NP PP .,\n",
       " NP -> DT NML JJ NN,\n",
       " DT -> 'A',\n",
       " NML -> JJ NN,\n",
       " JJ -> 'main',\n",
       " NN -> 'door',\n",
       " JJ -> 'Georgian',\n",
       " NN -> 'flat',\n",
       " PP -> IN NP,\n",
       " IN -> 'on',\n",
       " NP -> NNP NNP,\n",
       " NNP -> 'Moray',\n",
       " NNP -> 'Place',\n",
       " . -> '.',\n",
       " S -> NP VP .,\n",
       " NP -> PRP,\n",
       " PRP -> 'It',\n",
       " VP -> VBZ NP PP,\n",
       " VBZ -> 'has',\n",
       " NP -> DT JJ NN,\n",
       " DT -> 'a',\n",
       " JJ -> 'red',\n",
       " NN -> 'postbox',\n",
       " PP -> ADVP IN NP,\n",
       " ADVP -> RB,\n",
       " RB -> 'right',\n",
       " IN -> 'in',\n",
       " NP -> NP PP,\n",
       " NP -> NN,\n",
       " NN -> 'front',\n",
       " PP -> IN NP,\n",
       " IN -> 'of',\n",
       " NP -> PRP,\n",
       " PRP -> 'it',\n",
       " . -> '.',\n",
       " S -> NP VP .,\n",
       " NP -> PRP,\n",
       " PRP -> 'It',\n",
       " VP -> VBZ NP,\n",
       " VBZ -> \"'s\",\n",
       " NP -> NP PP,\n",
       " NP -> DT ADJP NN,\n",
       " DT -> 'the',\n",
       " ADJP -> JJ JJ,\n",
       " JJ -> 'bright',\n",
       " JJ -> 'blue',\n",
       " NN -> 'door',\n",
       " PP -> IN NP,\n",
       " IN -> 'in',\n",
       " NP -> NP PP,\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'middle',\n",
       " PP -> IN NP,\n",
       " IN -> 'of',\n",
       " NP -> NP PP,\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'building',\n",
       " PP -> IN NP,\n",
       " IN -> 'in',\n",
       " NP -> NNP NNP,\n",
       " NNP -> 'Ramsay',\n",
       " NNP -> 'Garden',\n",
       " . -> '.',\n",
       " S -> NP VP .,\n",
       " NP -> PRP,\n",
       " PRP -> 'It',\n",
       " VP -> VBZ VP,\n",
       " VBZ -> \"'s\",\n",
       " VP -> VBD NP,\n",
       " VBD -> 'got',\n",
       " NP -> NP VP,\n",
       " NP -> JJ NNS,\n",
       " JJ -> 'red',\n",
       " NNS -> 'steps',\n",
       " VP -> VBG ADVP,\n",
       " VBG -> 'leading',\n",
       " ADVP -> ADVP PP,\n",
       " ADVP -> RB,\n",
       " RB -> 'up',\n",
       " PP -> IN NP,\n",
       " IN -> 'to',\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'door',\n",
       " . -> '.',\n",
       " FRAG -> PP PP .,\n",
       " PP -> IN NP,\n",
       " IN -> 'To',\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'right',\n",
       " PP -> IN NP,\n",
       " IN -> 'of',\n",
       " NP -> NP VP,\n",
       " NP -> DT JJ NN,\n",
       " DT -> 'the',\n",
       " JJ -> 'old',\n",
       " NN -> 'church',\n",
       " VP -> VBG NP,\n",
       " VBG -> 'facing',\n",
       " NP -> DT JJ NNS,\n",
       " DT -> 'the',\n",
       " JJ -> 'large',\n",
       " NNS -> 'windows',\n",
       " . -> '.',\n",
       " NP -> NP PP,\n",
       " NP -> NP PP,\n",
       " NP -> CD,\n",
       " CD -> 'One',\n",
       " PP -> IN NP,\n",
       " IN -> 'to',\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'left',\n",
       " PP -> IN NP,\n",
       " IN -> 'of',\n",
       " NP -> NP NP,\n",
       " NP -> DT NNS,\n",
       " DT -> 'the',\n",
       " NNS -> 'buildings',\n",
       " NP -> JJ NN,\n",
       " JJ -> 'gable',\n",
       " NN -> 'wall',\n",
       " S -> VP .,\n",
       " VP -> VB NP,\n",
       " VB -> 'Locate',\n",
       " NP -> NP , PP,\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'spire',\n",
       " , -> ',',\n",
       " PP -> ADVP IN NP,\n",
       " ADVP -> RB,\n",
       " RB -> 'right',\n",
       " IN -> 'of',\n",
       " NP -> DT JJ NN,\n",
       " DT -> 'the',\n",
       " JJ -> 'towered',\n",
       " NN -> 'building',\n",
       " . -> '.',\n",
       " S -> PP , NP VP,\n",
       " PP -> ADVP IN NP,\n",
       " ADVP -> RB,\n",
       " RB -> 'Halfway',\n",
       " IN -> 'between',\n",
       " NP -> NP CC NP,\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'spire',\n",
       " CC -> 'and',\n",
       " NP -> DT ADJP NN,\n",
       " DT -> 'the',\n",
       " ADJP -> JJ JJ,\n",
       " JJ -> 'twin',\n",
       " JJ -> 'towered',\n",
       " NN -> 'building',\n",
       " , -> ',',\n",
       " NP -> JJ JJ NN NNS,\n",
       " JJ -> 'double',\n",
       " JJ -> 'white',\n",
       " NN -> 'roof',\n",
       " NNS -> 'windows',\n",
       " VP -> VBZ NP,\n",
       " VBZ -> 'is',\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'target',\n",
       " SINV -> S VP VP .,\n",
       " S -> VP,\n",
       " VP -> VBG NP,\n",
       " VBG -> 'Facing',\n",
       " NP -> DT NNP,\n",
       " DT -> 'the',\n",
       " NNP -> 'Building',\n",
       " VP -> VBZ NP PP,\n",
       " VBZ -> 'fronts',\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'building',\n",
       " PP -> IN NP,\n",
       " IN -> 'to',\n",
       " NP -> NP PP,\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'left',\n",
       " PP -> IN NP,\n",
       " IN -> 'of',\n",
       " NP -> NP PP,\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'cafe',\n",
       " PP -> IN NP,\n",
       " IN -> 'in',\n",
       " NP -> NN,\n",
       " NN -> 'red',\n",
       " VP -> VP NP,\n",
       " VP -> VBZ,\n",
       " VBZ -> 'is',\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'target',\n",
       " . -> '.',\n",
       " S -> PP NP VP,\n",
       " PP -> IN NP,\n",
       " IN -> 'At',\n",
       " NP -> NN NN,\n",
       " NN -> 'street',\n",
       " NN -> 'level',\n",
       " NP -> PRP,\n",
       " PRP -> 'it',\n",
       " VP -> VBZ NP,\n",
       " VBZ -> 'is',\n",
       " NP -> DT NN,\n",
       " DT -> 'a',\n",
       " NN -> 'shop',\n",
       " FRAG -> PP , NP .,\n",
       " PP -> IN NP,\n",
       " IN -> 'at',\n",
       " NP -> NP PP,\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'junction',\n",
       " PP -> IN PP,\n",
       " IN -> 'across',\n",
       " PP -> IN NP,\n",
       " IN -> 'from',\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'church',\n",
       " , -> ',',\n",
       " NP -> NP PP,\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'building',\n",
       " PP -> IN NP,\n",
       " IN -> 'with',\n",
       " NP -> NP CC NP,\n",
       " NP -> JJ NN NNS,\n",
       " JJ -> 'large',\n",
       " NN -> 'chimney',\n",
       " NNS -> 'stacks',\n",
       " CC -> 'and',\n",
       " NP -> DT VBN NN NN,\n",
       " DT -> 'a',\n",
       " VBN -> 'pointed',\n",
       " NN -> 'tower',\n",
       " NN -> 'roof',\n",
       " . -> '.',\n",
       " S -> S , VP .,\n",
       " S -> VP,\n",
       " VP -> VBG ADVP PP PP,\n",
       " VBG -> 'LOoking',\n",
       " ADVP -> RB,\n",
       " RB -> 'directly',\n",
       " PP -> IN NP,\n",
       " IN -> 'up',\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'road',\n",
       " PP -> IN NP,\n",
       " IN -> 'with',\n",
       " NP -> NP PP,\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'gallery',\n",
       " PP -> IN NP,\n",
       " IN -> 'on',\n",
       " NP -> PRP$ NN,\n",
       " PRP$ -> 'your',\n",
       " NN -> 'left',\n",
       " , -> ',',\n",
       " VP -> VB NP,\n",
       " VB -> 'locate',\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'spire',\n",
       " . -> '.',\n",
       " PP -> PP NP,\n",
       " PP -> PP VP,\n",
       " PP -> ADVP IN NP,\n",
       " ADVP -> RB,\n",
       " RB -> 'directly',\n",
       " IN -> 'in',\n",
       " NP -> NP PP,\n",
       " NP -> NN,\n",
       " NN -> 'front',\n",
       " PP -> IN NP,\n",
       " IN -> 'of',\n",
       " ...]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. extract productions with a progress bar\n",
    "all_prods_real_corpus = []\n",
    "for t in trees:\n",
    "    all_prods_real_corpus.extend(t.productions())\n",
    "\n",
    "# 4. build a grammar (dedupe productions)\n",
    "start_sym = 'S'\n",
    "prods = list({p for p in all_prods_real_corpus})\n",
    "grammar = CFG(Nonterminal(start_sym), prods)\n",
    "\n",
    "all_prods_real_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e780bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dump your list of productions to disk\n",
    "# with open('dataset/REAL_Corpus/real_corpus_prod.cfg', 'w') as fw:\n",
    "#     for prod in all_prods_real_corpus:\n",
    "#         fw.write(f\"{prod}\\n\")\n",
    "\n",
    "with open('dataset/REAL_Corpus/real_corpus_prod.cfg') as fr:\n",
    "    all_prods_real_corpus = [line.strip() for line in fr if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d2fadf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NP -> NP VP .',\n",
       " 'NP -> NN',\n",
       " \"NN -> 'Pub'\",\n",
       " 'VP -> VBN S',\n",
       " \"VBN -> 'called'\",\n",
       " 'S -> `` NP',\n",
       " '`` -> \"\\'\"',\n",
       " 'NP -> NP NNP',\n",
       " 'NP -> NNP NNP POS',\n",
       " \"NNP -> 'Deacon'\",\n",
       " \"NNP -> 'Brodie'\",\n",
       " 'POS -> \"\\'s\"',\n",
       " \"NNP -> 'Tavern'\",\n",
       " \". -> '.'\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> ADJP JJ NN',\n",
       " 'ADJP -> JJ CC JJ',\n",
       " \"JJ -> 'Black'\",\n",
       " \"CC -> 'and'\",\n",
       " \"JJ -> 'white'\",\n",
       " \"JJ -> 'traditional'\",\n",
       " \"NN -> 'building'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'with'\",\n",
       " 'NP -> NP CC NP',\n",
       " 'NP -> NP VP',\n",
       " 'NP -> NN',\n",
       " \"NN -> 'sign'\",\n",
       " 'VP -> VBG ADVP',\n",
       " \"VBG -> 'hanging'\",\n",
       " 'ADVP -> RB',\n",
       " \"RB -> 'outside'\",\n",
       " \"CC -> 'and'\",\n",
       " 'NP -> CD JJ JJ NNS',\n",
       " \"CD -> '3'\",\n",
       " \"JJ -> 'big'\",\n",
       " \"JJ -> 'arched'\",\n",
       " \"NNS -> 'windows'\",\n",
       " 'NP -> NP PP .',\n",
       " 'NP -> JJ ADJP NN',\n",
       " \"JJ -> 'Traditional'\",\n",
       " 'ADJP -> JJ JJ',\n",
       " \"JJ -> 'early'\",\n",
       " \"JJ -> 'Victorian'\",\n",
       " \"NN -> 'terrace'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'on'\",\n",
       " 'NP -> NP CC NP',\n",
       " 'NP -> CD NNS',\n",
       " \"CD -> '3'\",\n",
       " \"NNS -> 'floors'\",\n",
       " \"CC -> '+'\",\n",
       " 'NP -> JJR',\n",
       " \"JJR -> 'dormer'\",\n",
       " \". -> '.'\",\n",
       " 'FRAG -> ADVP PP .',\n",
       " 'ADVP -> JJ RB',\n",
       " \"JJ -> 'Third'\",\n",
       " \"RB -> 'in'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'from'\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'left'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'of'\",\n",
       " 'NP -> DT NN NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'end'\",\n",
       " \"NN -> 'terrace'\",\n",
       " \". -> '.'\",\n",
       " 'NP -> NP , NP , NP .',\n",
       " 'NP -> NNP NNP NNP',\n",
       " \"NNP -> 'Royal'\",\n",
       " \"NNP -> 'Blue'\",\n",
       " \"NNP -> 'Door'\",\n",
       " \", -> ','\",\n",
       " 'NP -> JJ',\n",
       " \"JJ -> 'Black'\",\n",
       " \", -> ','\",\n",
       " 'NP -> JJ NN NNS',\n",
       " \"JJ -> 'spiked'\",\n",
       " \"NN -> 'iron'\",\n",
       " \"NNS -> 'railings'\",\n",
       " \". -> '.'\",\n",
       " 'NP -> NP , VP .',\n",
       " 'NP -> JJ , JJ ADJP NN',\n",
       " \"JJ -> 'Large'\",\n",
       " \", -> ','\",\n",
       " \"JJ -> 'modern'\",\n",
       " 'ADJP -> NN VBN',\n",
       " \"NN -> 'glass'\",\n",
       " \"VBN -> 'fronted'\",\n",
       " \"NN -> 'building'\",\n",
       " \", -> ','\",\n",
       " 'VP -> VP , CC PP',\n",
       " 'VP -> VBN PRT PP',\n",
       " \"VBN -> 'butted'\",\n",
       " 'PRT -> RP',\n",
       " \"RP -> 'up'\",\n",
       " 'PP -> PP , VP',\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'against'\",\n",
       " 'NP -> JJ JJ NN',\n",
       " \"JJ -> 'traditional'\",\n",
       " \"JJ -> 'victorian'\",\n",
       " \"NN -> 'terrace'\",\n",
       " \", -> ','\",\n",
       " 'VP -> ADVP VBN ADVP',\n",
       " 'ADVP -> RB',\n",
       " \"RB -> 'slightly'\",\n",
       " \"VBN -> 'set'\",\n",
       " 'ADVP -> ADVP PP',\n",
       " 'ADVP -> RB',\n",
       " \"RB -> 'back'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'from'\",\n",
       " 'NP -> NN',\n",
       " \"NN -> 'road'\",\n",
       " \", -> ','\",\n",
       " \"CC -> 'and'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'with'\",\n",
       " 'NP -> VBG JJ NN',\n",
       " \"VBG -> 'facing'\",\n",
       " \"JJ -> 'bowed'\",\n",
       " \"NN -> 'frontage'\",\n",
       " \". -> '.'\",\n",
       " 'NP -> NP , PP , PP .',\n",
       " 'NP -> NML JJ NN NN',\n",
       " 'NML -> CD NN',\n",
       " \"CD -> 'Two'\",\n",
       " \"NN -> 'story'\",\n",
       " \"JJ -> 'traditional'\",\n",
       " \"NN -> 'stone'\",\n",
       " \"NN -> 'building'\",\n",
       " \", -> ','\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'on'\",\n",
       " 'NP -> NP PP SBAR',\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'left'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'of'\",\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'street'\",\n",
       " 'SBAR -> IN S',\n",
       " \"IN -> 'as'\",\n",
       " 'S -> VP',\n",
       " 'VP -> VBN',\n",
       " \"VBN -> 'faced'\",\n",
       " \", -> ','\",\n",
       " 'PP -> ADVP IN NP',\n",
       " 'ADVP -> RB',\n",
       " \"RB -> 'just'\",\n",
       " \"IN -> 'before'\",\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'bed'\",\n",
       " \". -> '.'\",\n",
       " 'NP -> CD JJ JJ JJ NNS .',\n",
       " \"CD -> 'Two'\",\n",
       " \"JJ -> 'large'\",\n",
       " \"JJ -> 'white'\",\n",
       " \"JJ -> 'double'\",\n",
       " \"NNS -> 'doors'\",\n",
       " \". -> '.'\",\n",
       " 'NP -> NP : NP CC PP .',\n",
       " 'NP -> NP PP',\n",
       " 'NP -> NN',\n",
       " \"NN -> 'Church'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'in'\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'middle'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'of'\",\n",
       " 'NP -> DT JJ NN SYM NN',\n",
       " \"DT -> 'a'\",\n",
       " \"JJ -> 'small'\",\n",
       " \"NN -> 'square'\",\n",
       " \"SYM -> '/'\",\n",
       " \"NN -> 'island'\",\n",
       " \": -> '-'\",\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'no'\",\n",
       " \"NN -> 'spire'\",\n",
       " \"CC -> 'but'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'with'\",\n",
       " 'NP -> NP , VP',\n",
       " 'NP -> NP PP',\n",
       " 'NP -> JJ NML NN',\n",
       " \"JJ -> 'large'\",\n",
       " 'NML -> VBN NN',\n",
       " \"VBN -> 'stained'\",\n",
       " \"NN -> 'glass'\",\n",
       " \"NN -> 'window'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'in'\",\n",
       " 'NP -> JJ NN',\n",
       " \"JJ -> 'gable'\",\n",
       " \"NN -> 'end'\",\n",
       " \", -> ','\",\n",
       " 'VP -> ADVP VBN PP',\n",
       " 'ADVP -> RB',\n",
       " \"RB -> 'slightly'\",\n",
       " \"VBN -> 'obscured'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'by'\",\n",
       " 'NP -> NNS',\n",
       " \"NNS -> 'trees'\",\n",
       " \". -> '.'\",\n",
       " 'NP -> NP PP .',\n",
       " 'NP -> NN',\n",
       " \"NN -> 'Building'\",\n",
       " 'PP -> PP , PP',\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'with'\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> NNS',\n",
       " \"NNS -> 'columns'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'behind'\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> DT CD',\n",
       " \"DT -> 'the'\",\n",
       " \"CD -> 'one'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'in'\",\n",
       " 'NP -> NN',\n",
       " \"NN -> 'front'\",\n",
       " \", -> ','\",\n",
       " 'PP -> ADVP IN NP',\n",
       " 'ADVP -> RB',\n",
       " \"RB -> 'also'\",\n",
       " \"IN -> 'in'\",\n",
       " 'NP -> JJ NN NN',\n",
       " \"JJ -> 'similar'\",\n",
       " \"NN -> 'architecture'\",\n",
       " \"NN -> 'style'\",\n",
       " \". -> '.'\",\n",
       " 'NP -> NP , ADVP , NP',\n",
       " 'NP -> NP PP',\n",
       " 'NP -> JJ NN',\n",
       " \"JJ -> 'End'\",\n",
       " \"NN -> 'centre'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'of'\",\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'street'\",\n",
       " \", -> ','\",\n",
       " 'ADVP -> RB RB',\n",
       " \"RB -> 'straight'\",\n",
       " \"RB -> 'ahead'\",\n",
       " \", -> ','\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> JJ NN',\n",
       " \"JJ -> 'big'\",\n",
       " \"NN -> 'building'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'with'\",\n",
       " 'NP -> NP CC NP',\n",
       " 'NP -> DT NNS',\n",
       " \"DT -> 'a'\",\n",
       " \"NNS -> 'columns'\",\n",
       " \"CC -> 'and'\",\n",
       " 'NP -> DT NN NN',\n",
       " \"DT -> 'a'\",\n",
       " \"NN -> 'dome'\",\n",
       " \"NN -> 'roof'\",\n",
       " 'NP -> NP , NP',\n",
       " 'NP -> NP PP',\n",
       " 'NP -> JJ NN NN',\n",
       " \"JJ -> 'Grey'\",\n",
       " \"NN -> 'building'\",\n",
       " \"NN -> 'block'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'with'\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> NNS',\n",
       " \"NNS -> 'statues'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'above'\",\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'entrance'\",\n",
       " \", -> ','\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> NNP NN',\n",
       " \"NNP -> 'Amarone'\",\n",
       " \"NN -> 'restaurant'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'on'\",\n",
       " 'NP -> DT JJ NN',\n",
       " \"DT -> 'the'\",\n",
       " \"JJ -> 'bottom'\",\n",
       " \"NN -> 'floor'\",\n",
       " 'NP -> NP PP .',\n",
       " 'NP -> JJ JJ NN',\n",
       " \"JJ -> 'Big'\",\n",
       " \"JJ -> 'historical'\",\n",
       " \"NN -> 'building'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'with'\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> JJ NNS CC NN',\n",
       " \"JJ -> 'huge'\",\n",
       " \"NNS -> 'columns'\",\n",
       " \"CC -> 'and'\",\n",
       " \"NN -> 'statue'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'at'\",\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'top'\",\n",
       " \". -> '.'\",\n",
       " 'NP -> NP PP PP PP .',\n",
       " 'NP -> JJ JJ NN',\n",
       " \"JJ -> 'Middle'\",\n",
       " \"JJ -> 'vertical'\",\n",
       " \"NN -> 'section'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'of'\",\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'house'\",\n",
       " 'PP -> PP PP',\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'above'\",\n",
       " 'NP -> DT JJ NN',\n",
       " \"DT -> 'the'\",\n",
       " \"JJ -> 'brown'\",\n",
       " \"NN -> 'door'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'of'\",\n",
       " 'NP -> DT NN NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'housing'\",\n",
       " \"NN -> 'block'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'with'\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> NNP NNP',\n",
       " \"NNP -> 'Eigin'\",\n",
       " \"NNP -> 'Cashmere'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'on'\",\n",
       " 'NP -> DT JJ NN',\n",
       " \"DT -> 'the'\",\n",
       " \"JJ -> 'ground'\",\n",
       " \"NN -> 'floor'\",\n",
       " \". -> '.'\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> JJ NN',\n",
       " \"JJ -> 'Small'\",\n",
       " \"NN -> 'shed'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'with'\",\n",
       " 'NP -> NN NN',\n",
       " \"NN -> 'lead'\",\n",
       " \"NN -> 'roof'\",\n",
       " 'NP -> NP ADVP',\n",
       " 'NP -> ADJP NN NN',\n",
       " 'ADJP -> JJ CC JJ',\n",
       " \"JJ -> 'Red'\",\n",
       " \"CC -> 'and'\",\n",
       " \"JJ -> 'gold'\",\n",
       " \"NN -> 'coffee'\",\n",
       " \"NN -> 'shop'\",\n",
       " 'ADVP -> JJ PP',\n",
       " \"JJ -> 'next'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'to'\",\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'bridge'\",\n",
       " 'NP -> NP PP .',\n",
       " 'NP -> JJ ADJP NN SYM NN',\n",
       " \"JJ -> 'Small'\",\n",
       " 'ADJP -> NN VBN',\n",
       " \"NN -> 'stone'\",\n",
       " \"VBN -> 'capped'\",\n",
       " \"NN -> 'monument'\",\n",
       " \"SYM -> '/'\",\n",
       " \"NN -> 'fountain'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'with'\",\n",
       " 'NP -> NP VP',\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'a'\",\n",
       " \"NN -> 'plaque'\",\n",
       " 'VP -> VBN PP',\n",
       " \"VBN -> 'built'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'on'\",\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'pavement'\",\n",
       " \". -> '.'\",\n",
       " 'NP -> NP SBAR',\n",
       " 'NP -> NP PP',\n",
       " 'NP -> JJ JJ NN',\n",
       " \"JJ -> 'Enormous'\",\n",
       " \"JJ -> 'Gothic'\",\n",
       " \"NN -> 'tower'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'in'\",\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'park'\",\n",
       " 'SBAR -> WHNP S',\n",
       " 'WHNP -> WDT',\n",
       " \"WDT -> 'that'\",\n",
       " 'S -> VP',\n",
       " 'VP -> VBZ VP',\n",
       " \"VBZ -> 'is'\",\n",
       " 'VP -> VBN ADVP',\n",
       " \"VBN -> 'stained'\",\n",
       " 'ADVP -> JJ',\n",
       " \"JJ -> 'black'\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> NN NN',\n",
       " \"NN -> 'Coffee'\",\n",
       " \"NN -> 'shop'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'to'\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'left'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'of'\",\n",
       " 'NP -> DT VBN NN',\n",
       " \"DT -> 'the'\",\n",
       " \"VBN -> 'converted'\",\n",
       " \"NN -> 'church'\",\n",
       " 'NP -> NP PP PP .',\n",
       " 'NP -> DT NNS',\n",
       " \"DT -> 'The'\",\n",
       " \"NNS -> 'windows'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'above'\",\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'restaurant'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'with'\",\n",
       " 'NP -> NP ADVP',\n",
       " 'NP -> DT JJ NN NN',\n",
       " \"DT -> 'the'\",\n",
       " \"JJ -> 'black'\",\n",
       " \"NN -> 'street'\",\n",
       " \"NN -> 'furniture'\",\n",
       " 'ADVP -> JJ PP',\n",
       " \"JJ -> 'next'\",\n",
       " 'PP -> PP PP',\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'to'\",\n",
       " 'NP -> DT NNP',\n",
       " \"DT -> 'the'\",\n",
       " \"NNP -> 'RBS'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'on'\",\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'corner'\",\n",
       " \". -> '.'\",\n",
       " 'NP -> NP PP .',\n",
       " 'NP -> DT ADJP JJ NN',\n",
       " \"DT -> 'The'\",\n",
       " 'ADJP -> JJ VBG',\n",
       " \"JJ -> 'Austrian'\",\n",
       " \"VBG -> 'looking'\",\n",
       " \"JJ -> 'white'\",\n",
       " \"NN -> 'house'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'with'\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> DT JJ JJ NNS',\n",
       " \"DT -> 'the'\",\n",
       " \"JJ -> 'dark'\",\n",
       " \"JJ -> 'wooden'\",\n",
       " \"NNS -> 'beams'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'at'\",\n",
       " 'NP -> DT NN NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'water'\",\n",
       " \"NN -> 'side'\",\n",
       " \". -> '.'\",\n",
       " 'S -> NP JJ VP .',\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'The'\",\n",
       " \"NN -> 'castle'\",\n",
       " \"JJ -> 'like'\",\n",
       " 'VP -> VBP PP',\n",
       " \"VBP -> 'tower'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'on'\",\n",
       " 'NP -> NP ADVP',\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'corner'\",\n",
       " 'ADVP -> JJ PP',\n",
       " \"JJ -> 'next'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'to'\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> DT NNP NNP NNP',\n",
       " \"DT -> 'the'\",\n",
       " \"NNP -> 'Tartan'\",\n",
       " \"NNP -> 'Weaving'\",\n",
       " \"NNP -> 'Mill'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'on'\",\n",
       " 'NP -> DT NNP NNP',\n",
       " \"DT -> 'the'\",\n",
       " \"NNP -> 'Royal'\",\n",
       " \"NNP -> 'Mile'\",\n",
       " \". -> '.'\",\n",
       " 'NP -> NP VP .',\n",
       " 'NP -> DT JJ NN',\n",
       " \"DT -> 'The'\",\n",
       " \"JJ -> 'greenish'\",\n",
       " \"NN -> 'statue'\",\n",
       " 'VP -> VBG PP PP',\n",
       " \"VBG -> 'standing'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'in'\",\n",
       " 'NP -> NN',\n",
       " \"NN -> 'front'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'on'\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'square'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'in'\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> NN',\n",
       " \"NN -> 'front'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'of'\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'church'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'on'\",\n",
       " 'NP -> DT NNP NNP',\n",
       " \"DT -> 'the'\",\n",
       " \"NNP -> 'Royal'\",\n",
       " \"NNP -> 'Mile'\",\n",
       " \". -> '.'\",\n",
       " 'NP -> NP PP PP .',\n",
       " 'NP -> DT JJ NN',\n",
       " \"DT -> 'The'\",\n",
       " \"JJ -> 'square'\",\n",
       " \"NN -> 'building'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'on'\",\n",
       " 'NP -> DT JJ NN NN',\n",
       " \"DT -> 'the'\",\n",
       " \"JJ -> 'right'\",\n",
       " \"NN -> 'hand'\",\n",
       " \"NN -> 'side'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'with'\",\n",
       " 'NP -> NP ADVP',\n",
       " 'NP -> DT CD JJ NNS',\n",
       " \"DT -> 'the'\",\n",
       " \"CD -> 'three'\",\n",
       " \"JJ -> 'tall'\",\n",
       " \"NNS -> 'pillars'\",\n",
       " 'ADVP -> JJ PP',\n",
       " \"JJ -> 'next'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'to'\",\n",
       " 'NP -> DT NNP NNP',\n",
       " \"DT -> 'the'\",\n",
       " \"NNP -> 'Pizza'\",\n",
       " \"NNP -> 'Hut'\",\n",
       " \". -> '.'\",\n",
       " 'NP -> NP CC NP .',\n",
       " 'NP -> DT NML NN',\n",
       " \"DT -> 'The'\",\n",
       " 'NML -> NN NN',\n",
       " \"NN -> 'elephant'\",\n",
       " \"NN -> 'house'\",\n",
       " \"NN -> 'pub'\",\n",
       " \"CC -> 'and'\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'flat'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'above'\",\n",
       " 'NP -> PRP',\n",
       " \"PRP -> 'it'\",\n",
       " \". -> '.'\",\n",
       " 'S -> NP VP .',\n",
       " 'NP -> PRP',\n",
       " \"PRP -> 'It'\",\n",
       " 'VP -> VP CC VP',\n",
       " 'VP -> VBZ NP',\n",
       " \"VBZ -> 'has'\",\n",
       " 'NP -> DT JJ NN',\n",
       " \"DT -> 'a'\",\n",
       " \"JJ -> 'red'\",\n",
       " \"NN -> 'facade'\",\n",
       " \"CC -> 'and'\",\n",
       " 'VP -> VBZ ADVP',\n",
       " \"VBZ -> 'is'\",\n",
       " 'ADVP -> JJ PP',\n",
       " \"JJ -> 'next'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'to'\",\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'patisserie'\",\n",
       " \". -> '.'\",\n",
       " 'NP -> NP , PP .',\n",
       " 'NP -> NP VP',\n",
       " 'NP -> DT JJ NN',\n",
       " \"DT -> 'A'\",\n",
       " \"JJ -> 'small'\",\n",
       " \"NN -> 'restaurant'\",\n",
       " 'VP -> VBN S',\n",
       " \"VBN -> 'called'\",\n",
       " 'S -> NP',\n",
       " 'NP -> NNP',\n",
       " \"NNP -> 'Suruchi'\",\n",
       " \", -> ','\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'with'\",\n",
       " 'NP -> DT ADJP NN',\n",
       " \"DT -> 'a'\",\n",
       " 'ADJP -> JJ CC JJ',\n",
       " \"JJ -> 'red'\",\n",
       " \"CC -> 'and'\",\n",
       " \"JJ -> 'white'\",\n",
       " \"NN -> 'front'\",\n",
       " \". -> '.'\",\n",
       " 'S -> NP VP .',\n",
       " 'NP -> PRP',\n",
       " \"PRP -> 'It'\",\n",
       " 'VP -> VBZ PP',\n",
       " 'VBZ -> \"\\'s\"',\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'between'\",\n",
       " 'NP -> NP CC NP',\n",
       " 'NP -> DT NNP NN NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NNP -> 'Subway'\",\n",
       " \"NN -> 'sandwich'\",\n",
       " \"NN -> 'shop'\",\n",
       " \"CC -> 'and'\",\n",
       " 'NP -> NP VP',\n",
       " 'NP -> DT JJ NN',\n",
       " \"DT -> 'another'\",\n",
       " \"JJ -> 'small'\",\n",
       " \"NN -> 'restaurant'\",\n",
       " 'VP -> VBN S',\n",
       " \"VBN -> 'called'\",\n",
       " 'S -> NP',\n",
       " 'NP -> NNP NNP',\n",
       " \"NNP -> 'Thai'\",\n",
       " \"NNP -> 'Basil'\",\n",
       " \". -> '.'\",\n",
       " 'S -> NP VP .',\n",
       " 'NP -> PRP',\n",
       " \"PRP -> 'It'\",\n",
       " 'VP -> VBZ NP',\n",
       " 'VBZ -> \"\\'s\"',\n",
       " 'NP -> DT JJ NN',\n",
       " \"DT -> 'a'\",\n",
       " \"JJ -> 'Georgian'\",\n",
       " \"NN -> 'building'\",\n",
       " \". -> '.'\",\n",
       " 'S -> NP VP .',\n",
       " 'NP -> PRP',\n",
       " \"PRP -> 'It'\",\n",
       " 'VP -> VBZ NP',\n",
       " 'VBZ -> \"\\'s\"',\n",
       " 'NP -> NP PP , PP',\n",
       " 'NP -> DT JJ NN',\n",
       " \"DT -> 'the'\",\n",
       " \"JJ -> 'second'\",\n",
       " \"NN -> 'door'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'from'\",\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'corner'\",\n",
       " \", -> ','\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'with'\",\n",
       " 'NP -> DT JJ NN',\n",
       " \"DT -> 'a'\",\n",
       " \"JJ -> 'white'\",\n",
       " \"NN -> 'door'\",\n",
       " \". -> '.'\",\n",
       " 'NP -> NP PP .',\n",
       " 'NP -> DT NML JJ NN',\n",
       " \"DT -> 'A'\",\n",
       " 'NML -> JJ NN',\n",
       " \"JJ -> 'main'\",\n",
       " \"NN -> 'door'\",\n",
       " \"JJ -> 'Georgian'\",\n",
       " \"NN -> 'flat'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'on'\",\n",
       " 'NP -> NNP NNP',\n",
       " \"NNP -> 'Moray'\",\n",
       " \"NNP -> 'Place'\",\n",
       " \". -> '.'\",\n",
       " 'S -> NP VP .',\n",
       " 'NP -> PRP',\n",
       " \"PRP -> 'It'\",\n",
       " 'VP -> VBZ NP PP',\n",
       " \"VBZ -> 'has'\",\n",
       " 'NP -> DT JJ NN',\n",
       " \"DT -> 'a'\",\n",
       " \"JJ -> 'red'\",\n",
       " \"NN -> 'postbox'\",\n",
       " 'PP -> ADVP IN NP',\n",
       " 'ADVP -> RB',\n",
       " \"RB -> 'right'\",\n",
       " \"IN -> 'in'\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> NN',\n",
       " \"NN -> 'front'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'of'\",\n",
       " 'NP -> PRP',\n",
       " \"PRP -> 'it'\",\n",
       " \". -> '.'\",\n",
       " 'S -> NP VP .',\n",
       " 'NP -> PRP',\n",
       " \"PRP -> 'It'\",\n",
       " 'VP -> VBZ NP',\n",
       " 'VBZ -> \"\\'s\"',\n",
       " 'NP -> NP PP',\n",
       " 'NP -> DT ADJP NN',\n",
       " \"DT -> 'the'\",\n",
       " 'ADJP -> JJ JJ',\n",
       " \"JJ -> 'bright'\",\n",
       " \"JJ -> 'blue'\",\n",
       " \"NN -> 'door'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'in'\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'middle'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'of'\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'building'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'in'\",\n",
       " 'NP -> NNP NNP',\n",
       " \"NNP -> 'Ramsay'\",\n",
       " \"NNP -> 'Garden'\",\n",
       " \". -> '.'\",\n",
       " 'S -> NP VP .',\n",
       " 'NP -> PRP',\n",
       " \"PRP -> 'It'\",\n",
       " 'VP -> VBZ VP',\n",
       " 'VBZ -> \"\\'s\"',\n",
       " 'VP -> VBD NP',\n",
       " \"VBD -> 'got'\",\n",
       " 'NP -> NP VP',\n",
       " 'NP -> JJ NNS',\n",
       " \"JJ -> 'red'\",\n",
       " \"NNS -> 'steps'\",\n",
       " 'VP -> VBG ADVP',\n",
       " \"VBG -> 'leading'\",\n",
       " 'ADVP -> ADVP PP',\n",
       " 'ADVP -> RB',\n",
       " \"RB -> 'up'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'to'\",\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'door'\",\n",
       " \". -> '.'\",\n",
       " 'FRAG -> PP PP .',\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'To'\",\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'right'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'of'\",\n",
       " 'NP -> NP VP',\n",
       " 'NP -> DT JJ NN',\n",
       " \"DT -> 'the'\",\n",
       " \"JJ -> 'old'\",\n",
       " \"NN -> 'church'\",\n",
       " 'VP -> VBG NP',\n",
       " \"VBG -> 'facing'\",\n",
       " 'NP -> DT JJ NNS',\n",
       " \"DT -> 'the'\",\n",
       " \"JJ -> 'large'\",\n",
       " \"NNS -> 'windows'\",\n",
       " \". -> '.'\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> NP PP',\n",
       " 'NP -> CD',\n",
       " \"CD -> 'One'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'to'\",\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'left'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'of'\",\n",
       " 'NP -> NP NP',\n",
       " 'NP -> DT NNS',\n",
       " \"DT -> 'the'\",\n",
       " \"NNS -> 'buildings'\",\n",
       " 'NP -> JJ NN',\n",
       " \"JJ -> 'gable'\",\n",
       " \"NN -> 'wall'\",\n",
       " 'S -> VP .',\n",
       " 'VP -> VB NP',\n",
       " \"VB -> 'Locate'\",\n",
       " 'NP -> NP , PP',\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'spire'\",\n",
       " \", -> ','\",\n",
       " 'PP -> ADVP IN NP',\n",
       " 'ADVP -> RB',\n",
       " \"RB -> 'right'\",\n",
       " \"IN -> 'of'\",\n",
       " 'NP -> DT JJ NN',\n",
       " \"DT -> 'the'\",\n",
       " \"JJ -> 'towered'\",\n",
       " \"NN -> 'building'\",\n",
       " \". -> '.'\",\n",
       " 'S -> PP , NP VP',\n",
       " 'PP -> ADVP IN NP',\n",
       " 'ADVP -> RB',\n",
       " \"RB -> 'Halfway'\",\n",
       " \"IN -> 'between'\",\n",
       " 'NP -> NP CC NP',\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'spire'\",\n",
       " \"CC -> 'and'\",\n",
       " 'NP -> DT ADJP NN',\n",
       " \"DT -> 'the'\",\n",
       " 'ADJP -> JJ JJ',\n",
       " \"JJ -> 'twin'\",\n",
       " \"JJ -> 'towered'\",\n",
       " \"NN -> 'building'\",\n",
       " \", -> ','\",\n",
       " 'NP -> JJ JJ NN NNS',\n",
       " \"JJ -> 'double'\",\n",
       " \"JJ -> 'white'\",\n",
       " \"NN -> 'roof'\",\n",
       " \"NNS -> 'windows'\",\n",
       " 'VP -> VBZ NP',\n",
       " \"VBZ -> 'is'\",\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'target'\",\n",
       " 'SINV -> S VP VP .',\n",
       " 'S -> VP',\n",
       " 'VP -> VBG NP',\n",
       " \"VBG -> 'Facing'\",\n",
       " 'NP -> DT NNP',\n",
       " \"DT -> 'the'\",\n",
       " \"NNP -> 'Building'\",\n",
       " 'VP -> VBZ NP PP',\n",
       " \"VBZ -> 'fronts'\",\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'building'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'to'\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'left'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'of'\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'cafe'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'in'\",\n",
       " 'NP -> NN',\n",
       " \"NN -> 'red'\",\n",
       " 'VP -> VP NP',\n",
       " 'VP -> VBZ',\n",
       " \"VBZ -> 'is'\",\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'target'\",\n",
       " \". -> '.'\",\n",
       " 'S -> PP NP VP',\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'At'\",\n",
       " 'NP -> NN NN',\n",
       " \"NN -> 'street'\",\n",
       " \"NN -> 'level'\",\n",
       " 'NP -> PRP',\n",
       " \"PRP -> 'it'\",\n",
       " 'VP -> VBZ NP',\n",
       " \"VBZ -> 'is'\",\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'a'\",\n",
       " \"NN -> 'shop'\",\n",
       " 'FRAG -> PP , NP .',\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'at'\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'junction'\",\n",
       " 'PP -> IN PP',\n",
       " \"IN -> 'across'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'from'\",\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'church'\",\n",
       " \", -> ','\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'building'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'with'\",\n",
       " 'NP -> NP CC NP',\n",
       " 'NP -> JJ NN NNS',\n",
       " \"JJ -> 'large'\",\n",
       " \"NN -> 'chimney'\",\n",
       " \"NNS -> 'stacks'\",\n",
       " \"CC -> 'and'\",\n",
       " 'NP -> DT VBN NN NN',\n",
       " \"DT -> 'a'\",\n",
       " \"VBN -> 'pointed'\",\n",
       " \"NN -> 'tower'\",\n",
       " \"NN -> 'roof'\",\n",
       " \". -> '.'\",\n",
       " 'S -> S , VP .',\n",
       " 'S -> VP',\n",
       " 'VP -> VBG ADVP PP PP',\n",
       " \"VBG -> 'LOoking'\",\n",
       " 'ADVP -> RB',\n",
       " \"RB -> 'directly'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'up'\",\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'road'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'with'\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'gallery'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'on'\",\n",
       " 'NP -> PRP$ NN',\n",
       " \"PRP$ -> 'your'\",\n",
       " \"NN -> 'left'\",\n",
       " \", -> ','\",\n",
       " 'VP -> VB NP',\n",
       " \"VB -> 'locate'\",\n",
       " 'NP -> DT NN',\n",
       " \"DT -> 'the'\",\n",
       " \"NN -> 'spire'\",\n",
       " \". -> '.'\",\n",
       " 'PP -> PP NP',\n",
       " 'PP -> PP VP',\n",
       " 'PP -> ADVP IN NP',\n",
       " 'ADVP -> RB',\n",
       " \"RB -> 'directly'\",\n",
       " \"IN -> 'in'\",\n",
       " 'NP -> NP PP',\n",
       " 'NP -> NN',\n",
       " \"NN -> 'front'\",\n",
       " 'PP -> IN NP',\n",
       " \"IN -> 'of'\",\n",
       " ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_prods_real_corpus = [line.strip() for line in all_prods_real_corpus if line.strip()]\n",
    "all_prods_real_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee887e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexical rules  = 1963\n",
      "nonlexical rules = 976\n"
     ]
    }
   ],
   "source": [
    "from nltk import Nonterminal\n",
    "\n",
    "lexical   = [p for p in prods if all(not isinstance(sym, Nonterminal) for sym in p.rhs())]\n",
    "nonlexical= [p for p in prods if any(isinstance(sym, Nonterminal)     for sym in p.rhs())]\n",
    "\n",
    "print(f\"lexical rules  = {len(lexical)}\")\n",
    "print(f\"nonlexical rules = {len(nonlexical)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dd27ec",
   "metadata": {},
   "source": [
    "cfg shrinking rule by only keeping nonlexical rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5bcc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en3 to\n",
      "[nltk_data]     /Users/glora/projects/populate-ns-lex/.venv-\n",
      "[nltk_data]     benepar/share/nltk_data...\n",
      "[nltk_data]   Unzipping models/benepar_en3.zip.\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "LanguageError",
     "evalue": "The detected language of the given sentence is not supported. Currently, ConstituentTree only supports: English, German, French, Polish, Hungarian, Swedish, Chinese and Korean.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLanguageError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Example\u001b[39;00m\n\u001b[1;32m     50\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPub called \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeacon Brodie\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms Tavern.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 51\u001b[0m parsed_tree \u001b[38;5;241m=\u001b[39m \u001b[43mmake_pos_cfg_and_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(parsed_tree\u001b[38;5;241m.\u001b[39mpformat(margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m))\n",
      "Cell \u001b[0;32mIn[46], line 21\u001b[0m, in \u001b[0;36mmake_pos_cfg_and_parse\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     18\u001b[0m pos_seq \u001b[38;5;241m=\u001b[39m [tok\u001b[38;5;241m.\u001b[39mtag_ \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m doc]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 3) Build CTL tree *without* token leaves (i.e. leaves are POS tags)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m ctl \u001b[38;5;241m=\u001b[39m \u001b[43mConstituentTree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStructure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWithoutTokenLeaves\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 4) Extract its productions and make a CFG\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#    (CTLs `tree` is an NLTK Tree under the hood)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m tree \u001b[38;5;241m=\u001b[39m ctl\u001b[38;5;241m.\u001b[39mtree  \n",
      "File \u001b[0;32m~/projects/populate-ns-lex/.venv-benepar/lib/python3.9/site-packages/constituent_treelib/core.py:289\u001b[0m, in \u001b[0;36mConstituentTree.__init__\u001b[0;34m(self, sentence, nlp, structure, expand_contractions, create_pipeline)\u001b[0m\n\u001b[1;32m    287\u001b[0m supported_languages \u001b[38;5;241m=\u001b[39m [e\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m Language \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m detected_language \u001b[38;5;241m==\u001b[39m Language\u001b[38;5;241m.\u001b[39mUnsupported:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LanguageError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe detected language of the given sentence is not supported. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently, ConstituentTree only supports: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(supported_languages[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_languages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentence_language \u001b[38;5;241m=\u001b[39m detected_language\n",
      "\u001b[0;31mLanguageError\u001b[0m: The detected language of the given sentence is not supported. Currently, ConstituentTree only supports: English, German, French, Polish, Hungarian, Swedish, Chinese and Korean."
     ]
    }
   ],
   "source": [
    "\n",
    "from constituent_treelib import ConstituentTree, Structure\n",
    "\n",
    "\n",
    "\n",
    "from constituent_treelib import ConstituentTree, BracketedTree, Language, Structure\n",
    "\n",
    "language = Language.English\n",
    "nlp = ConstituentTree.create_pipeline(language, ConstituentTree.SpacyModelSize.Small)\n",
    "\n",
    "\n",
    "def make_pos_cfg_and_parse(text):\n",
    "\n",
    "    doc     = nlp(text)\n",
    "    tokens  = [tok.text for tok in doc]\n",
    "    pos_seq = [tok.tag_ for tok in doc]\n",
    "\n",
    "    # build CTL tree without token leaves -> leaf POS tag\n",
    "    ctl = ConstituentTree(doc, nlp, Structure.WithoutTokenLeaves)\n",
    "\n",
    "    \n",
    "    tree = ctl.tree  # NLTK tree obj\n",
    "    prods = tree.productions()\n",
    "    G     = CFG(Nonterminal(\"S\"), prods)\n",
    "\n",
    "\n",
    "    parser = EarleyChartParser(G)\n",
    "    try:\n",
    "        G.check_coverage(pos_seq)\n",
    "    except ValueError as e:\n",
    "        raise RuntimeError(f\"Missing POS in lexicon: {e}\")    \n",
    "    parse = next(parser.parse(pos_seq), None)\n",
    "    if parse is None:\n",
    "        raise RuntimeError(\"No parse found!\")\n",
    "\n",
    "    i = 0\n",
    "    for subtree in parse.subtrees():\n",
    "        # pre-terminals have height()==2, i.e. (TAG leaf)\n",
    "        if subtree.height() == 2:\n",
    "            subtree[0] = tokens[i]\n",
    "            i += 1\n",
    "\n",
    "    return parse\n",
    "\n",
    "sentence = \"Pub called 'Deacon Brodie's Tavern.\"\n",
    "parsed_tree = make_pos_cfg_and_parse(sentence)\n",
    "print(parsed_tree.pformat(margin=60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3341602f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en3 to\n",
      "[nltk_data]     /Users/glora/projects/populate-ns-lex/.venv-\n",
      "[nltk_data]     benepar/share/nltk_data...\n",
      "[nltk_data]   Package benepar_en3 is already up-to-date!\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TESTING SENTENCE: Pub called Deacon Brodie's Tavern.\n",
      "============================================================\n",
      "\n",
      "--- Trying Aggressive Cleaning ---\n",
      "Original text: Pub called Deacon Brodie's Tavern.\n",
      "Cleaned text: Pub called Deacon Brodie's Tavern.\n",
      "Tokens: ['Pub', 'called', 'Deacon', 'Brodie', \"'s\", 'Tavern', '.']\n",
      "POS tags: ['NN', 'VBN', 'NNP', 'NNP', 'POS', 'NNP', '.']\n",
      "Failed: The detected language of the given sentence is not supported. Currently, ConstituentTree only supports: English, German, French, Polish, Hungarian, Swedish, Chinese and Korean.\n",
      "\n",
      "--- Trying English Enhancement ---\n",
      "Original text: Pub called Deacon Brodie's Tavern.\n",
      "English-enhanced text: Pub called Deacon Brodie's Tavern.\n",
      "Tokens: ['Pub', 'called', 'Deacon', 'Brodie', \"'s\", 'Tavern', '.']\n",
      "POS tags: ['NN', 'VBN', 'NNP', 'NNP', 'POS', 'NNP', '.']\n",
      "Failed: The detected language of the given sentence is not supported. Currently, ConstituentTree only supports: English, German, French, Polish, Hungarian, Swedish, Chinese and Korean.\n",
      "\n",
      "--- Trying Simple Grammar ---\n",
      "Target text: Pub called Deacon Brodie's Tavern.\n",
      "Using simple sentence for grammar: The building is traditional.\n",
      "Failed: The detected language of the given sentence is not supported. Currently, ConstituentTree only supports: English, German, French, Polish, Hungarian, Swedish, Chinese and Korean.\n",
      "\n",
      " All approaches failed for: Pub called Deacon Brodie's Tavern.\n",
      "\n",
      "============================================================\n",
      "TESTING SENTENCE: Traditional building with windows.\n",
      "============================================================\n",
      "\n",
      "--- Trying Aggressive Cleaning ---\n",
      "Original text: Traditional building with windows.\n",
      "Cleaned text: Traditional building with windows.\n",
      "Tokens: ['Traditional', 'building', 'with', 'windows', '.']\n",
      "POS tags: ['JJ', 'NN', 'IN', 'NNS', '.']\n",
      "Failed: The detected language of the given sentence is not supported. Currently, ConstituentTree only supports: English, German, French, Polish, Hungarian, Swedish, Chinese and Korean.\n",
      "\n",
      "--- Trying English Enhancement ---\n",
      "Original text: Traditional building with windows.\n",
      "English-enhanced text: Traditional building with windows.\n",
      "Tokens: ['Traditional', 'building', 'with', 'windows', '.']\n",
      "POS tags: ['JJ', 'NN', 'IN', 'NNS', '.']\n",
      "Failed: The detected language of the given sentence is not supported. Currently, ConstituentTree only supports: English, German, French, Polish, Hungarian, Swedish, Chinese and Korean.\n",
      "\n",
      "--- Trying Simple Grammar ---\n",
      "Target text: Traditional building with windows.\n",
      "Using simple sentence for grammar: The building is traditional.\n",
      "Failed: The detected language of the given sentence is not supported. Currently, ConstituentTree only supports: English, German, French, Polish, Hungarian, Swedish, Chinese and Korean.\n",
      "\n",
      " All approaches failed for: Traditional building with windows.\n",
      "\n",
      "============================================================\n",
      "TESTING SENTENCE: The cat sat on the mat.\n",
      "============================================================\n",
      "\n",
      "--- Trying Aggressive Cleaning ---\n",
      "Original text: The cat sat on the mat.\n",
      "Cleaned text: The cat sat on the mat.\n",
      "Tokens: ['The', 'cat', 'sat', 'on', 'the', 'mat', '.']\n",
      "POS tags: ['DT', 'NN', 'VBD', 'IN', 'DT', 'NN', '.']\n",
      "Failed: The detected language of the given sentence is not supported. Currently, ConstituentTree only supports: English, German, French, Polish, Hungarian, Swedish, Chinese and Korean.\n",
      "\n",
      "--- Trying English Enhancement ---\n",
      "Original text: The cat sat on the mat.\n",
      "English-enhanced text: The cat sat on the mat.\n",
      "Tokens: ['The', 'cat', 'sat', 'on', 'the', 'mat', '.']\n",
      "POS tags: ['DT', 'NN', 'VBD', 'IN', 'DT', 'NN', '.']\n",
      "Failed: The detected language of the given sentence is not supported. Currently, ConstituentTree only supports: English, German, French, Polish, Hungarian, Swedish, Chinese and Korean.\n",
      "\n",
      "--- Trying Simple Grammar ---\n",
      "Target text: The cat sat on the mat.\n",
      "Using simple sentence for grammar: The building is traditional.\n",
      "Failed: The detected language of the given sentence is not supported. Currently, ConstituentTree only supports: English, German, French, Polish, Hungarian, Swedish, Chinese and Korean.\n",
      "\n",
      " All approaches failed for: The cat sat on the mat.\n",
      "\n",
      "============================================================\n",
      "TESTING SENTENCE: A red door.\n",
      "============================================================\n",
      "\n",
      "--- Trying Aggressive Cleaning ---\n",
      "Original text: A red door.\n",
      "Cleaned text: A red door.\n",
      "Tokens: ['A', 'red', 'door', '.']\n",
      "POS tags: ['DT', 'JJ', 'NN', '.']\n",
      "Failed: The detected language of the given sentence is not supported. Currently, ConstituentTree only supports: English, German, French, Polish, Hungarian, Swedish, Chinese and Korean.\n",
      "\n",
      "--- Trying English Enhancement ---\n",
      "Original text: A red door.\n",
      "English-enhanced text: A red door.\n",
      "Tokens: ['A', 'red', 'door', '.']\n",
      "POS tags: ['DT', 'JJ', 'NN', '.']\n",
      "Failed: The detected language of the given sentence is not supported. Currently, ConstituentTree only supports: English, German, French, Polish, Hungarian, Swedish, Chinese and Korean.\n",
      "\n",
      "--- Trying Simple Grammar ---\n",
      "Target text: A red door.\n",
      "Using simple sentence for grammar: The building is traditional.\n",
      "Failed: The detected language of the given sentence is not supported. Currently, ConstituentTree only supports: English, German, French, Polish, Hungarian, Swedish, Chinese and Korean.\n",
      "\n",
      " All approaches failed for: A red door.\n",
      "\n",
      "============================================================\n",
      "RECOMMENDATION:\n",
      "If none of these work, consider:\n",
      "1. Updating constituent_treelib: pip install --upgrade constituent-treelib\n",
      "2. Using a different parsing library like spacy-stanza\n",
      "3. Using NLTK's built-in parsers directly\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Define the language for the sentence as well as for the spaCy and benepar models\n",
    "language = Language.English\n",
    "nlp = ConstituentTree.create_pipeline(language, ConstituentTree.SpacyModelSize.Small)\n",
    "\n",
    "def clean_text_aggressively(text):\n",
    "    \"\"\"More aggressive text cleaning to help language detection\"\"\"\n",
    "    # Replace smart quotes and special characters\n",
    "    cleaned = text.replace(''', \"'\").replace(''', \"'\").replace('\"', '\"').replace('\"', '\"')\n",
    "    \n",
    "    # Replace problematic characters that might confuse language detection\n",
    "    cleaned = cleaned.replace('+', 'and')  # Replace + with 'and'\n",
    "    cleaned = re.sub(r'\\d+', 'NUM', cleaned)  # Replace numbers with 'NUM'\n",
    "    \n",
    "    # Ensure proper sentence structure\n",
    "    if not cleaned.strip().endswith(('.', '!', '?')):\n",
    "        cleaned = cleaned.strip() + '.'\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def make_text_more_english(text):\n",
    "    \"\"\"Make text more obviously English to help language detection\"\"\"\n",
    "    # Add common English words to help detection\n",
    "    if not any(word in text.lower() for word in ['the', 'a', 'an', 'is', 'was', 'are', 'were']):\n",
    "        # Prepend with \"This is\" to make it more clearly English\n",
    "        return f\"This is {text.lower()}\"\n",
    "    return text\n",
    "\n",
    "def make_pos_cfg_and_parse_v1(text):\n",
    "    \"\"\"Version 1: Aggressive text cleaning\"\"\"\n",
    "    print(f\"Original text: {text}\")\n",
    "    \n",
    "    # Clean the text aggressively\n",
    "    cleaned_text = clean_text_aggressively(text)\n",
    "    print(f\"Cleaned text: {cleaned_text}\")\n",
    "    \n",
    "    # Tokenize & POS-tag\n",
    "    doc = nlp(cleaned_text)\n",
    "    tokens = [tok.text for tok in doc]\n",
    "    pos_seq = [tok.tag_ for tok in doc]\n",
    "    \n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"POS tags: {pos_seq}\")\n",
    "    \n",
    "    # Build CTL tree without the sentence_language parameter\n",
    "    ctl = ConstituentTree(doc, nlp, Structure.WithoutTokenLeaves)\n",
    "    \n",
    "    # Extract productions and make CFG\n",
    "    tree = ctl.tree\n",
    "    prods = tree.productions()\n",
    "    G = CFG(Nonterminal(\"S\"), prods)\n",
    "    \n",
    "    print(f\"Grammar productions: {len(prods)}\")\n",
    "    \n",
    "    # Parse the POS sequence\n",
    "    parser = EarleyChartParser(G)\n",
    "    try:\n",
    "        G.check_coverage(pos_seq)\n",
    "    except ValueError as e:\n",
    "        print(f\"Grammar coverage issue: {e}\")\n",
    "        terminals = set()\n",
    "        for prod in prods:\n",
    "            if prod.is_lexical():\n",
    "                terminals.add(prod.rhs()[0])\n",
    "        print(f\"Available POS tags in grammar: {sorted(terminals)}\")\n",
    "        print(f\"Required POS tags: {pos_seq}\")\n",
    "        raise RuntimeError(f\"Missing POS in lexicon: {e}\")\n",
    "    \n",
    "    parses = list(parser.parse(pos_seq))\n",
    "    if not parses:\n",
    "        raise RuntimeError(\"No parse found!\")\n",
    "    \n",
    "    print(f\"Found {len(parses)} parse(s)\")\n",
    "    parse = parses[0]\n",
    "    \n",
    "    # Re-attach original tokens (not cleaned ones)\n",
    "    original_doc = nlp(text.replace(''', \"'\").replace(''', \"'\").replace('\"', '\"').replace('\"', '\"'))\n",
    "    original_tokens = [tok.text for tok in original_doc]\n",
    "    \n",
    "    i = 0\n",
    "    for subtree in parse.subtrees():\n",
    "        if subtree.height() == 2 and i < len(original_tokens):\n",
    "            subtree[0] = original_tokens[i]\n",
    "            i += 1\n",
    "    \n",
    "    return parse\n",
    "\n",
    "def make_pos_cfg_and_parse_v2(text):\n",
    "    \"\"\"Version 2: Make text more obviously English\"\"\"\n",
    "    print(f\"Original text: {text}\")\n",
    "    \n",
    "    # Make text more English-like\n",
    "    english_text = make_text_more_english(text)\n",
    "    cleaned_text = clean_text_aggressively(english_text)\n",
    "    print(f\"English-enhanced text: {cleaned_text}\")\n",
    "    \n",
    "    # Tokenize & POS-tag\n",
    "    doc = nlp(cleaned_text)\n",
    "    tokens = [tok.text for tok in doc]\n",
    "    pos_seq = [tok.tag_ for tok in doc]\n",
    "    \n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"POS tags: {pos_seq}\")\n",
    "    \n",
    "    # Build CTL tree\n",
    "    ctl = ConstituentTree(doc, nlp, Structure.WithoutTokenLeaves)\n",
    "    \n",
    "    # Extract productions and make CFG\n",
    "    tree = ctl.tree\n",
    "    prods = tree.productions()\n",
    "    G = CFG(Nonterminal(\"S\"), prods)\n",
    "    \n",
    "    print(f\"Grammar productions: {len(prods)}\")\n",
    "    \n",
    "    # Parse\n",
    "    parser = EarleyChartParser(G)\n",
    "    try:\n",
    "        G.check_coverage(pos_seq)\n",
    "    except ValueError as e:\n",
    "        print(f\"Grammar coverage issue: {e}\")\n",
    "        terminals = set()\n",
    "        for prod in prods:\n",
    "            if prod.is_lexical():\n",
    "                terminals.add(prod.rhs()[0])\n",
    "        print(f\"Available POS tags in grammar: {sorted(terminals)}\")\n",
    "        print(f\"Required POS tags: {pos_seq}\")\n",
    "        raise RuntimeError(f\"Missing POS in lexicon: {e}\")\n",
    "    \n",
    "    parses = list(parser.parse(pos_seq))\n",
    "    if not parses:\n",
    "        raise RuntimeError(\"No parse found!\")\n",
    "    \n",
    "    parse = parses[0]\n",
    "    \n",
    "    # Re-attach tokens\n",
    "    i = 0\n",
    "    for subtree in parse.subtrees():\n",
    "        if subtree.height() == 2:\n",
    "            subtree[0] = tokens[i]\n",
    "            i += 1\n",
    "    \n",
    "    return parse\n",
    "\n",
    "def make_pos_cfg_and_parse_v3(text):\n",
    "    \"\"\"Version 3: Use simple, known English sentences for grammar extraction\"\"\"\n",
    "    print(f\"Target text: {text}\")\n",
    "    \n",
    "    # Use a simple English sentence to extract grammar\n",
    "    simple_sentence = \"The building is traditional.\"\n",
    "    print(f\"Using simple sentence for grammar: {simple_sentence}\")\n",
    "    \n",
    "    # Get grammar from simple sentence\n",
    "    simple_doc = nlp(simple_sentence)\n",
    "    ctl_simple = ConstituentTree(simple_doc, nlp, Structure.WithoutTokenLeaves)\n",
    "    tree_simple = ctl_simple.tree\n",
    "    prods = tree_simple.productions()\n",
    "    \n",
    "    # Now parse the target text\n",
    "    cleaned_text = clean_text_aggressively(text)\n",
    "    doc = nlp(cleaned_text)\n",
    "    tokens = [tok.text for tok in doc]\n",
    "    pos_seq = [tok.tag_ for tok in doc]\n",
    "    \n",
    "    print(f\"Target tokens: {tokens}\")\n",
    "    print(f\"Target POS tags: {pos_seq}\")\n",
    "    \n",
    "    G = CFG(Nonterminal(\"S\"), prods)\n",
    "    print(f\"Grammar productions: {len(prods)}\")\n",
    "    \n",
    "    parser = EarleyChartParser(G)\n",
    "    try:\n",
    "        G.check_coverage(pos_seq)\n",
    "    except ValueError as e:\n",
    "        print(f\"Grammar coverage issue: {e}\")\n",
    "        # Try to add missing productions\n",
    "        print(\"Attempting to extend grammar...\")\n",
    "        raise RuntimeError(f\"Missing POS in lexicon: {e}\")\n",
    "    \n",
    "    parses = list(parser.parse(pos_seq))\n",
    "    if not parses:\n",
    "        raise RuntimeError(\"No parse found!\")\n",
    "    \n",
    "    parse = parses[0]\n",
    "    \n",
    "    i = 0\n",
    "    for subtree in parse.subtrees():\n",
    "        if subtree.height() == 2:\n",
    "            subtree[0] = tokens[i]\n",
    "            i += 1\n",
    "    \n",
    "    return parse\n",
    "\n",
    "# Test different approaches\n",
    "test_sentences = [\n",
    "    \"Pub called Deacon Brodie's Tavern.\",\n",
    "    \"Traditional building with windows.\",\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"A red door.\",\n",
    "]\n",
    "\n",
    "approaches = [\n",
    "    (\"Aggressive Cleaning\", make_pos_cfg_and_parse_v1),\n",
    "    (\"English Enhancement\", make_pos_cfg_and_parse_v2),\n",
    "    (\"Simple Grammar\", make_pos_cfg_and_parse_v3),\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TESTING SENTENCE: {sentence}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    success = False\n",
    "    for approach_name, approach_func in approaches:\n",
    "        print(f\"\\n--- Trying {approach_name} ---\")\n",
    "        try:\n",
    "            parsed_tree = approach_func(sentence)\n",
    "            print(\"SUCCESS!\")\n",
    "            print(parsed_tree.pformat(margin=60))\n",
    "            success = True\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if success:\n",
    "        print(f\"\\n Successfully parsed: {sentence}\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"\\n All approaches failed for: {sentence}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RECOMMENDATION:\")\n",
    "print(\"If none of these work, consider:\")\n",
    "print(\"1. Updating constituent_treelib: pip install --upgrade constituent-treelib\")\n",
    "print(\"2. Using a different parsing library like spacy-stanza\")\n",
    "print(\"3. Using NLTK's built-in parsers directly\")\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202cc4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/glora/projects/populate-ns-lex/.venv-benepar/lib/python3.9/site-packages/torch/distributions/distribution.py:55: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "[E046] Can't retrieve unregistered extension attribute 'constituent_tree'. Did you forget to call the `set_extension` method?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(sent)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m span \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# ._.constituent_tree gives you an NLTK Tree object\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     trees\u001b[38;5;241m.\u001b[39mappend(\u001b[43mspan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstituent_tree\u001b[49m)\n",
      "File \u001b[0;32m~/projects/populate-ns-lex/.venv-benepar/lib/python3.9/site-packages/spacy/tokens/underscore.py:48\u001b[0m, in \u001b[0;36mUnderscore.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extensions:\n\u001b[0;32m---> 48\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE046\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m     49\u001b[0m     default, method, getter, setter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extensions[name]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m getter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: [E046] Can't retrieve unregistered extension attribute 'constituent_tree'. Did you forget to call the `set_extension` method?"
     ]
    }
   ],
   "source": [
    "# 2. Parse each sentence and collect its constituent tree\n",
    "trees = []\n",
    "for sent in real_split_annotations:\n",
    "    doc = nlp(sent)\n",
    "    for span in doc.sents:\n",
    "        # use benepars registered parse_tree extension\n",
    "        trees.append(span._.parse_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4b4132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (PRP It))\n",
      "  (VP\n",
      "    (VBZ is)\n",
      "    (ADVP (RB never))\n",
      "    (ADJP\n",
      "      (RB too)\n",
      "      (JJ late)\n",
      "      (S (VP (TO to) (VP (VB do) (NP (NN something)))))))\n",
      "  (. .))\n",
      "(NP (PRP It))\n"
     ]
    }
   ],
   "source": [
    "# doc = nlp('The time for action is now. It is never too late to do something.')\n",
    "# sent = list(doc.sents)[0]\n",
    "# bracketed = sent._.parse_string\n",
    "\n",
    "# # build an nltk.Tree\n",
    "# tree = Tree.fromstring(bracketed)\n",
    "\n",
    "# # print and visualize\n",
    "# print(tree)\n",
    "# # tree.pretty_print()\n",
    "\n",
    "# # inspect first child of the sentence\n",
    "# print(tree[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e107cfad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"360px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,824.0,360.0\" width=\"824px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"40.7767%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"45.2381%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADJP</text></svg><svg width=\"36.8421%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Black</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"18.4211%\" y1=\"20px\" y2=\"48px\" /><svg width=\"26.3158%\" x=\"36.8421%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CC</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">and</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /><svg width=\"36.8421%\" x=\"63.1579%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">white</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"81.5789%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"22.619%\" y1=\"20px\" y2=\"48px\" /><svg width=\"30.9524%\" x=\"45.2381%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">traditional</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"60.7143%\" y1=\"20px\" y2=\"48px\" /><svg width=\"23.8095%\" x=\"76.1905%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">building</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"88.0952%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"20.3883%\" y1=\"20px\" y2=\"48px\" /><svg width=\"59.2233%\" x=\"40.7767%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PP</text></svg><svg width=\"9.83607%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">with</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"4.91803%\" y1=\"20px\" y2=\"48px\" /><svg width=\"90.1639%\" x=\"9.83607%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"43.6364%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"25%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">sign</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.5%\" y1=\"20px\" y2=\"48px\" /><svg width=\"75%\" x=\"25%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VP</text></svg><svg width=\"50%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBG</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">hanging</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25%\" y1=\"20px\" y2=\"48px\" /><svg width=\"50%\" x=\"50%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADVP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">RB</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">outside</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"62.5%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"21.8182%\" y1=\"20px\" y2=\"48px\" /><svg width=\"9.09091%\" x=\"43.6364%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CC</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">and</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"48.1818%\" y1=\"20px\" y2=\"48px\" /><svg width=\"47.2727%\" x=\"52.7273%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"15.3846%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">3</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"7.69231%\" y1=\"20px\" y2=\"48px\" /><svg width=\"19.2308%\" x=\"15.3846%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">big</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25%\" y1=\"20px\" y2=\"48px\" /><svg width=\"30.7692%\" x=\"34.6154%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">arched</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /><svg width=\"34.6154%\" x=\"65.3846%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">windows</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"82.6923%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.3636%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"54.918%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"70.3883%\" y1=\"20px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('NP', [Tree('NP', [Tree('ADJP', [Tree('JJ', ['Black']), Tree('CC', ['and']), Tree('JJ', ['white'])]), Tree('JJ', ['traditional']), Tree('NN', ['building'])]), Tree('PP', [Tree('IN', ['with']), Tree('NP', [Tree('NP', [Tree('NP', [Tree('NN', ['sign'])]), Tree('VP', [Tree('VBG', ['hanging']), Tree('ADVP', [Tree('RB', ['outside'])])])]), Tree('CC', ['and']), Tree('NP', [Tree('CD', ['3']), Tree('JJ', ['big']), Tree('JJ', ['arched']), Tree('NNS', ['windows'])])])])])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trees[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5e51fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15429"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_prods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c1c087",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: '\"\\'\", \\'Brodie\\', \"\\'s\"'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[137], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# test on the first sentence\u001b[39;00m\n\u001b[1;32m      2\u001b[0m test_sent \u001b[38;5;241m=\u001b[39m trees[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mleaves()\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_sent\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# #     # t.pretty_print()\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(t)\n",
      "File \u001b[0;32m~/projects/populate-ns-lex/venv/lib/python3.9/site-packages/nltk/parse/chart.py:1474\u001b[0m, in \u001b[0;36mChartParser.parse\u001b[0;34m(self, tokens, tree_class)\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens, tree_class\u001b[38;5;241m=\u001b[39mTree):\n\u001b[0;32m-> 1474\u001b[0m     chart \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchart_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(chart\u001b[38;5;241m.\u001b[39mparses(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grammar\u001b[38;5;241m.\u001b[39mstart(), tree_class\u001b[38;5;241m=\u001b[39mtree_class))\n",
      "File \u001b[0;32m~/projects/populate-ns-lex/venv/lib/python3.9/site-packages/nltk/parse/earleychart.py:352\u001b[0m, in \u001b[0;36mIncrementalChartParser.chart_parse\u001b[0;34m(self, tokens, trace)\u001b[0m\n\u001b[1;32m    349\u001b[0m trace_new_edges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trace_new_edges\n\u001b[1;32m    351\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(tokens)\n\u001b[0;32m--> 352\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grammar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_coverage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m chart \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chart_class(tokens)\n\u001b[1;32m    354\u001b[0m grammar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grammar\n",
      "File \u001b[0;32m~/projects/populate-ns-lex/venv/lib/python3.9/site-packages/nltk/grammar.py:666\u001b[0m, in \u001b[0;36mCFG.check_coverage\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[1;32m    665\u001b[0m     missing \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m missing)\n\u001b[0;32m--> 666\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    667\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrammar does not cover some of the \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput words: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m missing\n\u001b[1;32m    668\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Grammar does not cover some of the input words: '\"\\'\", \\'Brodie\\', \"\\'s\"'."
     ]
    }
   ],
   "source": [
    "# test on the first sentence\n",
    "test_sent = trees[0].leaves()\n",
    "\n",
    "for t in parser.parse(test_sent):\n",
    "# #     # t.pretty_print()\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d604e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48cb60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25698\n"
     ]
    }
   ],
   "source": [
    "# 4. build a grammar (dedupe productions)\n",
    "start_sym = trees[0].label()           \n",
    "\n",
    "prods = list({ p for p in all_prods })  # set-dedupe, original producing 2000 cfg rules\n",
    "grammar = CFG(Nonterminal(start_sym), prods)\n",
    "\n",
    "punct_tags = {\",\", \":\", \"``\", \"''\", \".\", \"(\", \")\", \"--\"}\n",
    "filtered_prods = [\n",
    "  p for p in all_prods\n",
    "  if not any(\n",
    "    isinstance(sym, Nonterminal) and sym.symbol() in punct_tags\n",
    "    for sym in p.rhs()\n",
    "  )\n",
    "]\n",
    "# grammar = CFG(Nonterminal(start_sym), filtered_prods) # producing 26k++ rules\n",
    "print(len(filtered_prods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ea666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# punct_tags = {\",\", \":\", \"``\", \"''\", \".\", \"(\", \")\", \"--\"}\n",
    "# filtered_prods = [\n",
    "#   p for p in all_prods\n",
    "#   if not any(\n",
    "#     isinstance(sym, Nonterminal) and sym.symbol() in punct_tags\n",
    "#     for sym in p.rhs()\n",
    "#   )\n",
    "# ]\n",
    "# grammar = CFG(Nonterminal(start_sym), filtered_prods) # producing 25k+ rules\n",
    "# # before you extract productions, do for each tree:\n",
    "# for t in trees:\n",
    "#     # this will break any rule X  A B C D  into a chain of binaries\n",
    "#     Tree.chomsky_normal_form(t, horzMarkov=2)\n",
    "\n",
    "# # extract production to keep RHS at length 2 or 1\n",
    "\n",
    "# from collections import Counter\n",
    "\n",
    "# counts = Counter(all_prods)\n",
    "# keep = [p for p in all_prods if counts[p] > 1]   # e.g. freq > 1\n",
    "# grammar = CFG(Nonterminal(start_sym), list({p for p in keep}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cba191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cb59b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 1254 productions (start state = NP)\n",
      "    VP -> MD PP\n",
      "    VBN -> 'named'\n",
      "    NN -> 'blind'\n",
      "    NNS -> 'pillars'\n",
      "    NN -> 'archway'\n",
      "    NN -> 'person'\n",
      "    NN -> 'postbox'\n",
      "    ADJP -> CD HYPH VBN\n",
      "    JJ -> 'beautiful'\n",
      "    NNS -> 'tables'\n",
      "    NP -> NP , ADJP\n",
      "    NN -> 'clock'\n",
      "    NP -> DT NNP\n",
      "    NN -> 'day'\n",
      "    NN -> 'image'\n",
      "    NP -> DT JJ , JJ NN\n",
      "    IN -> 'in'\n",
      "    NN -> 'style'\n",
      "    VP -> VBZ ADVP VP\n",
      "    RB -> 'directly'\n",
      "    JJ -> 'opposite'\n",
      "    VBN -> 'passed'\n",
      "    JJ -> 'vertical'\n",
      "    NN -> 'design'\n",
      "    IN -> 'up'\n",
      "    VP -> VBP VP\n",
      "    VB -> 'look'\n",
      "    NN -> 'outside'\n",
      "    NN -> 'tavern'\n",
      "    S -> NP\n",
      "    NN -> 'colour'\n",
      "    NNP -> 'Castro'\n",
      "    NN -> 'pyramid'\n",
      "    NP -> DT NML JJ NN\n",
      "    JJ -> 'outside'\n",
      "    IN -> 'from'\n",
      "    S -> CC NP VP\n",
      "    NNP -> 'Carlton'\n",
      "    NP -> JJ PP\n",
      "    NN -> 'part'\n",
      "    JJ -> 'pretty'\n",
      "    ADVP -> RBS RB\n",
      "    NP -> NNS NN\n",
      "    VBN -> 'nestled'\n",
      "    RBS -> 'least'\n",
      "    : -> '-'\n",
      "    NP -> DT JJ , NML NN\n",
      "    NNP -> 'Church'\n",
      "    JJ -> 'twin'\n",
      "    DT -> 'no'\n",
      "    DT -> 'an'\n",
      "    NN -> 'board'\n",
      "    DT -> 'The'\n",
      "    SQ -> VBP NP VP\n",
      "    NNP -> 'Frankenstein'\n",
      "    S -> S , NP VP\n",
      "    NN -> 'stone'\n",
      "    RB -> 'probably'\n",
      "    VBN -> 'set'\n",
      "    JJ -> 'large'\n",
      "    SBAR -> WHADVP S\n",
      "    NN -> 'thing'\n",
      "    NN -> 'couple'\n",
      "    NN -> 'wall'\n",
      "    IN -> 'inbetween'\n",
      "    NNS -> 'windows'\n",
      "    NNS -> 'doors'\n",
      "    JJ -> 'interior'\n",
      "    VB -> 'locate'\n",
      "    JJ -> 'Modern'\n",
      "    JJ -> 'detailed'\n",
      "    NN -> 'effect'\n",
      "    IN -> 'around'\n",
      "    JJ -> 'gray'\n",
      "    JJ -> 'octagonal'\n",
      "    NNP -> 'Basil'\n",
      "    NN -> 'name'\n",
      "    ADJP -> RB PP\n",
      "    CD -> '+-'\n",
      "    PP -> NP IN NP\n",
      "    PRP -> 'You'\n",
      "    NN -> 'scene'\n",
      "    JJ -> 'towered'\n",
      "    NN -> 'river'\n",
      "    '' -> \"'\"\n",
      "    VP -> VBN PP PP\n",
      "    VBP -> 'do'\n",
      "    NNS -> 'menus'\n",
      "    NNS -> 'princes'\n",
      "    DT -> 'either'\n",
      "    VBG -> 'flying'\n",
      "    S -> ADVP NP VP\n",
      "    NNS -> 'blinds'\n",
      "    NNS -> 'stones'\n",
      "    NP -> JJ JJ NNS\n",
      "    NNP -> 'Edinburgh'\n",
      "    JJ -> 'external'\n",
      "    JJ -> 'orange'\n",
      "    NN -> 'line'\n",
      "    NN -> 'edge'\n",
      "    NN -> 'cabin'\n",
      "    NN -> 'Cafe'\n",
      "    JJ -> 'second'\n",
      "    NNS -> 'ones'\n",
      "    NNP -> 'Amarone'\n",
      "    NP -> NP ADVP\n",
      "    JJ -> 'typical'\n",
      "    ADJP -> ADJP JJ\n",
      "    NP -> JJR NNS\n",
      "    VP -> VBN NP\n",
      "    NNS -> 'things'\n",
      "    JJ -> 'cool'\n",
      "    NN -> 'architecture'\n",
      "    NN -> 'balcony'\n",
      "    NN -> 'metal'\n",
      "    PRT -> RP\n",
      "    VBG -> 'standing'\n",
      "    RB -> 'front'\n",
      "    NN -> 'column'\n",
      "    NP -> `` NP ''\n",
      "    NN -> 'police'\n",
      "    NN -> 'end'\n",
      "    VP -> VBG NP\n",
      "    NN -> 'something'\n",
      "    DT -> 'that'\n",
      "    NNP -> 'Exhibition'\n",
      "    RB -> 'about'\n",
      "    JJ -> '1st'\n",
      "    NNP -> 'Cathedral'\n",
      "    VB -> 'get'\n",
      "    NP -> DT NNP NNP\n",
      "    ADJP -> RB RB PP\n",
      "    NNP -> 'Festival'\n",
      "    ADJP -> JJ HYPH VBG\n",
      "    NML -> NN HYPH NN\n",
      "    ADJP -> NP JJ\n",
      "    NN -> 'street'\n",
      "    ADVP -> RB IN\n",
      "    S -> NP PP PP\n",
      "    NP -> NP , PP\n",
      "    VBN -> 'stained'\n",
      "    ADJP -> ADJP CC ADJP\n",
      "    VB -> 'See'\n",
      "    PRP -> 'us'\n",
      "    VB -> 'find'\n",
      "    NNS -> 'lots'\n",
      "    JJR -> 'older'\n",
      "    RB -> 'nearby'\n",
      "    JJ -> 'Old'\n",
      "    CD -> '4'\n",
      "    NP -> ADJP NNS\n",
      "    NML -> NNP NNP\n",
      "    RB -> 'up'\n",
      "    HYPH -> '+-'\n",
      "    JJ -> 'Blue'\n",
      "    VBZ -> 'happens'\n",
      "    JJ -> 'possible'\n",
      "    JJ -> 'interesting'\n",
      "    VBN -> 'surrounded'\n",
      "    VBG -> 'being'\n",
      "    NP -> DT NML\n",
      "    RB -> 'now'\n",
      "    NP -> NP , CC NP\n",
      "    NP -> NP RB\n",
      "    NP -> DT JJ NML NN\n",
      "    NP -> DT NNPS\n",
      "    VP -> VBG NP NP\n",
      "    NN -> 'cream'\n",
      "    VB -> 'Point'\n",
      "    NML -> JJ NNP\n",
      "    NN -> 'sand'\n",
      "    VP -> VB ADJP\n",
      "    NN -> 'block'\n",
      "    RB -> 'immediately'\n",
      "    CD -> 'three'\n",
      "    PP -> IN ADVP\n",
      "    JJ -> 'full'\n",
      "    DT -> 'this'\n",
      "    NN -> 'structure'\n",
      "    NP -> DT JJ NN SYM NN\n",
      "    NN -> 'interest'\n",
      "    NN -> 'cupola'\n",
      "    S -> RB VP\n",
      "    NP -> JJ CD\n",
      "    NN -> \n",
      "    EX -> 'There'\n",
      "    VP -> VBZ NP\n",
      "    VP -> VBP RB VP\n",
      "    PP -> ADVP IN NP\n",
      "    JJ -> \n",
      "    JJ -> 'greek'\n",
      "    VP -> VBZ NP PP PP\n",
      "    JJ -> 'beige'\n",
      "    NP -> JJ NN\n",
      "    NN -> 'suruchi'\n",
      "    IN -> 'below'\n",
      "    NN -> 'Pub'\n",
      "    JJ -> 'circular'\n",
      "    ADJP -> JJ HYPH VBN\n",
      "    SBAR -> WHNP S\n",
      "    ADJP -> VBN PP\n",
      "    NP -> CD NNS\n",
      "    VBN -> 'called'\n",
      "    NN -> 'base'\n",
      "    NP -> NML NN NN\n",
      "    RB -> 'temporarily'\n",
      "    NN -> 'wood'\n",
      "    NN -> 'height'\n",
      "    DT -> 'some'\n",
      "    JJ -> 'gold'\n",
      "    VP -> VBZ ADJP SBAR\n",
      "    JJ -> 'grassy'\n",
      "    NN -> 'copper'\n",
      "    ADJP -> JJ CC JJ\n",
      "    ADVP -> ADVP PP\n",
      "    NN -> 'home'\n",
      "    -RRB- -> '?'\n",
      "    CC -> 'And'\n",
      "    JJ -> 'nice'\n",
      "    NN -> 'doorway'\n",
      "    JJ -> 'cobbled'\n",
      "    NP -> NML NN\n",
      "    VP -> VBG PRT PP\n",
      "    NP -> DT JJ JJ NNS\n",
      "    NP -> CD JJ NN NNS\n",
      "    RP -> 'out'\n",
      "    NNP -> 'Cashmere'\n",
      "    NN -> 'frankenstein'\n",
      "    JJ -> 'blue'\n",
      "    VBD -> 'got'\n",
      "    NNP -> 'Monument'\n",
      "    VBG -> 'looking'\n",
      "    JJ -> 'long'\n",
      "    NN -> 'guest'\n",
      "    NN -> 'branch'\n",
      "    VP -> VB\n",
      "    JJ -> 'Georgian'\n",
      "    NP -> DT NN NN\n",
      "    NNS -> 'Poundsavers'\n",
      "    JJR -> 'lower'\n",
      "    NN -> 'bend'\n",
      "    NN -> 'pediment'\n",
      "    NN -> 'entry'\n",
      "    ADJP -> ADVP JJ\n",
      "    RB -> 'rather'\n",
      "    NN -> 'museum'\n",
      "    NN -> 'maroon'\n",
      "    JJ -> 'many'\n",
      "    JJ -> 'golden'\n",
      "    IN -> 'off'\n",
      "    NN -> 'pole'\n",
      "    `` -> '-LSB-'\n",
      "    S -> PP , NP VP\n",
      "    PP -> IN S\n",
      "    NNS -> 'awnings'\n",
      "    NML -> NNP NNP NNP\n",
      "    NN -> 'group'\n",
      "    NNS -> 'curtains'\n",
      "    NP -> NP , NP\n",
      "    NN -> 'statue'\n",
      "    NP -> RB JJ NN\n",
      "    JJ -> 'victorian'\n",
      "    NNP -> 'Brodie'\n",
      "    PRP -> 'itself'\n",
      "    NN -> 'tower'\n",
      "    NP -> CD JJ NNS\n",
      "    NP -> NP : NP\n",
      "    NN -> 'photo'\n",
      "    VP -> VBP\n",
      "    VP -> VBP NP PP\n",
      "    JJ -> 'reddish'\n",
      "    NNS -> 'gables'\n",
      "    NN -> 'shopfront'\n",
      "    ADJP -> JJ HYPH JJ\n",
      "    JJ -> 'classical'\n",
      "    NN -> 'bunch'\n",
      "    JJ -> 'next'\n",
      "    JJ -> 'only'\n",
      "    IN -> 'before'\n",
      "    NNS -> 'seats'\n",
      "    NP -> DT CD JJ NN NNS\n",
      "    ADJP -> JJ SYM JJ\n",
      "    S -> S , S\n",
      "    PRP -> \n",
      "    PP -> IN ADJP\n",
      "    JJ -> 'similar'\n",
      "    NP -> DT NNP NNPS\n",
      "    NP -> NP IN NP\n",
      "    NML -> NN NN\n",
      "    NNS -> 'corners'\n",
      "    NN -> 'right'\n",
      "    NP -> DT VBG NNS\n",
      "    IN -> 'than'\n",
      "    NML -> CD NN\n",
      "    IN -> 'On'\n",
      "    RB -> 'closest'\n",
      "    NP -> NNP JJ NN\n",
      "    NN -> 'sign'\n",
      "    VBN -> 'faced'\n",
      "    JJ -> 'right'\n",
      "    NP -> NP , PP , PP\n",
      "    JJ -> 'upper'\n",
      "    NN -> 'bar'\n",
      "    VBG -> 'moving'\n",
      "    NNS -> 'bikes'\n",
      "    NNS -> 'flowers'\n",
      "    JJ -> 'elephant'\n",
      "    NP -> DT ADJP NNS\n",
      "    NNP -> 'The'\n",
      "    NNS -> 'arches'\n",
      "    NN -> 'granite'\n",
      "    NP -> NP -LRB- NP -RRB-\n",
      "    JJ -> 'high'\n",
      "    NP -> DT CD NNS\n",
      "    VP -> VBD PP PP\n",
      "    NN -> 'cathedral'\n",
      "    NN -> 'side'\n",
      "    NP -> ADJP JJ NN\n",
      "    S -> NP NP VP\n",
      "    NN -> 'steeple'\n",
      "    VP -> TO VP\n",
      "    NN -> 'garage'\n",
      "    NNP -> 'Thai'\n",
      "    NN -> 'van'\n",
      "    PRP -> 'i'\n",
      "    PP -> PP , PP\n",
      "    NN -> 'car'\n",
      "    JJ -> 'whole'\n",
      "    NN -> 'ground'\n",
      "    S -> '' NP VP\n",
      "    IN -> 'under'\n",
      "    JJ -> 'historic'\n",
      "    VBG -> 'overlooking'\n",
      "    PRP -> 'we'\n",
      "    NNP -> 'National'\n",
      "    NN -> 'flat'\n",
      "    JJ -> 'ground'\n",
      "    VP -> VBZ SBAR\n",
      "    NNS -> 'people'\n",
      "    NN -> 'subway'\n",
      "    NP -> CD\n",
      "    RB -> 'alone'\n",
      "    JJ -> 'flat'\n",
      "    VP -> VBG PP\n",
      "    NNP -> 'Mile'\n",
      "    NNP -> 'New'\n",
      "    JJ -> 'good'\n",
      "    VB -> 'Look'\n",
      "    VBG -> 'starting'\n",
      "    CD -> '1'\n",
      "    ADVP -> JJ\n",
      "    NN -> 'House'\n",
      "    IN -> 'against'\n",
      "    NP -> NN\n",
      "    IN -> 'In'\n",
      "    NP -> DT NN NN NN\n",
      "    NN -> 'building'\n",
      "    NNP -> 'Moray'\n",
      "    NN -> 'scaffolding'\n",
      "    VP -> VBP S\n",
      "    NNP -> 'Fraser'\n",
      "    JJ -> 'pointed'\n",
      "    NP -> NP\n",
      "    VB -> 'ask'\n",
      "    NNP -> 'Decon'\n",
      "    NN -> 'set'\n",
      "    NN -> 'ness'\n",
      "    UCP -> ADVP CC PP\n",
      "    JJ -> 'white'\n",
      "    S -> PP NP VP\n",
      "    NP -> VBN NN\n",
      "    NN -> 'strip'\n",
      "    JJ -> 'Grey'\n",
      "    NN -> 'google'\n",
      "    VBG -> 'hanging'\n",
      "    NP -> NP PP ADVP\n",
      "    NN -> 'red'\n",
      "    NP -> DT CD JJ NNS\n",
      "    VP -> VBN PP\n",
      "    VP -> VBG NP PP\n",
      "    `` -> \"'\"\n",
      "    NNS -> 'bricks'\n",
      "    VBG -> 'wearing'\n",
      "    NNS -> 'stories'\n",
      "    VBN -> 'parked'\n",
      "    JJ -> 'red'\n",
      "    NP -> JJS NN\n",
      "    NN -> 'criss'\n",
      "    NN -> 'water'\n",
      "    NN -> 'gap'\n",
      "    NP -> NP NNP NNP\n",
      "    NP -> JJ NNP\n",
      "    NP -> JJ JJ NN\n",
      "    NNP -> 'Hart'\n",
      "    NP -> JJ NN NNS\n",
      "    NNP -> 'Street'\n",
      "    RB -> 'back'\n",
      "    RB -> 'before'\n",
      "    NP -> NN CC NN\n",
      "    NNS -> 'lights'\n",
      "    NP -> QP NNS\n",
      "    ADJP -> RB JJ\n",
      "    NP -> NP , SBAR\n",
      "    CD -> '7'\n",
      "    NNS -> 'feet'\n",
      "    ADJP -> NP HYPH VBN\n",
      "    NN -> 'shape'\n",
      "    NP -> DT NML NN NN\n",
      "    NNP -> 'Bridge'\n",
      "    NN -> 'pattern'\n",
      "    NN -> 'wedge'\n",
      "    NP -> PRP\n",
      "    MD -> 'will'\n",
      "    IN -> 'between'\n",
      "    NP -> JJ , JJ NN\n",
      "    VBN -> 'flanked'\n",
      "    IN -> 'for'\n",
      "    NN -> 'bronze'\n",
      "    VBN -> 'painted'\n",
      "    JJ -> 'ugly'\n",
      "    NP -> NML NNP\n",
      "    ADJP -> RB JJ PP\n",
      "    NP -> PRP POS\n",
      "    VP -> VBP ADVP VP\n",
      "    NP -> JJR NN\n",
      "    JJ -> 'Black'\n",
      "    NN -> 'stop'\n",
      "    S -> `` NP\n",
      "    NN -> 'road'\n",
      "    VBZ -> \"'s\"\n",
      "    NP -> NNS CC NNS\n",
      "    JJ -> 'yellow'\n",
      "    NN -> 'sandwich'\n",
      "    NN -> 'verandah'\n",
      "    IN -> 'after'\n",
      "    S -> NP ADVP VP\n",
      "    ADVP -> RB RBS\n",
      "    VBN -> 'capped'\n",
      "    NN -> 'pillar'\n",
      "    VBN -> 'pointed'\n",
      "    NNS -> 'storeys'\n",
      "    NN -> 'booth'\n",
      "    NNS -> 'stairs'\n",
      "    NP -> NNP POS\n",
      "    NNPS -> 'Gardens'\n",
      "    VP -> MD VP\n",
      "    NN -> 'Building'\n",
      "    NP -> CD JJ JJ JJ NNS\n",
      "    NP -> NN CD\n",
      "    JJ -> 'quaint'\n",
      "    JJ -> 'shaped'\n",
      "    IN -> 'towards'\n",
      "    NN -> 'arch'\n",
      "    NN -> 'pub'\n",
      "    JJ -> 'pointy'\n",
      "    NN -> 'letter'\n",
      "    NP -> DT JJ\n",
      "    NNS -> 'floors'\n",
      "    VP -> VBD S\n",
      "    NP -> DT NML NNS\n",
      "    JJ -> 'third'\n",
      "    NN -> 'section'\n",
      "    NN -> 'complex'\n",
      "    NNS -> 'signs'\n",
      "    NN -> 'arrow'\n",
      "    CD -> '2'\n",
      "    JJS -> 'closest'\n",
      "    NNS -> 'balconies'\n",
      "    NN -> 'top'\n",
      "    NN -> 'facade'\n",
      "    CD -> 'five'\n",
      "    VP -> VBZ PP\n",
      "    VP -> NN\n",
      "    RB -> 'ahead'\n",
      "    CD -> 'Two'\n",
      "    VBN -> 'built'\n",
      "    JJ -> 'complete'\n",
      "    WHNP -> WP\n",
      "    NP -> NP PP PP\n",
      "    NN -> 'bit'\n",
      "    NN -> 'square'\n",
      "    S -> NP , PP CC VP\n",
      "    NP -> DT JJ NNS\n",
      "    JJ -> 'open'\n",
      "    NN -> 'plaque'\n",
      "    NNP -> 'North'\n",
      "    NN -> 'mile'\n",
      "    -RRB- -> ''\n",
      "    JJ -> 'square'\n",
      "    NP -> NN SYM NN\n",
      "    NP -> NNP NNP NNP\n",
      "    NML -> CD HYPH NN\n",
      "    VP -> VBN S PP\n",
      "    NP -> DT JJ CD NNS\n",
      "    NN -> 'apartment'\n",
      "    NNS -> 'newsagents'\n",
      "    NN -> 'flag'\n",
      "    PRP$ -> 'its'\n",
      "    JJ -> 'same'\n",
      "    NN -> 'Church'\n",
      "    VP -> ADVP VBN PP\n",
      "    VP -> VP , CC VP\n",
      "    JJ -> 'fancy'\n",
      "    VP -> VBG ADVP PP\n",
      "    NNS -> 'shutters'\n",
      "    NN -> 'box'\n",
      "    PP -> PP NP VP\n",
      "    NN -> 'grass'\n",
      "    DT -> 'An'\n",
      "    ADJP -> ADJP SBAR\n",
      "    NN -> 'courtyard'\n",
      "    NN -> 'hand'\n",
      "    NN -> 'life'\n",
      "    IN -> 'into'\n",
      "    ADJP -> ADJP ADJP\n",
      "    VP -> VBZ NP ADVP\n",
      "    JJ -> 'several'\n",
      "    NP -> NML NNPS\n",
      "    NML -> NML CC NML\n",
      "    NP -> DT JJR JJ NN\n",
      "    VBN -> 'shaped'\n",
      "    NP -> NN NN\n",
      "    NNP -> 'City'\n",
      "    VBZ -> 'says'\n",
      "    IN -> 'near'\n",
      "    NN -> 'assembly'\n",
      "    NML -> NN\n",
      "    IN -> 'atop'\n",
      "    VP -> VP CC VP\n",
      "    NP -> CD NN\n",
      "    JJ -> 'brownish'\n",
      "    NNP -> 'Harry'\n",
      "    PRP$ -> 'our'\n",
      "    ADJP -> NN HYPH VBN\n",
      "    NN -> 'frame'\n",
      "    PP -> PP CC NP\n",
      "    NN -> 'bus'\n",
      "    WHADVP -> WRB\n",
      "    DT -> 'This'\n",
      "    JJ -> 'Narrow'\n",
      "    NNS -> 'bits'\n",
      "    ADJP -> NN HYPH JJ\n",
      "    JJ -> 'like'\n",
      "    VBZ -> 'looks'\n",
      "    NNS -> 'columns'\n",
      "    NP -> NP NNP NN\n",
      "    JJ -> 'double'\n",
      "    NML -> VBN NN\n",
      "    IN -> 'beyond'\n",
      "    CD -> 'one'\n",
      "    JJ -> 'tall'\n",
      "    QP -> ADVP CD\n",
      "    NN -> 'target'\n",
      "    NN -> 'bike'\n",
      "    NN -> 'close'\n",
      "    NN -> 'dog'\n",
      "    NN -> 'site'\n",
      "    CD -> '6'\n",
      "    NP -> RB\n",
      "    DT -> 'those'\n",
      "    JJ -> 'pink'\n",
      "    IN -> 'opposite'\n",
      "    VBG -> 'passing'\n",
      "    NP -> NNP\n",
      "    RB -> 'possibly'\n",
      "    JJ -> 'classic'\n",
      "    NN -> 'city'\n",
      "    NN -> 'script'\n",
      "    JJ -> 'left'\n",
      "    NN -> 'theatre'\n",
      "    NN -> 'sash'\n",
      "    SYM -> '/'\n",
      "    NNS -> 'parasols'\n",
      "    NN -> 'cafe'\n",
      "    NP -> NP RB RB NP\n",
      "    DT -> 'all'\n",
      "    ADJP -> JJ VBG\n",
      "    RB -> 'approx'\n",
      "    VP -> VBZ ADVP NP\n",
      "    NN -> 'sky'\n",
      "    NN -> 'word'\n",
      "    VB -> 'be'\n",
      "    VBZ -> 'is'\n",
      "    IN -> 'outside'\n",
      "    NN -> 'pizza'\n",
      "    NN -> 'distance'\n",
      "    NP -> DT ADJP JJ NN\n",
      "    NN -> 'construction'\n",
      "    NP -> ADVP NP\n",
      "    ADJP -> ADJP PP\n",
      "    NN -> 'exterior'\n",
      "    VBN -> 'made'\n",
      "    ADJP -> NN CC JJ\n",
      "    IN -> 'at'\n",
      "    PP -> ADVP PP\n",
      "    CD -> '18'\n",
      "    NNP -> 'Giles'\n",
      "    ADJP -> JJR\n",
      "    S -> NP , VP\n",
      "    SBAR -> SINV\n",
      "    RB -> 'so'\n",
      "    VP -> VBG ADVP\n",
      "    CD -> 'four'\n",
      "    NP -> CD NN NNS\n",
      "    NP -> DT NNP POS\n",
      "    JJ -> 'wide'\n",
      "    IN -> 'Across'\n",
      "    ADVP -> IN\n",
      "    NP -> PRP$ JJ\n",
      "    NP -> PRP$ ADJP\n",
      "    JJ -> 'huge'\n",
      "    NN -> 'traffic'\n",
      "    NP -> NP , SBAR ,\n",
      "    , -> ','\n",
      "    JJ -> 'royal'\n",
      "    JJS -> 'tallest'\n",
      "    DT -> 'All'\n",
      "    VB -> 'check'\n",
      "    VBN -> 'got'\n",
      "    NN -> 'woman'\n",
      "    RB -> 'Just'\n",
      "    RB -> 'far'\n",
      "    NP -> NN NN NN\n",
      "    VBG -> 'walking'\n",
      "    NP -> NP ADJP\n",
      "    VBP -> 'know'\n",
      "    JJ -> 'new'\n",
      "    NNP -> 'Rowling'\n",
      "    NN -> 'town'\n",
      "    DT -> 'the'\n",
      "    PP -> VBG NP\n",
      "    NN -> 'patisserie'\n",
      "    JJ -> 'green'\n",
      "    NNS -> 'POUNDSAVERS'\n",
      "    NN -> 'glass'\n",
      "    NNS -> 'steps'\n",
      "    VP -> VBN ADVP\n",
      "    POS -> \"'s\"\n",
      "    NN -> 'yard'\n",
      "    NNS -> 'turrets'\n",
      "    VBG -> 'having'\n",
      "    NNP -> 'Patisserie'\n",
      "    IN -> 'without'\n",
      "    MD -> 'can'\n",
      "    VBN -> 'left'\n",
      "    JJR -> 'closer'\n",
      "    CD -> '435'\n",
      "    PP -> PP , CC PP\n",
      "    VP -> VB PP\n",
      "    NN -> 'hall'\n",
      "    NNP -> 'Restaurant'\n",
      "    NN -> 'plinth'\n",
      "    IN -> 'inside'\n",
      "    NN -> 'tudor'\n",
      "    NP -> ADJP NN NN\n",
      "    VB -> 'think'\n",
      "    NNS -> 'houses'\n",
      "    NP -> NN CC JJ\n",
      "    NNS -> 'shades'\n",
      "    NN -> 'cashmere'\n",
      "    VP -> VBG PP PP\n",
      "    INTJ -> UH\n",
      "    VBZ -> 'seems'\n",
      "    VBP -> 'have'\n",
      "    NN -> 'ft'\n",
      "    JJ -> 'entire'\n",
      "    NN -> 'front'\n",
      "    RB -> 'diagonally'\n",
      "    JJ -> 'bright'\n",
      "    PP -> PP SBAR\n",
      "    VBD -> 'left'\n",
      "    NP -> NN RB\n",
      "    NN -> 'Art'\n",
      "    JJ -> 'front'\n",
      "    NP -> DT JJS\n",
      "    NP -> PRP$ JJ NN\n",
      "    S -> VP .\n",
      "    JJ -> 'brown'\n",
      "    NN -> 'office'\n",
      "    RB -> 'just'\n",
      "    PRP -> 'them'\n",
      "    ADVP -> JJ PP\n",
      "    '' -> '\"'\n",
      "    PRP$ -> 'Its'\n",
      "    FRAG -> PP , NP\n",
      "    -RRB- -> '-RRB-'\n",
      "    NNPS -> 'Poundsavers'\n",
      "    JJS -> 'nearest'\n",
      "    RB -> 'very'\n",
      "    VP -> VBZ ADJP PP\n",
      "    NP -> NP , NP , CC NP\n",
      "    NN -> 'door'\n",
      "    NNS -> 'stacks'\n",
      "    VBZ -> 'consists'\n",
      "    NP -> NP CC NP\n",
      "    NN -> 'weaving'\n",
      "    NP -> DT NN\n",
      "    NN -> 'elephant'\n",
      "    S -> VP\n",
      "    NNP -> 'College'\n",
      "    NP -> PDT DT NNS\n",
      "    ADJP -> JJ VBN\n",
      "    VBN -> 'located'\n",
      "    NN -> 'sandstone'\n",
      "    NP -> JJ ADJP NNS\n",
      "    NP -> NP PP , PP\n",
      "    RB -> 'outside'\n",
      "    HYPH -> '-'\n",
      "    IN -> 'along'\n",
      "    : -> ';'\n",
      "    JJ -> 'roman'\n",
      "    JJ -> 'Tall'\n",
      "    RB -> 'straight'\n",
      "    VBN -> 'seen'\n",
      "    JJ -> 'black'\n",
      "    JJ -> 'residential'\n",
      "    RP -> 'up'\n",
      "    ADJP -> JJ JJ\n",
      "    NP -> DT NNS\n",
      "    QP -> RB CD\n",
      "    NNS -> 'pedestrians'\n",
      "    VB -> 'Come'\n",
      "    NP -> DT CD NN NNS\n",
      "    JJ -> 'Big'\n",
      "    NN -> 'umbrella'\n",
      "    VP -> VBZ ADVP\n",
      "    IN -> 'down'\n",
      "    NNP -> 'Tavern'\n",
      "    NNP -> 'Potter'\n",
      "    ADJP -> RB JJ CC JJ\n",
      "    VB -> 'Walk'\n",
      "    NP -> EX\n",
      "    JJ -> 'historical'\n",
      "    NN -> 'view'\n",
      "    VBG -> 'sitting'\n",
      "    NP -> NP , ADVP\n",
      "    VP -> VBP ADJP\n",
      "    JJ -> 'coloured'\n",
      "    RB -> 'away'\n",
      "    JJ -> 'modern'\n",
      "    RB -> 'Not'\n",
      "    VP -> VBP SBAR\n",
      "    PP -> IN `` NP ''\n",
      "    MD -> 'would'\n",
      "    JJ -> 'grey'\n",
      "    NN -> 'area'\n",
      "    VB -> 'know'\n",
      "    ADVP -> RB RB\n",
      "    NN -> 'brick'\n",
      "    S -> S , CC S\n",
      "    JJ -> 'tiny'\n",
      "    JJ -> 'massive'\n",
      "    NNS -> 'galleries'\n",
      "    NN -> 'Statue'\n",
      "    PRP -> 'he'\n",
      "    NP -> DT NN NNS\n",
      "    NML -> JJ HYPH NN\n",
      "    JJ -> 'gothic'\n",
      "    NN -> 'coat'\n",
      "    WHPP -> IN WHNP\n",
      "    VP -> VBN PRT\n",
      "    NNP -> 'Garden'\n",
      "    PP -> IN PP\n",
      "    VP -> VB NP PP\n",
      "    IN -> 'behind'\n",
      "    IN -> 'by'\n",
      "    NNP -> 'Place'\n",
      "    PP -> PP CC PP\n",
      "    NNP -> 'Valerie'\n",
      "    NNP -> 'Town'\n",
      "    FW -> 'florentine'\n",
      "    NN -> 'Pound'\n",
      "    VP -> VB NP\n",
      "    IN -> 'to'\n",
      "    RB -> 'further'\n",
      "    RB -> 'too'\n",
      "    IN -> 'out'\n",
      "    NP -> NP NNP NNP -RRB- CD\n",
      "    JJ -> 'White'\n",
      "    -RRB- -> '-RSB-'\n",
      "    NP -> CD JJ JJ NNS\n",
      "    IN -> 'At'\n",
      "    NN -> 'art'\n",
      "    NNS -> 'cars'\n",
      "    PP -> PP , VP\n",
      "    WDT -> 'that'\n",
      "    RB -> \"n't\"\n",
      "    VBN -> 'situated'\n",
      "    NP -> DT NML NN\n",
      "    RB -> 'not'\n",
      "    IN -> 'Beside'\n",
      "    NP -> JJ NML NN\n",
      "    NNP -> 'Express'\n",
      "    ADVP -> RB PP\n",
      "    ADJP -> RB JJ HYPH JJ\n",
      "    NN -> 'sidewalk'\n",
      "    NN -> 'ade'\n",
      "    ADVP -> IN PP\n",
      "    JJ -> 'Thai'\n",
      "    VP -> VBZ ADVP PP\n",
      "    NN -> 'bay'\n",
      "    NP -> DT ADJP\n",
      "    NN -> 'church'\n",
      "    JJ -> 'round'\n",
      "    VBN -> 'coloured'\n",
      "    NN -> 'park'\n",
      "    S -> S NP VP\n",
      "    NN -> 'chapel'\n",
      "    VP -> VBP PP\n",
      "    RB -> 'infront'\n",
      "    VBG -> 'leading'\n",
      "    DT -> 'A'\n",
      "    . -> '!'\n",
      "    JJ -> 'looking'\n",
      "    WP -> 'what'\n",
      "    NN -> 'level'\n",
      "    NNS -> 'flags'\n",
      "    DT -> 'each'\n",
      "    JJ -> 'domed'\n",
      "    FW -> 'fa+'\n",
      "    JJ -> 'central'\n",
      "    NP -> ADJP NN\n",
      "    VP -> VBZ PP ADVP\n",
      "    PP -> JJ NP\n",
      "    JJ -> 'New'\n",
      "    VBP -> 'see'\n",
      "    NN -> 'restaurant'\n",
      "    JJ -> 'indian'\n",
      "    CC -> 'or'\n",
      "    NNPS -> 'Galleries'\n",
      "    CD -> '3'\n",
      "    DT -> 'another'\n",
      "    NNP -> 'Deacon'\n",
      "    PRP -> 'me'\n",
      "    NNP -> 'Brodies'\n",
      "    NN -> 'valerie'\n",
      "    NNP -> 'St'\n",
      "    VP -> VB PRT NP\n",
      "    VBN -> 'blocked'\n",
      "    NP -> PRP$ NN\n",
      "    NN -> 'man'\n",
      "    NN -> 'entrance'\n",
      "    NN -> 'driveway'\n",
      "    JJ -> 'grand'\n",
      "    RB -> 'Also'\n",
      "    NN -> 'coffee'\n",
      "    NP -> NP , NP CC NP\n",
      "    JJ -> 'Red'\n",
      "    NN -> 'pedestrian'\n",
      "    WHNP -> WDT\n",
      "    PRP -> 'it'\n",
      "    NNP -> 'Elgin'\n",
      "    S -> INTJ , VP\n",
      "    JJ -> 'main'\n",
      "    ADVP -> RB RB PP\n",
      "    RB -> 'much'\n",
      "    RB -> 'there'\n",
      "    IN -> 'above'\n",
      "    NN -> 'Block'\n",
      "    RB -> 'down'\n",
      "    IN -> 'on'\n",
      "    NN -> 'shop'\n",
      "    S -> PRP$ NP\n",
      "    IN -> 'past'\n",
      "    NP -> NP NN\n",
      "    JJ -> 'Gothic'\n",
      "    JJ -> 'silver'\n",
      "    CD -> 'Three'\n",
      "    NNP -> 'Hotel'\n",
      "    UH -> 'no'\n",
      "    VP -> VBZ VP\n",
      "    NNS -> 'walls'\n",
      "    VP -> VBZ S\n",
      "    NNP -> 'Cafe'\n",
      "    JJ -> 'big'\n",
      "    NNP -> 'Assembly'\n",
      "    NP -> NP NP\n",
      "    JJ -> 'glassy'\n",
      "    NN -> 'spire'\n",
      "    IN -> 'of'\n",
      "    RB -> \n",
      "    WRB -> 'where'\n",
      "    NNP -> 'Royal'\n",
      "    ADJP -> JJ\n",
      "    NP -> NP , S\n",
      "    NP -> JJ NN CC NN\n",
      "    WP -> 'who'\n",
      "    VBZ -> 'has'\n",
      "    IN -> 'over'\n",
      "    NP -> DT JJ NN NN\n",
      "    MD -> 'ca'\n",
      "    JJ -> 'visible'\n",
      "    EX -> 'there'\n",
      "    VBZ -> 'Looks'\n",
      "    NN -> 'storey'\n",
      "    NN -> 'roof'\n",
      "    VBN -> 'covered'\n",
      "    NN -> 'centre'\n",
      "    JJ -> 'storey'\n",
      "    NN -> 'bank'\n",
      "    NP -> NNP NNP POS\n",
      "    NP -> DT JJ JJ NN\n",
      "    NN -> 'tree'\n",
      "    VP -> VBG PP ADVP\n",
      "    NNS -> 'poundsavers'\n",
      "    NP -> VBG NNS\n",
      "    NP -> DT JJ JJ JJ NN\n",
      "    NN -> 'tenement'\n",
      "    S -> ADVP VP\n",
      "    SBAR -> IN S\n",
      "    VP -> VBZ RB ADJP\n",
      "    NN -> 'festival'\n",
      "    NP -> DT ADJP NN NN\n",
      "    PP -> PP , PP ,\n",
      "    IN -> 'with'\n",
      "    NP -> DT JJS NN\n",
      "    VP -> VBG\n",
      "    FW -> 'suruchi'\n",
      "    VP -> VB SBAR\n",
      "    VP -> VBZ `` NP -RRB-\n",
      "    JJ -> 'outdoor'\n",
      "    PDT -> 'all'\n",
      "    NP -> NP CC PP\n",
      "    NP -> NNS\n",
      "    JJ -> 'half'\n",
      "    IN -> 'onto'\n",
      "    NN -> 'friend'\n",
      "    JJ -> 'slight'\n",
      "    NN -> 'rooftop'\n",
      "    JJ -> 'fun'\n",
      "    VBG -> 'including'\n",
      "    NNP -> 'Tartan'\n",
      "    NN -> 'place'\n",
      "    NN -> 'cherry'\n",
      "    JJ -> 'ancient'\n",
      "    NN -> 'cross'\n",
      "    VBN -> 'topped'\n",
      "    VP -> VBZ ADJP ADVP\n",
      "    NP -> JJ NN HYPH NN\n",
      "    VBP -> 'think'\n",
      "    NP -> NP , VP\n",
      "    JJ -> 'Small'\n",
      "    NN -> 'junction'\n",
      "    NN -> 'point'\n",
      "    PP -> IN NP\n",
      "    CD -> 'two'\n",
      "    NN -> 'chimney'\n",
      "    NP -> VBN NNS\n",
      "    UH -> 'Wow'\n",
      "    NP -> PRP$ JJ NN NN\n",
      "    . -> '?'\n",
      "    NP -> JJ\n",
      "    JJ -> 'symmetrical'\n",
      "    NN -> 'foreground'\n",
      "    NP -> CD ADJP NNS\n",
      "    NN -> 'pantheon'\n",
      "    VP -> VB ADVP\n",
      "    VB -> 'see'\n",
      "    NP -> NP PP , ADVP\n",
      "    ADJP -> JJR JJ\n",
      "    NNS -> 'trees'\n",
      "    CD -> '12'\n",
      "    NNP -> 'Ramsay'\n",
      "    NP -> DT ADJP CD\n",
      "    NN -> 'monument'\n",
      "    NN -> 'picture'\n",
      "    VP -> VBD PP\n",
      "    RB -> 'next'\n",
      "    DT -> 'a'\n",
      "    NNS -> 'parts'\n",
      "    VBN -> 'bricked'\n",
      "    RB -> 'only'\n",
      "    NN -> 'back'\n",
      "    NN -> 'property'\n",
      "    NN -> 'way'\n",
      "    NP -> NP JJ NN\n",
      "    JJR -> 'smaller'\n",
      "    NN -> 'left'\n",
      "    NN -> 'camera'\n",
      "    CC -> 'but'\n",
      "    PP -> IN\n",
      "    NN -> 'hut'\n",
      "    JJ -> 'Triangular'\n",
      "    JJ -> 'Typical'\n",
      "    NP -> DT ADJP NN\n",
      "    VBZ -> 'appears'\n",
      "    NP -> JJ JJ NN NNS\n",
      "    JJ -> 'narrow'\n",
      "    RB -> 'right'\n",
      "    NNP -> 'Subway'\n",
      "    NN -> 'middle'\n",
      "    NN -> 'house'\n",
      "    NNS -> 'sides'\n",
      "    RB -> 'almost'\n",
      "    NNP -> 'Caf+'\n",
      "    VP -> VP , VP\n",
      "    VB -> 'tell'\n",
      "    RB -> 'above'\n",
      "    PP -> PP ADVP\n",
      "    JJ -> 'middle'\n",
      "    NP -> DT NML NNP\n",
      "    NP -> DT NN HYPH NN\n",
      "    NNS -> 'chairs'\n",
      "    JJ -> 'Middle'\n",
      "    NNS -> 'banners'\n",
      "    NP -> DT\n",
      "    VP -> VBP NP\n",
      "    NN -> 'horse'\n",
      "    RB -> 'Probably'\n",
      "    JJ -> 'light'\n",
      "    VBN -> 'obscured'\n",
      "    NP -> JJ HYPH NN\n",
      "    `` -> '\"'\n",
      "    RB -> 'partly'\n",
      "    NP -> DT VBN NN\n",
      "    NP -> CD JJ NN\n",
      "    UH -> 'hey'\n",
      "    NN -> 'everything'\n",
      "    JJ -> 'arched'\n",
      "    NP -> RB CD NN\n",
      "    RB -> 'of'\n",
      "    IN -> 'beside'\n",
      "    JJ -> 'Left'\n",
      "    VP -> VB NP ADVP\n",
      "    PP -> RB IN NP\n",
      "    -RRB- -> ''\n",
      "    NP -> PRP NN\n",
      "    NN -> 'type'\n",
      "    VBG -> 'surrounding'\n",
      "    PRP -> 'you'\n",
      "    S -> ADJP\n",
      "    NNP -> 'Elephant'\n",
      "    S -> `` NP ''\n",
      "    PRP -> 'I'\n",
      "    VBG -> 'Looking'\n",
      "    NN -> 'fence'\n",
      "    NNP -> 'Georgian'\n",
      "    JJ -> 'Opposite'\n",
      "    NP -> NNP NN\n",
      "    CC -> '+'\n",
      "    NP -> FW\n",
      "    JJ -> 'fronted'\n",
      "    NNP -> 'Theatre'\n",
      "    RB -> 'Ahead'\n",
      "    NNS -> 'blocks'\n",
      "    IN -> 'as'\n",
      "    RB -> 'really'\n",
      "    NN -> 'store'\n",
      "    NNS -> 'towers'\n",
      "    NN -> 'hill'\n",
      "    JJ -> 'Large'\n",
      "    NP -> NP SBAR\n",
      "    NNP -> 'Weaving'\n",
      "    ADJP -> NP PP\n",
      "    IN -> 'With'\n",
      "    NP -> PDT DT NN\n",
      "    VP -> VP ''\n",
      "    ADJP -> RB JJR\n",
      "    NN -> 'gallery'\n",
      "    JJR -> 'larger'\n",
      "    VBN -> 'storied'\n",
      "    NN -> 'Door'\n",
      "    NP -> DT NN CC NN\n",
      "    JJ -> 'wooden'\n",
      "    NP -> DT CD\n",
      "    NP -> NN NNS\n",
      "    NP -> JJ ADJP NN\n",
      "    NN -> 'seating'\n",
      "    NNS -> 'streets'\n",
      "    IN -> 'To'\n",
      "    VP -> VB VP\n",
      "    NNS -> 'plants'\n",
      "    VBG -> 'building'\n",
      "    VP -> VB S\n",
      "    ADJP -> NN VBN\n",
      "    S -> PP VP NP\n",
      "    ADJP -> NN JJ\n",
      "    NNP -> 'RBS'\n",
      "    NN -> 'dome'\n",
      "    NNP -> 'Scott'\n",
      "    RB -> 'Next'\n",
      "    NN -> 'turret'\n",
      "    ADJP -> CD JJ\n",
      "    JJ -> 'thai'\n",
      "    VP -> VBN\n",
      "    NP -> DT JJR NN\n",
      "    JJ -> 'old'\n",
      "    NNS -> 'offices'\n",
      "    JJ -> 'traditional'\n",
      "    RB -> 'already'\n",
      "    NN -> 'window'\n",
      "    NP -> DT JJ , ADJP NN\n",
      "    NP -> NP NNP\n",
      "    VP -> MD RB VP\n",
      "    NNS -> 'faces'\n",
      "    JJ -> 'dark'\n",
      "    VBN -> 'colored'\n",
      "    NNS -> 'dormers'\n",
      "    JJ -> 'gable'\n",
      "    VBD -> 'called'\n",
      "    VBG -> 'facing'\n",
      "    PP -> PP CC ADVP\n",
      "    NP -> PRP PRP\n",
      "    VBN -> 'arched'\n",
      "    PRP -> 'He'\n",
      "    SBAR -> S\n",
      "    NP -> NML NNS\n",
      "    VP -> VBZ\n",
      "    VBZ -> 'shows'\n",
      "    VP -> VBZ ADJP S\n",
      "    NNS -> 'posters'\n",
      "    CD -> '9'\n",
      "    DT -> 'That'\n",
      "    VP -> VBD NP\n",
      "    NP -> JJ NN NN\n",
      "    PP -> PP PP\n",
      "    JJS -> 'most'\n",
      "    RB -> 'kind'\n",
      "    JJ -> 'multi'\n",
      "    CC -> '&'\n",
      "    TO -> 'to'\n",
      "    NN -> 'number'\n",
      "    NP -> QP NN\n",
      "    RBS -> 'most'\n",
      "    NNS -> 'railings'\n",
      "    VBN -> 'fronted'\n",
      "    NML -> NNS\n",
      "    JJ -> 'national'\n",
      "    NP -> NP PP\n",
      "    IN -> 'like'\n",
      "    S -> NP VP .\n",
      "    VP -> VP NP\n",
      "    JJ -> 'medieval'\n",
      "    NN -> 'iron'\n",
      "    NNS -> 'boards'\n",
      "    CC -> 'and'\n",
      "    NP -> DT NNP NNP NNP\n",
      "    ADVP -> IN RB\n",
      "    NNP -> 'Scottish'\n",
      "    VP -> VBP ADJP PP\n",
      "    VP -> VBZ NP PP\n",
      "    NP -> JJ NNS\n",
      "    NN -> 'menu'\n",
      "    JJ -> 'funny'\n",
      "    VBP -> 'are'\n",
      "    NN -> 'one'\n",
      "    NN -> 'castle'\n",
      "    NN -> 'shed'\n",
      "    NP -> JJ NNS CC NNS\n",
      "    JJ -> 'short'\n",
      "    RP -> 'in'\n",
      "    JJ -> 'little'\n",
      "    IN -> 'across'\n",
      "    PP -> IN NP PP\n",
      "    JJ -> '2nd'\n",
      "    CD -> '10'\n",
      "    VP -> VB PP PP\n",
      "    VP -> VBN S\n",
      "    RB -> 'at'\n",
      "    S -> S CC S\n",
      "    NNP -> 'House'\n",
      "    NN -> 'center'\n",
      "    VBN -> 'terraced'\n",
      "    JJ -> 'small'\n",
      "    VBP -> 'look'\n",
      "    VP -> VP .\n",
      "    VBZ -> 'sits'\n",
      "    NP -> NP VP\n",
      "    NN -> 'color'\n",
      "    PRP -> 'It'\n",
      "    NN -> 'corner'\n",
      "    NP -> DT JJ CD\n",
      "    VP -> VB NP S\n",
      "    RB -> 'as'\n",
      "    RB -> 'slightly'\n",
      "    NN -> 'terrace'\n",
      "    NN -> 'basil'\n",
      "    ADVP -> RB JJ PP\n",
      "    JJ -> 'portable'\n",
      "    JJR -> 'lighter'\n",
      "    NN -> 'storefront'\n",
      "    NN -> 'bottom'\n",
      "    NN -> 'floor'\n",
      "    JJ -> 'greenish'\n",
      "    NN -> 'flagpole'\n",
      "    NNS -> 'cones'\n",
      "    PRP -> 'they'\n",
      "    NNP -> 'Suruchi'\n",
      "    NP -> DT JJ NN\n",
      "    WDT -> 'which'\n",
      "    JJ -> 'different'\n",
      "    NNS -> 'battlements'\n",
      "    NP -> CD JJR JJ NNS\n",
      "    NP -> DT NNP NN\n",
      "    JJ -> 'bottom'\n",
      "    NNP -> 'Florentine'\n",
      "    JJR -> 'darker'\n",
      "    NP -> NP SYM NP\n",
      "    JJ -> 'triangular'\n",
      "    VP -> VBN ADVP PP\n",
      "    NN -> 'pavement'\n",
      "    JJ -> 'far'\n",
      "    VBZ -> 'Has'\n",
      "    JJ -> 'rectangular'\n",
      "    IN -> 'Between'\n",
      "    NN -> 'story'\n",
      "    NNS -> 'buildings'\n",
      "    CD -> 'six'\n",
      "    NP -> NML JJ NN\n",
      "    NP -> NN FW\n",
      "    NNP -> 'Mill'\n",
      "    NN -> 'post'\n",
      "    NP -> DT JJ ADJP NN\n",
      "    VBN -> 'Called'\n",
      "    NN -> 'bridge'\n",
      "    RB -> 'also'\n",
      "    VBP -> \"'re\"\n",
      "    NNP -> 'Gallery'\n",
      "    SINV -> PP VP NP\n",
      "    NNP -> 'Princes'\n",
      "    NNS -> 'apartments'\n",
      "    NNS -> 'statues'\n",
      "    NN -> 'background'\n",
      "    NNS -> 'flats'\n",
      "    S -> NP VP\n",
      "    CD -> '5'\n",
      "    ADJP -> JJ PP\n",
      "    PRP$ -> 'your'\n",
      "    NML -> JJ NN\n",
      "    RB -> 'quite'\n",
      "    NP -> NN HYPH NN\n",
      "    NN -> 'frontage'\n",
      "    JJ -> 'Italian'\n",
      "    JJ -> 'other'\n",
      "    RB -> 'here'\n",
      "    ADVP -> RB\n",
      "    JJR -> 'taller'\n",
      "    NNS -> 'rooms'\n",
      "    JJ -> 'temporary'\n",
      "    NN -> 'row'\n",
      "    NP -> NNP NNP\n",
      "    -LRB- -> '-LRB-'\n",
      "    NP -> JJS\n",
      "    NP -> JJ NNP NN\n",
      "    JJ -> 'Greek'\n",
      "    JJ -> 'squat'\n",
      "    NML -> NML NN\n",
      "    SQ -> VBZ NP VP\n",
      "    SBAR -> WHPP S\n",
      "    JJ -> 'first'\n",
      "    NP -> NP ADVP PP\n",
      "    JJ -> 'ornate'\n",
      "    JJ -> 'Roman'\n",
      "    VP -> VBZ ADJP\n"
     ]
    }
   ],
   "source": [
    "print(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6303ab2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Traditional', 'early', 'Victorian', 'terrace', 'on', '3', 'floors', '+', 'dormer']\n"
     ]
    }
   ],
   "source": [
    "test_tokens = trees[2].leaves()\n",
    "print(test_tokens)\n",
    "# for t in parser.parse(test_tokens):\n",
    "# #     # t.pretty_print()\n",
    "#     print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6677d778",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-benepar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
